wandb: Agent Starting Run: hwhlvyb7 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: adam
wandb: 	weight_decay: 0
wandb: Currently logged in as: naddeok (use `wandb login --relogin` to force relogin)
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep
Loading Data...
...done

Loading Academy...
Create sweep with ID: 2w567fe2
Sweep URL: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:19:25.409194: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:19:25.415712: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run laced-sweep-1
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/hwhlvyb7
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_141923-hwhlvyb7
wandb: Run `wandb offline` to turn off syncing.
Epoch:  1 	 Loss:  0.028323357924818993
Epoch:  2 	 Loss:  0.028278743848204613
Epoch:  3 	 Loss:  0.027793889865279198
Epoch:  4 	 Loss:  0.029040982946753502
Epoch:  5 	 Loss:  0.02796933799982071
Epoch:  6 	 Loss:  0.028111623600125313
Epoch:  7 	 Loss:  0.02766062691807747
Epoch:  8 	 Loss:  0.027983389794826508
Epoch:  9 	 Loss:  0.028619691729545593
Epoch:  10 	 Loss:  0.028147634118795395
Epoch:  11 	 Loss:  0.028496425598859787
Epoch:  12 	 Loss:  0.02783619426190853
Epoch:  13 	 Loss:  0.028693951666355133
Epoch:  14 	 Loss:  0.028038963675498962
Epoch:  15 	 Loss:  0.028321225196123123
Epoch:  16 	 Loss:  0.02765587717294693
Epoch:  17 	 Loss:  0.028075989335775375
Epoch:  18 	 Loss:  0.02831876277923584
Epoch:  19 	 Loss:  0.028517786413431168
Epoch:  20 	 Loss:  0.02797776833176613
Epoch:  21 	 Loss:  0.02766243740916252
Epoch:  22 	 Loss:  0.02786092460155487
Epoch:  23 	 Loss:  0.02716156654059887
Epoch:  24 	 Loss:  0.028448766097426414
Epoch:  25 	 Loss:  0.028498847037553787
Epoch:  26 	 Loss:  0.02803839184343815
Epoch:  27 	 Loss:  0.027915440499782562
Epoch:  28 	 Loss:  0.02805270068347454
Epoch:  29 	 Loss:  0.028822677209973335
Epoch:  30 	 Loss:  0.028116660192608833
Epoch:  31 	 Loss:  0.02858942374587059
Epoch:  32 	 Loss:  0.028417367488145828
Epoch:  33 	 Loss:  0.028470924124121666
Epoch:  34 	 Loss:  0.02855033613741398
Epoch:  35 	 Loss:  0.027369357645511627
Epoch:  36 	 Loss:  0.027992568910121918
Epoch:  37 	 Loss:  0.02821359597146511
Epoch:  38 	 Loss:  0.028123391792178154
Epoch:  39 	 Loss:  0.02815377153456211
Epoch:  40 	 Loss:  0.027698498219251633
Epoch:  41 	 Loss:  0.028662901371717453
Epoch:  42 	 Loss:  0.02810242399573326
Epoch:  43 	 Loss:  0.028027553111314774
Epoch:  44 	 Loss:  0.028232092037796974
Epoch:  45 	 Loss:  0.02788013592362404
Epoch:  46 	 Loss:  0.028384722769260406
Epoch:  47 	 Loss:  0.028485210612416267
Epoch:  48 	 Loss:  0.028471441939473152
Epoch:  49 	 Loss:  0.029078377410769463
Epoch:  50 	 Loss:  0.028578326106071472
Epoch:  51 	 Loss:  0.027664564549922943
Epoch:  52 	 Loss:  0.02782435156404972
Epoch:  53 	 Loss:  0.028192175552248955
Epoch:  54 	 Loss:  0.02830037660896778
Epoch:  55 	 Loss:  0.028256777673959732
Epoch:  56 	 Loss:  0.028217187151312828
Epoch:  57 	 Loss:  0.027673469856381416
Epoch:  58 	 Loss:  0.028532596305012703
Epoch:  59 	 Loss:  0.0273921936750412
Epoch:  60 	 Loss:  0.02791229635477066
Epoch:  61 	 Loss:  0.0276672150939703
Epoch:  62 	 Loss:  0.028079692274332047
Epoch:  63 	 Loss:  0.02869311161339283
Epoch:  64 	 Loss:  0.027450382709503174
Epoch:  65 	 Loss:  0.028036171570420265
Epoch:  66 	 Loss:  0.027707833796739578
Epoch:  67 	 Loss:  0.02834799513220787
Epoch:  68 	 Loss:  0.028141316026449203
Epoch:  69 	 Loss:  0.0280916690826416
Epoch:  70 	 Loss:  0.0279298834502697
Epoch:  71 	 Loss:  0.028732355684041977
Epoch:  72 	 Loss:  0.027928795665502548
Epoch:  73 	 Loss:  0.02832038141787052
Epoch:  74 	 Loss:  0.027957383543252945
Epoch:  75 	 Loss:  0.028474073857069016
Epoch:  76 	 Loss:  0.028139833360910416
Epoch:  77 	 Loss:  0.0279606394469738
Epoch:  78 	 Loss:  0.027948511764407158
Epoch:  79 	 Loss:  0.027965664863586426
Epoch:  80 	 Loss:  0.027936572209000587
Epoch:  81 	 Loss:  0.028008267283439636
Epoch:  82 	 Loss:  0.028344525024294853
Epoch:  83 	 Loss:  0.028247293084859848
Epoch:  84 	 Loss:  0.028508173301815987
Epoch:  85 	 Loss:  0.028448177501559258
Epoch:  86 	 Loss:  0.028319045901298523
Epoch:  87 	 Loss:  0.02759447880089283
Epoch:  88 	 Loss:  0.028265994042158127
Epoch:  89 	 Loss:  0.028252696618437767
Epoch:  90 	 Loss:  0.027719635516405106
Epoch:  91 	 Loss:  0.02797292359173298
Epoch:  92 	 Loss:  0.02855447679758072
Epoch:  93 	 Loss:  0.028375042602419853
Epoch:  94 	 Loss:  0.028048917651176453
Epoch:  95 	 Loss:  0.028742006048560143
Epoch:  96 	 Loss:  0.02830132097005844
Epoch:  97 	 Loss:  0.02830555848777294
Epoch:  98 	 Loss:  0.028073575347661972
Epoch:  99 	 Loss:  0.02846832387149334
Epoch:  100 	 Loss:  0.028477361425757408
wandb: Waiting for W&B process to finish, PID 1619147
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_141923-hwhlvyb7/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_141923-hwhlvyb7/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02848
wandb:        _step 800000
wandb:     _runtime 82
wandb:   _timestamp 1612898445
wandb:         loss 0.02848
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–ƒâ–„â–„â–†â–‡â–ƒâ–…â–ƒâ–â–„â–„â–†â–†â–„â–…â–†â–…â–…â–ˆâ–ƒâ–…â–…â–‚â–ƒâ–‚â–…â–„â–„â–„â–„â–„â–…â–†â–ƒâ–…â–†â–„â–…â–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced laced-sweep-1: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/hwhlvyb7
wandb: Agent Starting Run: gdpwbrh1 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:20:52.814242: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:20:52.821578: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run frosty-sweep-2
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/gdpwbrh1
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_142050-gdpwbrh1
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.031493041664361954
Epoch:  2 	 Loss:  0.03040848858654499
Epoch:  3 	 Loss:  0.030241282656788826
Epoch:  4 	 Loss:  0.031277645379304886
Epoch:  5 	 Loss:  0.03071839176118374
Epoch:  6 	 Loss:  0.030231865122914314
Epoch:  7 	 Loss:  0.03146001696586609
Epoch:  8 	 Loss:  0.030894430354237556
Epoch:  9 	 Loss:  0.03021937608718872
Epoch:  10 	 Loss:  0.03150488808751106
Epoch:  11 	 Loss:  0.03142980858683586
Epoch:  12 	 Loss:  0.031035717576742172
Epoch:  13 	 Loss:  0.030557746067643166
Epoch:  14 	 Loss:  0.030791662633419037
Epoch:  15 	 Loss:  0.03127588704228401
Epoch:  16 	 Loss:  0.030073491856455803
Epoch:  17 	 Loss:  0.031026219949126244
Epoch:  18 	 Loss:  0.030778173357248306
Epoch:  19 	 Loss:  0.0308676827698946
Epoch:  20 	 Loss:  0.031003914773464203
Epoch:  21 	 Loss:  0.030254032462835312
Epoch:  22 	 Loss:  0.030747776851058006
Epoch:  23 	 Loss:  0.030739741399884224
Epoch:  24 	 Loss:  0.031006570905447006
Epoch:  25 	 Loss:  0.030696062371134758
Epoch:  26 	 Loss:  0.029777513816952705
Epoch:  27 	 Loss:  0.030696289613842964
Epoch:  28 	 Loss:  0.03059728629887104
Epoch:  29 	 Loss:  0.03132421150803566
Epoch:  30 	 Loss:  0.03149973601102829
Epoch:  31 	 Loss:  0.0302822757512331
Epoch:  32 	 Loss:  0.03089381940662861
Epoch:  33 	 Loss:  0.03091365471482277
Epoch:  34 	 Loss:  0.031205108389258385
Epoch:  35 	 Loss:  0.03057476133108139
Epoch:  36 	 Loss:  0.03108517825603485
Epoch:  37 	 Loss:  0.030758431181311607
Epoch:  38 	 Loss:  0.031706202775239944
Epoch:  39 	 Loss:  0.03163417801260948
Epoch:  40 	 Loss:  0.031098442152142525
Epoch:  41 	 Loss:  0.030307475477457047
Epoch:  42 	 Loss:  0.030950399115681648
Epoch:  43 	 Loss:  0.03047878108918667
Epoch:  44 	 Loss:  0.031192760914564133
Epoch:  45 	 Loss:  0.03118063323199749
Epoch:  46 	 Loss:  0.03175464645028114
Epoch:  47 	 Loss:  0.031212367117404938
Epoch:  48 	 Loss:  0.0302272979170084
Epoch:  49 	 Loss:  0.030498629435896873
Epoch:  50 	 Loss:  0.03089318238198757
Epoch:  51 	 Loss:  0.030778462067246437
Epoch:  52 	 Loss:  0.030702948570251465
Epoch:  53 	 Loss:  0.030572257936000824
Epoch:  54 	 Loss:  0.030666232109069824
Epoch:  55 	 Loss:  0.03027118556201458
Epoch:  56 	 Loss:  0.031672388315200806
Epoch:  57 	 Loss:  0.030931102111935616
Epoch:  58 	 Loss:  0.031142523512244225
Epoch:  59 	 Loss:  0.0315680131316185
Epoch:  60 	 Loss:  0.031295374035835266
Epoch:  61 	 Loss:  0.030285898596048355
Epoch:  62 	 Loss:  0.031119873747229576
Epoch:  63 	 Loss:  0.03053956851363182
Epoch:  64 	 Loss:  0.030771255493164062
Epoch:  65 	 Loss:  0.030620798468589783
Epoch:  66 	 Loss:  0.029922209680080414
Epoch:  67 	 Loss:  0.03124273382127285
Epoch:  68 	 Loss:  0.03087315522134304
Epoch:  69 	 Loss:  0.03050386905670166
Epoch:  70 	 Loss:  0.030602380633354187
Epoch:  71 	 Loss:  0.031680334359407425
Epoch:  72 	 Loss:  0.030948249623179436
Epoch:  73 	 Loss:  0.031288497149944305
Epoch:  74 	 Loss:  0.03069809265434742
Epoch:  75 	 Loss:  0.03044499270617962
Epoch:  76 	 Loss:  0.0309640821069479
Epoch:  77 	 Loss:  0.029983175918459892
Epoch:  78 	 Loss:  0.031429849565029144
Epoch:  79 	 Loss:  0.030223799869418144
Epoch:  80 	 Loss:  0.031410783529281616
Epoch:  81 	 Loss:  0.0314304456114769
Epoch:  82 	 Loss:  0.03104502335190773
Epoch:  83 	 Loss:  0.030905095860362053
Epoch:  84 	 Loss:  0.031383268535137177
Epoch:  85 	 Loss:  0.031386375427246094
Epoch:  86 	 Loss:  0.0314502976834774
Epoch:  87 	 Loss:  0.03163478150963783
Epoch:  88 	 Loss:  0.030692683532834053
Epoch:  89 	 Loss:  0.030767930671572685
Epoch:  90 	 Loss:  0.031133294105529785
Epoch:  91 	 Loss:  0.030095214024186134
Epoch:  92 	 Loss:  0.03068995475769043
Epoch:  93 	 Loss:  0.031107444316148758
Epoch:  94 	 Loss:  0.030703915283083916
Epoch:  95 	 Loss:  0.03114589862525463
Epoch:  96 	 Loss:  0.030654625967144966
Epoch:  97 	 Loss:  0.030906017869710922
Epoch:  98 	 Loss:  0.030276766046881676
Epoch:  99 	 Loss:  0.030809057876467705
Epoch:  100 	 Loss:  0.03110501915216446
wandb: Waiting for W&B process to finish, PID 1622838
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142050-gdpwbrh1/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142050-gdpwbrh1/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.03111
wandb:        _step 800000
wandb:     _runtime 78
wandb:   _timestamp 1612898528
wandb:         loss 0.03111
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–‡â–ƒâ–ƒâ–…â–‡â–„â–‚â–…â–ƒâ–„â–â–„â–ƒâ–†â–†â–ˆâ–ƒâ–†â–ˆâ–„â–…â–„â–ˆâ–‡â–ƒâ–…â–†â–„â–…â–„â–‚â–ƒâ–…â–‡â–ˆâ–…â–„â–„â–…â–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced frosty-sweep-2: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/gdpwbrh1
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: h4168kmc with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:22:27.855843: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:22:27.861022: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run usual-sweep-3
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/h4168kmc
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_142225-h4168kmc
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02854287624359131
Epoch:  2 	 Loss:  0.02904602140188217
Epoch:  3 	 Loss:  0.028856484219431877
Epoch:  4 	 Loss:  0.02936873957514763
Epoch:  5 	 Loss:  0.02872125804424286
Epoch:  6 	 Loss:  0.028925592079758644
Epoch:  7 	 Loss:  0.028666352853178978
Epoch:  8 	 Loss:  0.028626399114727974
Epoch:  9 	 Loss:  0.028966853395104408
Epoch:  10 	 Loss:  0.029194777831435204
Epoch:  11 	 Loss:  0.028844520449638367
Epoch:  12 	 Loss:  0.02863514982163906
Epoch:  13 	 Loss:  0.027770524844527245
Epoch:  14 	 Loss:  0.028241261839866638
Epoch:  15 	 Loss:  0.02933347225189209
Epoch:  16 	 Loss:  0.02878325805068016
Epoch:  17 	 Loss:  0.028652235865592957
Epoch:  18 	 Loss:  0.02861650474369526
Epoch:  19 	 Loss:  0.02852325700223446
Epoch:  20 	 Loss:  0.028708940371870995
Epoch:  21 	 Loss:  0.028974900022149086
Epoch:  22 	 Loss:  0.02938746102154255
Epoch:  23 	 Loss:  0.02898612804710865
Epoch:  24 	 Loss:  0.02914481982588768
Epoch:  25 	 Loss:  0.028638161718845367
Epoch:  26 	 Loss:  0.028840990737080574
Epoch:  27 	 Loss:  0.029312197118997574
Epoch:  28 	 Loss:  0.028647620230913162
Epoch:  29 	 Loss:  0.02858113683760166
Epoch:  30 	 Loss:  0.028392823413014412
Epoch:  31 	 Loss:  0.029077423736453056
Epoch:  32 	 Loss:  0.02826024405658245
Epoch:  33 	 Loss:  0.02900753915309906
Epoch:  34 	 Loss:  0.02871757186949253
Epoch:  35 	 Loss:  0.028803380206227303
Epoch:  36 	 Loss:  0.028900351375341415
Epoch:  37 	 Loss:  0.028391506522893906
Epoch:  38 	 Loss:  0.028609005734324455
Epoch:  39 	 Loss:  0.029223991557955742
Epoch:  40 	 Loss:  0.029155179858207703
Epoch:  41 	 Loss:  0.028619665652513504
Epoch:  42 	 Loss:  0.02887354977428913
Epoch:  43 	 Loss:  0.028760652989149094
Epoch:  44 	 Loss:  0.028743857517838478
Epoch:  45 	 Loss:  0.029142392799258232
Epoch:  46 	 Loss:  0.029468266293406487
Epoch:  47 	 Loss:  0.028606340289115906
Epoch:  48 	 Loss:  0.029002483934164047
Epoch:  49 	 Loss:  0.028698956593871117
Epoch:  50 	 Loss:  0.029236944392323494
Epoch:  51 	 Loss:  0.028742846101522446
Epoch:  52 	 Loss:  0.02928798459470272
Epoch:  53 	 Loss:  0.028489043936133385
Epoch:  54 	 Loss:  0.029507951810956
Epoch:  55 	 Loss:  0.028972497209906578
Epoch:  56 	 Loss:  0.02906212769448757
Epoch:  57 	 Loss:  0.028223389759659767
Epoch:  58 	 Loss:  0.028513330966234207
Epoch:  59 	 Loss:  0.02902493067085743
Epoch:  60 	 Loss:  0.029064573347568512
Epoch:  61 	 Loss:  0.029190286993980408
Epoch:  62 	 Loss:  0.02962355874478817
Epoch:  63 	 Loss:  0.028785323724150658
Epoch:  64 	 Loss:  0.028915222734212875
Epoch:  65 	 Loss:  0.02866951748728752
Epoch:  66 	 Loss:  0.028386441990733147
Epoch:  67 	 Loss:  0.028140371665358543
Epoch:  68 	 Loss:  0.029015835374593735
Epoch:  69 	 Loss:  0.028801914304494858
Epoch:  70 	 Loss:  0.02881450392305851
Epoch:  71 	 Loss:  0.028736256062984467
Epoch:  72 	 Loss:  0.029177533462643623
Epoch:  73 	 Loss:  0.02830364555120468
Epoch:  74 	 Loss:  0.02859906107187271
Epoch:  75 	 Loss:  0.028573984280228615
Epoch:  76 	 Loss:  0.02865699678659439
Epoch:  77 	 Loss:  0.028870224952697754
Epoch:  78 	 Loss:  0.028652483597397804
Epoch:  79 	 Loss:  0.029065068811178207
Epoch:  80 	 Loss:  0.028584228828549385
Epoch:  81 	 Loss:  0.028560863807797432
Epoch:  82 	 Loss:  0.02891874685883522
Epoch:  83 	 Loss:  0.028890911489725113
Epoch:  84 	 Loss:  0.02894785813987255
Epoch:  85 	 Loss:  0.028573058545589447
Epoch:  86 	 Loss:  0.028514966368675232
Epoch:  87 	 Loss:  0.028729351237416267
Epoch:  88 	 Loss:  0.02890581265091896
Epoch:  89 	 Loss:  0.028851255774497986
Epoch:  90 	 Loss:  0.028585204854607582
Epoch:  91 	 Loss:  0.02794625237584114
Epoch:  92 	 Loss:  0.028489112854003906
Epoch:  93 	 Loss:  0.02923351153731346
Epoch:  94 	 Loss:  0.02875078283250332
Epoch:  95 	 Loss:  0.028902988880872726
Epoch:  96 	 Loss:  0.02888501062989235
Epoch:  97 	 Loss:  0.028880853205919266
Epoch:  98 	 Loss:  0.028408778831362724
Epoch:  99 	 Loss:  0.02941296063363552
Epoch:  100 	 Loss:  0.028520839288830757
wandb: Waiting for W&B process to finish, PID 1626540
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142225-h4168kmc/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142225-h4168kmc/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02852
wandb:        _step 800000
wandb:     _runtime 86
wandb:   _timestamp 1612898631
wandb:         loss 0.02852
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–…â–†â–„â–…â–â–…â–„â–†â–†â–…â–…â–†â–…â–†â–‡â–„â–…â–ˆâ–…â–…â–ˆâ–†â–†â–‡â–†â–‚â–…â–‡â–„â–…â–†â–†â–†â–…â–…â–„â–…â–…â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced usual-sweep-3: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/h4168kmc
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: a0txg3o6 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:24:07.828964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:24:07.835496: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run faithful-sweep-4
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/a0txg3o6
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_142405-a0txg3o6
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027030624449253082
Epoch:  2 	 Loss:  0.027124380692839622
Epoch:  3 	 Loss:  0.027069684118032455
Epoch:  4 	 Loss:  0.027539871633052826
Epoch:  5 	 Loss:  0.02742171846330166
Epoch:  6 	 Loss:  0.026920946314930916
Epoch:  7 	 Loss:  0.02714717388153076
Epoch:  8 	 Loss:  0.02708418481051922
Epoch:  9 	 Loss:  0.02763514779508114
Epoch:  10 	 Loss:  0.02748885378241539
Epoch:  11 	 Loss:  0.02698477916419506
Epoch:  12 	 Loss:  0.027070218697190285
Epoch:  13 	 Loss:  0.027064882218837738
Epoch:  14 	 Loss:  0.027723971754312515
Epoch:  15 	 Loss:  0.027571802958846092
Epoch:  16 	 Loss:  0.02747775986790657
Epoch:  17 	 Loss:  0.02714398317039013
Epoch:  18 	 Loss:  0.027491794899106026
Epoch:  19 	 Loss:  0.027631383389234543
Epoch:  20 	 Loss:  0.027392035350203514
Epoch:  21 	 Loss:  0.027086835354566574
Epoch:  22 	 Loss:  0.027408059686422348
Epoch:  23 	 Loss:  0.027487626299262047
Epoch:  24 	 Loss:  0.027783827856183052
Epoch:  25 	 Loss:  0.026930784806609154
Epoch:  26 	 Loss:  0.02713700383901596
Epoch:  27 	 Loss:  0.02733662538230419
Epoch:  28 	 Loss:  0.027504902333021164
Epoch:  29 	 Loss:  0.027486136183142662
Epoch:  30 	 Loss:  0.02698708511888981
Epoch:  31 	 Loss:  0.02702472172677517
Epoch:  32 	 Loss:  0.026931170374155045
Epoch:  33 	 Loss:  0.02692457288503647
Epoch:  34 	 Loss:  0.026824500411748886
Epoch:  35 	 Loss:  0.027267634868621826
Epoch:  36 	 Loss:  0.02754165604710579
Epoch:  37 	 Loss:  0.02740878239274025
Epoch:  38 	 Loss:  0.027320440858602524
Epoch:  39 	 Loss:  0.02701154164969921
Epoch:  40 	 Loss:  0.027333484962582588
Epoch:  41 	 Loss:  0.02747408300638199
Epoch:  42 	 Loss:  0.027692386880517006
Epoch:  43 	 Loss:  0.02749442495405674
Epoch:  44 	 Loss:  0.027203692123293877
Epoch:  45 	 Loss:  0.027043694630265236
Epoch:  46 	 Loss:  0.027032306417822838
Epoch:  47 	 Loss:  0.027363555505871773
Epoch:  48 	 Loss:  0.02691967971622944
Epoch:  49 	 Loss:  0.027787446975708008
Epoch:  50 	 Loss:  0.027518749237060547
Epoch:  51 	 Loss:  0.026876691728830338
Epoch:  52 	 Loss:  0.027042552828788757
Epoch:  53 	 Loss:  0.02712870016694069
Epoch:  54 	 Loss:  0.0272272489964962
Epoch:  55 	 Loss:  0.027287643402814865
Epoch:  56 	 Loss:  0.026736924424767494
Epoch:  57 	 Loss:  0.02733786217868328
Epoch:  58 	 Loss:  0.027174334973096848
Epoch:  59 	 Loss:  0.027009014040231705
Epoch:  60 	 Loss:  0.027040738612413406
Epoch:  61 	 Loss:  0.02675764262676239
Epoch:  62 	 Loss:  0.027991464361548424
Epoch:  63 	 Loss:  0.02705417573451996
Epoch:  64 	 Loss:  0.026813607662916183
Epoch:  65 	 Loss:  0.02751508541405201
Epoch:  66 	 Loss:  0.02733803726732731
Epoch:  67 	 Loss:  0.02753356844186783
Epoch:  68 	 Loss:  0.027242427691817284
Epoch:  69 	 Loss:  0.02739095687866211
Epoch:  70 	 Loss:  0.027633849531412125
Epoch:  71 	 Loss:  0.027329232543706894
Epoch:  72 	 Loss:  0.02740851789712906
Epoch:  73 	 Loss:  0.02739950828254223
Epoch:  74 	 Loss:  0.02781742438673973
Epoch:  75 	 Loss:  0.027332650497555733
Epoch:  76 	 Loss:  0.027868879958987236
Epoch:  77 	 Loss:  0.027296656742691994
Epoch:  78 	 Loss:  0.02743983268737793
Epoch:  79 	 Loss:  0.027470437809824944
Epoch:  80 	 Loss:  0.027554301545023918
Epoch:  81 	 Loss:  0.027204422280192375
Epoch:  82 	 Loss:  0.027726279571652412
Epoch:  83 	 Loss:  0.02711942419409752
Epoch:  84 	 Loss:  0.027156280353665352
Epoch:  85 	 Loss:  0.02719629369676113
Epoch:  86 	 Loss:  0.02741974964737892
Epoch:  87 	 Loss:  0.02706429548561573
Epoch:  88 	 Loss:  0.02753487043082714
Epoch:  89 	 Loss:  0.027128659188747406
Epoch:  90 	 Loss:  0.027319224551320076
Epoch:  91 	 Loss:  0.02724309079349041
Epoch:  92 	 Loss:  0.026843702420592308
Epoch:  93 	 Loss:  0.027175508439540863
Epoch:  94 	 Loss:  0.02721480280160904
Epoch:  95 	 Loss:  0.027337776497006416
Epoch:  96 	 Loss:  0.026848549023270607
Epoch:  97 	 Loss:  0.027292508631944656
Epoch:  98 	 Loss:  0.027187641710042953
Epoch:  99 	 Loss:  0.027378197759389877
Epoch:  100 	 Loss:  0.027411924675107002
wandb: Waiting for W&B process to finish, PID 1630252
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142405-a0txg3o6/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142405-a0txg3o6/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02741
wandb:        _step 800000
wandb:     _runtime 86
wandb:   _timestamp 1612898731
wandb:         loss 0.02741
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–†â–†â–ƒâ–†â–„â–†â–ƒâ–‚â–†â–ƒâ–†â–„â–ƒâ–ˆâ–‚â–„â–â–ƒâ–â–â–†â–…â–…â–ˆâ–…â–†â–‡â–„â–ƒâ–„â–‚â–„â–…â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced faithful-sweep-4: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/a0txg3o6
wandb: Agent Starting Run: feog51s6 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:25:37.996604: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:25:38.001168: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run copper-sweep-5
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/feog51s6
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_142536-feog51s6
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027485664933919907
Epoch:  2 	 Loss:  0.027350937947630882
Epoch:  3 	 Loss:  0.0273867417126894
Epoch:  4 	 Loss:  0.02755054086446762
Epoch:  5 	 Loss:  0.027587372809648514
Epoch:  6 	 Loss:  0.027618367224931717
Epoch:  7 	 Loss:  0.027811793610453606
Epoch:  8 	 Loss:  0.027040807530283928
Epoch:  9 	 Loss:  0.027292851358652115
Epoch:  10 	 Loss:  0.02733704075217247
Epoch:  11 	 Loss:  0.027546469122171402
Epoch:  12 	 Loss:  0.027643855661153793
Epoch:  13 	 Loss:  0.027286501601338387
Epoch:  14 	 Loss:  0.02763000875711441
Epoch:  15 	 Loss:  0.02798478864133358
Epoch:  16 	 Loss:  0.02767101116478443
Epoch:  17 	 Loss:  0.027264388278126717
Epoch:  18 	 Loss:  0.027725430205464363
Epoch:  19 	 Loss:  0.027818473055958748
Epoch:  20 	 Loss:  0.027212198823690414
Epoch:  21 	 Loss:  0.02720472402870655
Epoch:  22 	 Loss:  0.027047405019402504
Epoch:  23 	 Loss:  0.027530934661626816
Epoch:  24 	 Loss:  0.027964290231466293
Epoch:  25 	 Loss:  0.027868108823895454
Epoch:  26 	 Loss:  0.027665141969919205
Epoch:  27 	 Loss:  0.027697479352355003
Epoch:  28 	 Loss:  0.027889754623174667
Epoch:  29 	 Loss:  0.027502834796905518
Epoch:  30 	 Loss:  0.027700841426849365
Epoch:  31 	 Loss:  0.027535783126950264
Epoch:  32 	 Loss:  0.02729310840368271
Epoch:  33 	 Loss:  0.0272230077534914
Epoch:  34 	 Loss:  0.02784682996571064
Epoch:  35 	 Loss:  0.027760831639170647
Epoch:  36 	 Loss:  0.027963390573859215
Epoch:  37 	 Loss:  0.027706490829586983
Epoch:  38 	 Loss:  0.027615027502179146
Epoch:  39 	 Loss:  0.027806473895907402
Epoch:  40 	 Loss:  0.02801869437098503
Epoch:  41 	 Loss:  0.027138011530041695
Epoch:  42 	 Loss:  0.028010249137878418
Epoch:  43 	 Loss:  0.0275555569678545
Epoch:  44 	 Loss:  0.027254421263933182
Epoch:  45 	 Loss:  0.02732500247657299
Epoch:  46 	 Loss:  0.027546130120754242
Epoch:  47 	 Loss:  0.027468090876936913
Epoch:  48 	 Loss:  0.02739856205880642
Epoch:  49 	 Loss:  0.027272669598460197
Epoch:  50 	 Loss:  0.027951931580901146
Epoch:  51 	 Loss:  0.02762361615896225
Epoch:  52 	 Loss:  0.027479780837893486
Epoch:  53 	 Loss:  0.028065728023648262
Epoch:  54 	 Loss:  0.027123037725687027
Epoch:  55 	 Loss:  0.027049077674746513
Epoch:  56 	 Loss:  0.02768145129084587
Epoch:  57 	 Loss:  0.027735985815525055
Epoch:  58 	 Loss:  0.027376627549529076
Epoch:  59 	 Loss:  0.027907179668545723
Epoch:  60 	 Loss:  0.027395358309149742
Epoch:  61 	 Loss:  0.0276432354003191
Epoch:  62 	 Loss:  0.027952106669545174
Epoch:  63 	 Loss:  0.02771393209695816
Epoch:  64 	 Loss:  0.02747759036719799
Epoch:  65 	 Loss:  0.027355844154953957
Epoch:  66 	 Loss:  0.027364147827029228
Epoch:  67 	 Loss:  0.027964962646365166
Epoch:  68 	 Loss:  0.028316235169768333
Epoch:  69 	 Loss:  0.02765166200697422
Epoch:  70 	 Loss:  0.02763216197490692
Epoch:  71 	 Loss:  0.02717464230954647
Epoch:  72 	 Loss:  0.02744092419743538
Epoch:  73 	 Loss:  0.027612673118710518
Epoch:  74 	 Loss:  0.026722466573119164
Epoch:  75 	 Loss:  0.02728799730539322
Epoch:  76 	 Loss:  0.02744518220424652
Epoch:  77 	 Loss:  0.028071541339159012
Epoch:  78 	 Loss:  0.02711014449596405
Epoch:  79 	 Loss:  0.027539968490600586
Epoch:  80 	 Loss:  0.0276434738188982
Epoch:  81 	 Loss:  0.0273211058229208
Epoch:  82 	 Loss:  0.02749674953520298
Epoch:  83 	 Loss:  0.02755744941532612
Epoch:  84 	 Loss:  0.027391726151108742
Epoch:  85 	 Loss:  0.027581842616200447
Epoch:  86 	 Loss:  0.027826465666294098
Epoch:  87 	 Loss:  0.027564479038119316
Epoch:  88 	 Loss:  0.02792229875922203
Epoch:  89 	 Loss:  0.027664637193083763
Epoch:  90 	 Loss:  0.02772514522075653
Epoch:  91 	 Loss:  0.027314521372318268
Epoch:  92 	 Loss:  0.0275683905929327
Epoch:  93 	 Loss:  0.02711300551891327
Epoch:  94 	 Loss:  0.027451179921627045
Epoch:  95 	 Loss:  0.027859292924404144
Epoch:  96 	 Loss:  0.027493197470903397
Epoch:  97 	 Loss:  0.02753301151096821
Epoch:  98 	 Loss:  0.027050649747252464
Epoch:  99 	 Loss:  0.027797846123576164
Epoch:  100 	 Loss:  0.02771800383925438
wandb: Waiting for W&B process to finish, PID 1633960
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142536-feog51s6/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142536-feog51s6/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02772
wandb:        _step 800000
wandb:     _runtime 75
wandb:   _timestamp 1612898811
wandb:         loss 0.02772
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–„â–†â–ƒâ–…â–„â–†â–†â–„â–…â–†â–‡â–…â–‡â–‡â–‡â–ƒâ–„â–…â–„â–†â–ƒâ–†â–‡â–†â–…â–‡â–†â–…â–â–ˆâ–…â–…â–„â–…â–†â–…â–…â–…â–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced copper-sweep-5: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/feog51s6
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: xtcxy93s with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:27:11.280731: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:27:11.287866: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run bright-sweep-6
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/xtcxy93s
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_142709-xtcxy93s
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028370743617415428
Epoch:  2 	 Loss:  0.02805302105844021
Epoch:  3 	 Loss:  0.028657963499426842
Epoch:  4 	 Loss:  0.028645703569054604
Epoch:  5 	 Loss:  0.028917457908391953
Epoch:  6 	 Loss:  0.028185393661260605
Epoch:  7 	 Loss:  0.029012953862547874
Epoch:  8 	 Loss:  0.02818210795521736
Epoch:  9 	 Loss:  0.029518740251660347
Epoch:  10 	 Loss:  0.028603320941329002
Epoch:  11 	 Loss:  0.028557972982525826
Epoch:  12 	 Loss:  0.029314622282981873
Epoch:  13 	 Loss:  0.028892379254102707
Epoch:  14 	 Loss:  0.02880599908530712
Epoch:  15 	 Loss:  0.028161268681287766
Epoch:  16 	 Loss:  0.02946004457771778
Epoch:  17 	 Loss:  0.028872095048427582
Epoch:  18 	 Loss:  0.02852228842675686
Epoch:  19 	 Loss:  0.029247742146253586
Epoch:  20 	 Loss:  0.028977403417229652
Epoch:  21 	 Loss:  0.02854040078818798
Epoch:  22 	 Loss:  0.028828855603933334
Epoch:  23 	 Loss:  0.02901727333664894
Epoch:  24 	 Loss:  0.02905907854437828
Epoch:  25 	 Loss:  0.028155647218227386
Epoch:  26 	 Loss:  0.029071418568491936
Epoch:  27 	 Loss:  0.02848128229379654
Epoch:  28 	 Loss:  0.028828734531998634
Epoch:  29 	 Loss:  0.02815682627260685
Epoch:  30 	 Loss:  0.02851751260459423
Epoch:  31 	 Loss:  0.028722969815135002
Epoch:  32 	 Loss:  0.029278604313731194
Epoch:  33 	 Loss:  0.028440222144126892
Epoch:  34 	 Loss:  0.02883884869515896
Epoch:  35 	 Loss:  0.027922850102186203
Epoch:  36 	 Loss:  0.029142703860998154
Epoch:  37 	 Loss:  0.02834395132958889
Epoch:  38 	 Loss:  0.02856414020061493
Epoch:  39 	 Loss:  0.028338151052594185
Epoch:  40 	 Loss:  0.028507158160209656
Epoch:  41 	 Loss:  0.028986996039748192
Epoch:  42 	 Loss:  0.028500575572252274
Epoch:  43 	 Loss:  0.028404371812939644
Epoch:  44 	 Loss:  0.028870567679405212
Epoch:  45 	 Loss:  0.028429348021745682
Epoch:  46 	 Loss:  0.028186889365315437
Epoch:  47 	 Loss:  0.02883000113070011
Epoch:  48 	 Loss:  0.0283411405980587
Epoch:  49 	 Loss:  0.02834915556013584
Epoch:  50 	 Loss:  0.029097726568579674
Epoch:  51 	 Loss:  0.0283797699958086
Epoch:  52 	 Loss:  0.029253840446472168
Epoch:  53 	 Loss:  0.02893191948533058
Epoch:  54 	 Loss:  0.029200604185461998
Epoch:  55 	 Loss:  0.028387995436787605
Epoch:  56 	 Loss:  0.028626693412661552
Epoch:  57 	 Loss:  0.028286144137382507
Epoch:  58 	 Loss:  0.02856607921421528
Epoch:  59 	 Loss:  0.028895821422338486
Epoch:  60 	 Loss:  0.02839575707912445
Epoch:  61 	 Loss:  0.02951575629413128
Epoch:  62 	 Loss:  0.029213936999440193
Epoch:  63 	 Loss:  0.028829656541347504
Epoch:  64 	 Loss:  0.02905881777405739
Epoch:  65 	 Loss:  0.02854338474571705
Epoch:  66 	 Loss:  0.029140161350369453
Epoch:  67 	 Loss:  0.028736434876918793
Epoch:  68 	 Loss:  0.028518080711364746
Epoch:  69 	 Loss:  0.028620677068829536
Epoch:  70 	 Loss:  0.02863934263586998
Epoch:  71 	 Loss:  0.028080463409423828
Epoch:  72 	 Loss:  0.028664162382483482
Epoch:  73 	 Loss:  0.02884371392428875
Epoch:  74 	 Loss:  0.028129329904913902
Epoch:  75 	 Loss:  0.028367184102535248
Epoch:  76 	 Loss:  0.029192470014095306
Epoch:  77 	 Loss:  0.028395051136612892
Epoch:  78 	 Loss:  0.028084473684430122
Epoch:  79 	 Loss:  0.0289762020111084
Epoch:  80 	 Loss:  0.028668459504842758
Epoch:  81 	 Loss:  0.02847377024590969
Epoch:  82 	 Loss:  0.028584424406290054
Epoch:  83 	 Loss:  0.029194112867116928
Epoch:  84 	 Loss:  0.028613077476620674
Epoch:  85 	 Loss:  0.02906099148094654
Epoch:  86 	 Loss:  0.029138917103409767
Epoch:  87 	 Loss:  0.029267875477671623
Epoch:  88 	 Loss:  0.02902277000248432
Epoch:  89 	 Loss:  0.027992531657218933
Epoch:  90 	 Loss:  0.02867889776825905
Epoch:  91 	 Loss:  0.02902865782380104
Epoch:  92 	 Loss:  0.02880660817027092
Epoch:  93 	 Loss:  0.029063839465379715
Epoch:  94 	 Loss:  0.028878267854452133
Epoch:  95 	 Loss:  0.02832043170928955
Epoch:  96 	 Loss:  0.029058456420898438
Epoch:  97 	 Loss:  0.028652604669332504
Epoch:  98 	 Loss:  0.028166821226477623
Epoch:  99 	 Loss:  0.0286739282310009
Epoch:  100 	 Loss:  0.029300348833203316
wandb: Waiting for W&B process to finish, PID 1637686
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142709-xtcxy93s/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142709-xtcxy93s/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0293
wandb:        _step 800000
wandb:     _runtime 80
wandb:   _timestamp 1612898909
wandb:         loss 0.0293
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–ƒâ–„â–‚â–‚â–„â–…â–ˆâ–ƒâ–„â–†â–†â–…â–„â–…â–†â–ƒâ–†â–…â–‚â–ƒâ–ƒâ–‡â–„â–…â–ˆâ–†â–„â–„â–„â–‚â–ƒâ–†â–„â–„â–‡â–â–…â–…â–„â–‡
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced bright-sweep-6: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/xtcxy93s
wandb: Agent Starting Run: jnmn9whi with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:28:35.463199: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:28:35.468289: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run fast-sweep-7
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/jnmn9whi
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_142833-jnmn9whi
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.029154475778341293
Epoch:  2 	 Loss:  0.02956824004650116
Epoch:  3 	 Loss:  0.02898917719721794
Epoch:  4 	 Loss:  0.02876075729727745
Epoch:  5 	 Loss:  0.028875574469566345
Epoch:  6 	 Loss:  0.029590368270874023
Epoch:  7 	 Loss:  0.029875969514250755
Epoch:  8 	 Loss:  0.029399501159787178
Epoch:  9 	 Loss:  0.029119595885276794
Epoch:  10 	 Loss:  0.029125457629561424
Epoch:  11 	 Loss:  0.02930600382387638
Epoch:  12 	 Loss:  0.029150348156690598
Epoch:  13 	 Loss:  0.02929903008043766
Epoch:  14 	 Loss:  0.029392605647444725
Epoch:  15 	 Loss:  0.029240407049655914
Epoch:  16 	 Loss:  0.029519356787204742
Epoch:  17 	 Loss:  0.02935103327035904
Epoch:  18 	 Loss:  0.029055790975689888
Epoch:  19 	 Loss:  0.02915123850107193
Epoch:  20 	 Loss:  0.02945607341825962
Epoch:  21 	 Loss:  0.029282253235578537
Epoch:  22 	 Loss:  0.02901584841310978
Epoch:  23 	 Loss:  0.028890343382954597
Epoch:  24 	 Loss:  0.029090963304042816
Epoch:  25 	 Loss:  0.02920929715037346
Epoch:  26 	 Loss:  0.028886111453175545
Epoch:  27 	 Loss:  0.029635122045874596
Epoch:  28 	 Loss:  0.029707513749599457
Epoch:  29 	 Loss:  0.02907481975853443
Epoch:  30 	 Loss:  0.029359780251979828
Epoch:  31 	 Loss:  0.028956811875104904
Epoch:  32 	 Loss:  0.028978608548641205
Epoch:  33 	 Loss:  0.029338674619793892
Epoch:  34 	 Loss:  0.028654051944613457
Epoch:  35 	 Loss:  0.029180901125073433
Epoch:  36 	 Loss:  0.029090341180562973
Epoch:  37 	 Loss:  0.029175564646720886
Epoch:  38 	 Loss:  0.028984354808926582
Epoch:  39 	 Loss:  0.029281701892614365
Epoch:  40 	 Loss:  0.02947591431438923
Epoch:  41 	 Loss:  0.028660310432314873
Epoch:  42 	 Loss:  0.029600469395518303
Epoch:  43 	 Loss:  0.029264839366078377
Epoch:  44 	 Loss:  0.029820941388607025
Epoch:  45 	 Loss:  0.02923864684998989
Epoch:  46 	 Loss:  0.02868129126727581
Epoch:  47 	 Loss:  0.029141392558813095
Epoch:  48 	 Loss:  0.029140302911400795
Epoch:  49 	 Loss:  0.02959897369146347
Epoch:  50 	 Loss:  0.029316093772649765
Epoch:  51 	 Loss:  0.029616083949804306
Epoch:  52 	 Loss:  0.02879294566810131
Epoch:  53 	 Loss:  0.029735051095485687
Epoch:  54 	 Loss:  0.029311569407582283
Epoch:  55 	 Loss:  0.029809625819325447
Epoch:  56 	 Loss:  0.029068341478705406
Epoch:  57 	 Loss:  0.029663877561688423
Epoch:  58 	 Loss:  0.029585128650069237
Epoch:  59 	 Loss:  0.029601072892546654
Epoch:  60 	 Loss:  0.029093893244862556
Epoch:  61 	 Loss:  0.029352085664868355
Epoch:  62 	 Loss:  0.02936447225511074
Epoch:  63 	 Loss:  0.02950536645948887
Epoch:  64 	 Loss:  0.028655853122472763
Epoch:  65 	 Loss:  0.0293416790664196
Epoch:  66 	 Loss:  0.029793206602334976
Epoch:  67 	 Loss:  0.029644237831234932
Epoch:  68 	 Loss:  0.02917085774242878
Epoch:  69 	 Loss:  0.029523473232984543
Epoch:  70 	 Loss:  0.029461421072483063
Epoch:  71 	 Loss:  0.029525544494390488
Epoch:  72 	 Loss:  0.028957413509488106
Epoch:  73 	 Loss:  0.02888840064406395
Epoch:  74 	 Loss:  0.029135145246982574
Epoch:  75 	 Loss:  0.029168670997023582
Epoch:  76 	 Loss:  0.029339011758565903
Epoch:  77 	 Loss:  0.02891683578491211
Epoch:  78 	 Loss:  0.029139578342437744
Epoch:  79 	 Loss:  0.02974828891456127
Epoch:  80 	 Loss:  0.029112035408616066
Epoch:  81 	 Loss:  0.02909746579825878
Epoch:  82 	 Loss:  0.029359767213463783
Epoch:  83 	 Loss:  0.029754651710391045
Epoch:  84 	 Loss:  0.02906077168881893
Epoch:  85 	 Loss:  0.028757071122527122
Epoch:  86 	 Loss:  0.028193848207592964
Epoch:  87 	 Loss:  0.029742615297436714
Epoch:  88 	 Loss:  0.029451169073581696
Epoch:  89 	 Loss:  0.028722304850816727
Epoch:  90 	 Loss:  0.02965502440929413
Epoch:  91 	 Loss:  0.028978081420063972
Epoch:  92 	 Loss:  0.029079632833600044
Epoch:  93 	 Loss:  0.028499452397227287
Epoch:  94 	 Loss:  0.028880175203084946
Epoch:  95 	 Loss:  0.029307512566447258
Epoch:  96 	 Loss:  0.029467297717928886
Epoch:  97 	 Loss:  0.029248425737023354
Epoch:  98 	 Loss:  0.029598964378237724
Epoch:  99 	 Loss:  0.029611242935061455
Epoch:  100 	 Loss:  0.02938244305551052
wandb: Waiting for W&B process to finish, PID 1641368
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142833-jnmn9whi/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142833-jnmn9whi/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02938
wandb:        _step 800000
wandb:     _runtime 79
wandb:   _timestamp 1612898992
wandb:         loss 0.02938
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–ƒâ–‡â–…â–…â–…â–†â–ƒâ–…â–‚â–‚â–‡â–ƒâ–â–„â–…â–â–ˆâ–â–‡â–‡â–…â–ƒâ–‡â–…â–â–‡â–†â–ƒâ–„â–ƒâ–ˆâ–…â–ƒâ–ˆâ–â–„â–‚â–…â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced fast-sweep-7: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/jnmn9whi
wandb: Agent Starting Run: eyfom3sx with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:29:58.832887: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:29:58.840107: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run ethereal-sweep-8
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/eyfom3sx
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_142956-eyfom3sx
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02769842930138111
Epoch:  2 	 Loss:  0.027533801272511482
Epoch:  3 	 Loss:  0.027291681617498398
Epoch:  4 	 Loss:  0.027766065672039986
Epoch:  5 	 Loss:  0.02719838358461857
Epoch:  6 	 Loss:  0.02762940712273121
Epoch:  7 	 Loss:  0.026890140026807785
Epoch:  8 	 Loss:  0.027277927845716476
Epoch:  9 	 Loss:  0.027034644037485123
Epoch:  10 	 Loss:  0.028029413893818855
Epoch:  11 	 Loss:  0.02771139144897461
Epoch:  12 	 Loss:  0.027369994670152664
Epoch:  13 	 Loss:  0.027440780773758888
Epoch:  14 	 Loss:  0.027568763121962547
Epoch:  15 	 Loss:  0.027209987863898277
Epoch:  16 	 Loss:  0.027547772973775864
Epoch:  17 	 Loss:  0.027709785848855972
Epoch:  18 	 Loss:  0.027036886662244797
Epoch:  19 	 Loss:  0.027567587792873383
Epoch:  20 	 Loss:  0.027564551681280136
Epoch:  21 	 Loss:  0.027509381994605064
Epoch:  22 	 Loss:  0.027769770473241806
Epoch:  23 	 Loss:  0.027435874566435814
Epoch:  24 	 Loss:  0.027683084830641747
Epoch:  25 	 Loss:  0.02748659811913967
Epoch:  26 	 Loss:  0.027484692633152008
Epoch:  27 	 Loss:  0.0278526172041893
Epoch:  28 	 Loss:  0.02771754376590252
Epoch:  29 	 Loss:  0.027849065139889717
Epoch:  30 	 Loss:  0.027624711394309998
Epoch:  31 	 Loss:  0.027132580056786537
Epoch:  32 	 Loss:  0.027693647891283035
Epoch:  33 	 Loss:  0.0278298482298851
Epoch:  34 	 Loss:  0.027927063405513763
Epoch:  35 	 Loss:  0.027535544708371162
Epoch:  36 	 Loss:  0.02733566239476204
Epoch:  37 	 Loss:  0.027366725727915764
Epoch:  38 	 Loss:  0.02790229208767414
Epoch:  39 	 Loss:  0.027673909440636635
Epoch:  40 	 Loss:  0.02785656973719597
Epoch:  41 	 Loss:  0.02791844867169857
Epoch:  42 	 Loss:  0.027207951992750168
Epoch:  43 	 Loss:  0.02766036055982113
Epoch:  44 	 Loss:  0.02710445411503315
Epoch:  45 	 Loss:  0.028144899755716324
Epoch:  46 	 Loss:  0.027589546516537666
Epoch:  47 	 Loss:  0.02723090350627899
Epoch:  48 	 Loss:  0.027113482356071472
Epoch:  49 	 Loss:  0.027942143380641937
Epoch:  50 	 Loss:  0.02748377062380314
Epoch:  51 	 Loss:  0.027925504371523857
Epoch:  52 	 Loss:  0.027668055146932602
Epoch:  53 	 Loss:  0.02729468047618866
Epoch:  54 	 Loss:  0.027552925050258636
Epoch:  55 	 Loss:  0.027439076453447342
Epoch:  56 	 Loss:  0.027184687554836273
Epoch:  57 	 Loss:  0.027668099850416183
Epoch:  58 	 Loss:  0.028139416128396988
Epoch:  59 	 Loss:  0.0277180727571249
Epoch:  60 	 Loss:  0.027286378666758537
Epoch:  61 	 Loss:  0.027473220601677895
Epoch:  62 	 Loss:  0.02772301807999611
Epoch:  63 	 Loss:  0.02827191911637783
Epoch:  64 	 Loss:  0.027738254517316818
Epoch:  65 	 Loss:  0.02727416716516018
Epoch:  66 	 Loss:  0.02822599932551384
Epoch:  67 	 Loss:  0.027437040582299232
Epoch:  68 	 Loss:  0.02760743722319603
Epoch:  69 	 Loss:  0.02769605442881584
Epoch:  70 	 Loss:  0.027409490197896957
Epoch:  71 	 Loss:  0.0271820779889822
Epoch:  72 	 Loss:  0.027411768212914467
Epoch:  73 	 Loss:  0.02761545591056347
Epoch:  74 	 Loss:  0.02795184589922428
Epoch:  75 	 Loss:  0.026779329404234886
Epoch:  76 	 Loss:  0.027670850977301598
Epoch:  77 	 Loss:  0.027745898813009262
Epoch:  78 	 Loss:  0.027590379118919373
Epoch:  79 	 Loss:  0.027750052511692047
Epoch:  80 	 Loss:  0.02680063620209694
Epoch:  81 	 Loss:  0.028074026107788086
Epoch:  82 	 Loss:  0.027152137830853462
Epoch:  83 	 Loss:  0.027413543313741684
Epoch:  84 	 Loss:  0.027610164135694504
Epoch:  85 	 Loss:  0.02709137834608555
Epoch:  86 	 Loss:  0.027474848553538322
Epoch:  87 	 Loss:  0.027448369190096855
Epoch:  88 	 Loss:  0.027817007154226303
Epoch:  89 	 Loss:  0.027871718630194664
Epoch:  90 	 Loss:  0.027800792828202248
Epoch:  91 	 Loss:  0.027924932539463043
Epoch:  92 	 Loss:  0.027219722047448158
Epoch:  93 	 Loss:  0.02742854505777359
Epoch:  94 	 Loss:  0.026712831109762192
Epoch:  95 	 Loss:  0.0276300348341465
Epoch:  96 	 Loss:  0.027730409055948257
Epoch:  97 	 Loss:  0.027701249346137047
Epoch:  98 	 Loss:  0.027529623359441757
Epoch:  99 	 Loss:  0.02806602604687214
Epoch:  100 	 Loss:  0.02735535055398941
wandb: Waiting for W&B process to finish, PID 1645052
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142956-eyfom3sx/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_142956-eyfom3sx/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02736
wandb:        _step 800000
wandb:     _runtime 79
wandb:   _timestamp 1612899076
wandb:         loss 0.02736
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–‡â–„â–†â–„â–‡â–…â–†â–ƒâ–†â–…â–…â–‡â–ƒâ–ˆâ–…â–†â–ˆâ–ƒâ–†â–ˆâ–ˆâ–†â–„â–‡â–…â–‡â–…â–‡â–…â–ˆâ–‡â–‡â–ƒâ–†â–…â–ˆâ–„â–â–‡â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced ethereal-sweep-8: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/eyfom3sx
wandb: Agent Starting Run: zda6q10j with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:31:27.286532: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:31:27.292112: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run super-sweep-9
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/zda6q10j
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_143125-zda6q10j
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02961045131087303
Epoch:  2 	 Loss:  0.030009008944034576
Epoch:  3 	 Loss:  0.029747454449534416
Epoch:  4 	 Loss:  0.029583968222141266
Epoch:  5 	 Loss:  0.02933337911963463
Epoch:  6 	 Loss:  0.029842935502529144
Epoch:  7 	 Loss:  0.029680415987968445
Epoch:  8 	 Loss:  0.029788943007588387
Epoch:  9 	 Loss:  0.03021484613418579
Epoch:  10 	 Loss:  0.029550565406680107
Epoch:  11 	 Loss:  0.029056236147880554
Epoch:  12 	 Loss:  0.029813803732395172
Epoch:  13 	 Loss:  0.02933899685740471
Epoch:  14 	 Loss:  0.02997693233191967
Epoch:  15 	 Loss:  0.030270956456661224
Epoch:  16 	 Loss:  0.02963101491332054
Epoch:  17 	 Loss:  0.030039314180612564
Epoch:  18 	 Loss:  0.029292909428477287
Epoch:  19 	 Loss:  0.02908407710492611
Epoch:  20 	 Loss:  0.029476862400770187
Epoch:  21 	 Loss:  0.029416248202323914
Epoch:  22 	 Loss:  0.029043348506093025
Epoch:  23 	 Loss:  0.029595017433166504
Epoch:  24 	 Loss:  0.030311044305562973
Epoch:  25 	 Loss:  0.029970433562994003
Epoch:  26 	 Loss:  0.029521258547902107
Epoch:  27 	 Loss:  0.029441649094223976
Epoch:  28 	 Loss:  0.028962474316358566
Epoch:  29 	 Loss:  0.0300056803971529
Epoch:  30 	 Loss:  0.02984730154275894
Epoch:  31 	 Loss:  0.029255494475364685
Epoch:  32 	 Loss:  0.029337525367736816
Epoch:  33 	 Loss:  0.030212050303816795
Epoch:  34 	 Loss:  0.029356272891163826
Epoch:  35 	 Loss:  0.028971418738365173
Epoch:  36 	 Loss:  0.029799053445458412
Epoch:  37 	 Loss:  0.030534105375409126
Epoch:  38 	 Loss:  0.029867153614759445
Epoch:  39 	 Loss:  0.030012959614396095
Epoch:  40 	 Loss:  0.029214708134531975
Epoch:  41 	 Loss:  0.030467363074421883
Epoch:  42 	 Loss:  0.02958090417087078
Epoch:  43 	 Loss:  0.029533831402659416
Epoch:  44 	 Loss:  0.029982300475239754
Epoch:  45 	 Loss:  0.03015238419175148
Epoch:  46 	 Loss:  0.029820505529642105
Epoch:  47 	 Loss:  0.02951856143772602
Epoch:  48 	 Loss:  0.029903685674071312
Epoch:  49 	 Loss:  0.029868025332689285
Epoch:  50 	 Loss:  0.030164116993546486
Epoch:  51 	 Loss:  0.02999250777065754
Epoch:  52 	 Loss:  0.029955148696899414
Epoch:  53 	 Loss:  0.029842885211110115
Epoch:  54 	 Loss:  0.02998633123934269
Epoch:  55 	 Loss:  0.03021579049527645
Epoch:  56 	 Loss:  0.029962539672851562
Epoch:  57 	 Loss:  0.029990704730153084
Epoch:  58 	 Loss:  0.029560400173068047
Epoch:  59 	 Loss:  0.02974710427224636
Epoch:  60 	 Loss:  0.029591020196676254
Epoch:  61 	 Loss:  0.02985345758497715
Epoch:  62 	 Loss:  0.03067660890519619
Epoch:  63 	 Loss:  0.029180223122239113
Epoch:  64 	 Loss:  0.03033294901251793
Epoch:  65 	 Loss:  0.029969472438097
Epoch:  66 	 Loss:  0.029012000188231468
Epoch:  67 	 Loss:  0.030223676934838295
Epoch:  68 	 Loss:  0.029604660347104073
Epoch:  69 	 Loss:  0.030642064288258553
Epoch:  70 	 Loss:  0.030424952507019043
Epoch:  71 	 Loss:  0.030287068337202072
Epoch:  72 	 Loss:  0.02965301275253296
Epoch:  73 	 Loss:  0.0293709859251976
Epoch:  74 	 Loss:  0.030168777331709862
Epoch:  75 	 Loss:  0.029944244772195816
Epoch:  76 	 Loss:  0.029492812231183052
Epoch:  77 	 Loss:  0.029977958649396896
Epoch:  78 	 Loss:  0.030312662944197655
Epoch:  79 	 Loss:  0.029803620651364326
Epoch:  80 	 Loss:  0.0300687737762928
Epoch:  81 	 Loss:  0.028961675241589546
Epoch:  82 	 Loss:  0.029794957488775253
Epoch:  83 	 Loss:  0.02995109185576439
Epoch:  84 	 Loss:  0.030338125303387642
Epoch:  85 	 Loss:  0.0306024681776762
Epoch:  86 	 Loss:  0.030810020864009857
Epoch:  87 	 Loss:  0.030407974496483803
Epoch:  88 	 Loss:  0.029842006042599678
Epoch:  89 	 Loss:  0.02927669696509838
Epoch:  90 	 Loss:  0.028873512521386147
Epoch:  91 	 Loss:  0.02961147390305996
Epoch:  92 	 Loss:  0.029287545010447502
Epoch:  93 	 Loss:  0.03055047057569027
Epoch:  94 	 Loss:  0.029634730890393257
Epoch:  95 	 Loss:  0.029908958822488785
Epoch:  96 	 Loss:  0.029959922656416893
Epoch:  97 	 Loss:  0.030250411480665207
Epoch:  98 	 Loss:  0.02955879084765911
Epoch:  99 	 Loss:  0.030148079618811607
Epoch:  100 	 Loss:  0.029186001047492027
wandb: Waiting for W&B process to finish, PID 1648742
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143125-zda6q10j/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143125-zda6q10j/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02919
wandb:        _step 800000
wandb:     _runtime 84
wandb:   _timestamp 1612899169
wandb:         loss 0.02919
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–„â–…â–„â–â–ƒâ–„â–‚â–ƒâ–„â–ƒâ–â–‚â–ƒâ–„â–…â–‡â–…â–…â–…â–…â–…â–…â–„â–…â–‡â–†â–ˆâ–„â–†â–…â–…â–„â–‡â–‡â–‚â–‚â–„â–†â–‚
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced super-sweep-9: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/zda6q10j
wandb: Agent Starting Run: ymo12e39 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:32:55.511999: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:32:55.517684: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run happy-sweep-10
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/ymo12e39
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_143253-ymo12e39
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.029257353395223618
Epoch:  2 	 Loss:  0.028885358944535255
Epoch:  3 	 Loss:  0.029558997601270676
Epoch:  4 	 Loss:  0.02916727028787136
Epoch:  5 	 Loss:  0.02819431759417057
Epoch:  6 	 Loss:  0.02901039645075798
Epoch:  7 	 Loss:  0.028836753219366074
Epoch:  8 	 Loss:  0.0293683223426342
Epoch:  9 	 Loss:  0.028954483568668365
Epoch:  10 	 Loss:  0.02923259697854519
Epoch:  11 	 Loss:  0.02898511104285717
Epoch:  12 	 Loss:  0.029277421534061432
Epoch:  13 	 Loss:  0.028687734156847
Epoch:  14 	 Loss:  0.028886254876852036
Epoch:  15 	 Loss:  0.028867460787296295
Epoch:  16 	 Loss:  0.02927822433412075
Epoch:  17 	 Loss:  0.028832830488681793
Epoch:  18 	 Loss:  0.029021240770816803
Epoch:  19 	 Loss:  0.029028410091996193
Epoch:  20 	 Loss:  0.028853219002485275
Epoch:  21 	 Loss:  0.029434598982334137
Epoch:  22 	 Loss:  0.029117152094841003
Epoch:  23 	 Loss:  0.028904959559440613
Epoch:  24 	 Loss:  0.02865658327937126
Epoch:  25 	 Loss:  0.029113594442605972
Epoch:  26 	 Loss:  0.02888392098248005
Epoch:  27 	 Loss:  0.028788868337869644
Epoch:  28 	 Loss:  0.028607124462723732
Epoch:  29 	 Loss:  0.028987571597099304
Epoch:  30 	 Loss:  0.028810802847146988
Epoch:  31 	 Loss:  0.028996411710977554
Epoch:  32 	 Loss:  0.029354190453886986
Epoch:  33 	 Loss:  0.029332078993320465
Epoch:  34 	 Loss:  0.029305288568139076
Epoch:  35 	 Loss:  0.029013385996222496
Epoch:  36 	 Loss:  0.0278179831802845
Epoch:  37 	 Loss:  0.02881719544529915
Epoch:  38 	 Loss:  0.02888082154095173
Epoch:  39 	 Loss:  0.02860438823699951
Epoch:  40 	 Loss:  0.02886614389717579
Epoch:  41 	 Loss:  0.02965879812836647
Epoch:  42 	 Loss:  0.028591224923729897
Epoch:  43 	 Loss:  0.029387671500444412
Epoch:  44 	 Loss:  0.02868080697953701
Epoch:  45 	 Loss:  0.0287505853921175
Epoch:  46 	 Loss:  0.02937442623078823
Epoch:  47 	 Loss:  0.02890682779252529
Epoch:  48 	 Loss:  0.028585340827703476
Epoch:  49 	 Loss:  0.029161328449845314
Epoch:  50 	 Loss:  0.02922956645488739
Epoch:  51 	 Loss:  0.02897101268172264
Epoch:  52 	 Loss:  0.028527479618787766
Epoch:  53 	 Loss:  0.02915726788341999
Epoch:  54 	 Loss:  0.02896999754011631
Epoch:  55 	 Loss:  0.028834355995059013
Epoch:  56 	 Loss:  0.02910281904041767
Epoch:  57 	 Loss:  0.028790408745408058
Epoch:  58 	 Loss:  0.028612690046429634
Epoch:  59 	 Loss:  0.0291911493986845
Epoch:  60 	 Loss:  0.02917730249464512
Epoch:  61 	 Loss:  0.028961842879652977
Epoch:  62 	 Loss:  0.028736449778079987
Epoch:  63 	 Loss:  0.02873948961496353
Epoch:  64 	 Loss:  0.028844375163316727
Epoch:  65 	 Loss:  0.028936313465237617
Epoch:  66 	 Loss:  0.029674800112843513
Epoch:  67 	 Loss:  0.02876603975892067
Epoch:  68 	 Loss:  0.02846965752542019
Epoch:  69 	 Loss:  0.028146706521511078
Epoch:  70 	 Loss:  0.028958145529031754
Epoch:  71 	 Loss:  0.029325511306524277
Epoch:  72 	 Loss:  0.02776200883090496
Epoch:  73 	 Loss:  0.028432514518499374
Epoch:  74 	 Loss:  0.028525369241833687
Epoch:  75 	 Loss:  0.028800973668694496
Epoch:  76 	 Loss:  0.02826293557882309
Epoch:  77 	 Loss:  0.028953157365322113
Epoch:  78 	 Loss:  0.029254283756017685
Epoch:  79 	 Loss:  0.02871880494058132
Epoch:  80 	 Loss:  0.028491295874118805
Epoch:  81 	 Loss:  0.02897128276526928
Epoch:  82 	 Loss:  0.02931707724928856
Epoch:  83 	 Loss:  0.028625519946217537
Epoch:  84 	 Loss:  0.029126767069101334
Epoch:  85 	 Loss:  0.028578607365489006
Epoch:  86 	 Loss:  0.028955766931176186
Epoch:  87 	 Loss:  0.028629120439291
Epoch:  88 	 Loss:  0.02927553653717041
Epoch:  89 	 Loss:  0.028688710182905197
Epoch:  90 	 Loss:  0.02932012639939785
Epoch:  91 	 Loss:  0.02881353721022606
Epoch:  92 	 Loss:  0.02921878732740879
Epoch:  93 	 Loss:  0.029512066394090652
Epoch:  94 	 Loss:  0.028813710436224937
Epoch:  95 	 Loss:  0.02883416786789894
Epoch:  96 	 Loss:  0.02860400639474392
Epoch:  97 	 Loss:  0.02904711849987507
Epoch:  98 	 Loss:  0.028737137094140053
Epoch:  99 	 Loss:  0.028823664411902428
Epoch:  100 	 Loss:  0.029198292642831802
wandb: Waiting for W&B process to finish, PID 1652452
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143253-ymo12e39/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143253-ymo12e39/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0292
wandb:        _step 800000
wandb:     _runtime 85
wandb:   _timestamp 1612899258
wandb:         loss 0.0292
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–‡â–ˆâ–†â–‡â–†â–„â–‡â–†â–‡â–…â–…â–„â–†â–‡â–â–„â–ˆâ–„â–‡â–†â–…â–…â–†â–†â–…â–…â–…â–‚â–â–„â–…â–…â–‡â–†â–„â–„â–†â–…â–†â–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced happy-sweep-10: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/ymo12e39
wandb: Agent Starting Run: wg7rt5vf with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:34:26.072375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:34:26.078936: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run easy-sweep-11
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/wg7rt5vf
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_143424-wg7rt5vf
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.029192673042416573
Epoch:  2 	 Loss:  0.0292044747620821
Epoch:  3 	 Loss:  0.029087025672197342
Epoch:  4 	 Loss:  0.02818397432565689
Epoch:  5 	 Loss:  0.028552280738949776
Epoch:  6 	 Loss:  0.02928309701383114
Epoch:  7 	 Loss:  0.02858535200357437
Epoch:  8 	 Loss:  0.028807109221816063
Epoch:  9 	 Loss:  0.02891063317656517
Epoch:  10 	 Loss:  0.028847726061940193
Epoch:  11 	 Loss:  0.028898973017930984
Epoch:  12 	 Loss:  0.029216120019555092
Epoch:  13 	 Loss:  0.028678087517619133
Epoch:  14 	 Loss:  0.029003042727708817
Epoch:  15 	 Loss:  0.02890896238386631
Epoch:  16 	 Loss:  0.02889423258602619
Epoch:  17 	 Loss:  0.02868085727095604
Epoch:  18 	 Loss:  0.028552820906043053
Epoch:  19 	 Loss:  0.028786512091755867
Epoch:  20 	 Loss:  0.028980281203985214
Epoch:  21 	 Loss:  0.028958452865481377
Epoch:  22 	 Loss:  0.028507288545370102
Epoch:  23 	 Loss:  0.02861584536731243
Epoch:  24 	 Loss:  0.028993651270866394
Epoch:  25 	 Loss:  0.028219643980264664
Epoch:  26 	 Loss:  0.02846594527363777
Epoch:  27 	 Loss:  0.028717417269945145
Epoch:  28 	 Loss:  0.029131092131137848
Epoch:  29 	 Loss:  0.029032666236162186
Epoch:  30 	 Loss:  0.029151903465390205
Epoch:  31 	 Loss:  0.0289989672601223
Epoch:  32 	 Loss:  0.028545338660478592
Epoch:  33 	 Loss:  0.028383882716298103
Epoch:  34 	 Loss:  0.029284214600920677
Epoch:  35 	 Loss:  0.02862626686692238
Epoch:  36 	 Loss:  0.02922659181058407
Epoch:  37 	 Loss:  0.029759801924228668
Epoch:  38 	 Loss:  0.029226329177618027
Epoch:  39 	 Loss:  0.028837837278842926
Epoch:  40 	 Loss:  0.027917472645640373
Epoch:  41 	 Loss:  0.028797563165426254
Epoch:  42 	 Loss:  0.02825096994638443
Epoch:  43 	 Loss:  0.028328634798526764
Epoch:  44 	 Loss:  0.029225394129753113
Epoch:  45 	 Loss:  0.02844635210931301
Epoch:  46 	 Loss:  0.029129482805728912
Epoch:  47 	 Loss:  0.029286736622452736
Epoch:  48 	 Loss:  0.02945679984986782
Epoch:  49 	 Loss:  0.029241511598229408
Epoch:  50 	 Loss:  0.028526723384857178
Epoch:  51 	 Loss:  0.02926468290388584
Epoch:  52 	 Loss:  0.028538648039102554
Epoch:  53 	 Loss:  0.02908547967672348
Epoch:  54 	 Loss:  0.02901865914463997
Epoch:  55 	 Loss:  0.028650810942053795
Epoch:  56 	 Loss:  0.028406715020537376
Epoch:  57 	 Loss:  0.028706248849630356
Epoch:  58 	 Loss:  0.02895130030810833
Epoch:  59 	 Loss:  0.028873952105641365
Epoch:  60 	 Loss:  0.028463896363973618
Epoch:  61 	 Loss:  0.028848253190517426
Epoch:  62 	 Loss:  0.02847234159708023
Epoch:  63 	 Loss:  0.02894955314695835
Epoch:  64 	 Loss:  0.028945891186594963
Epoch:  65 	 Loss:  0.02959858812391758
Epoch:  66 	 Loss:  0.02899150736629963
Epoch:  67 	 Loss:  0.029006902128458023
Epoch:  68 	 Loss:  0.02839898131787777
Epoch:  69 	 Loss:  0.028905034065246582
Epoch:  70 	 Loss:  0.028727348893880844
Epoch:  71 	 Loss:  0.0284558217972517
Epoch:  72 	 Loss:  0.028993725776672363
Epoch:  73 	 Loss:  0.028117023408412933
Epoch:  74 	 Loss:  0.028246218338608742
Epoch:  75 	 Loss:  0.028923625126481056
Epoch:  76 	 Loss:  0.029289547353982925
Epoch:  77 	 Loss:  0.028963284566998482
Epoch:  78 	 Loss:  0.02935107797384262
Epoch:  79 	 Loss:  0.029280414804816246
Epoch:  80 	 Loss:  0.028641249984502792
Epoch:  81 	 Loss:  0.028350604698061943
Epoch:  82 	 Loss:  0.029280085116624832
Epoch:  83 	 Loss:  0.028456280007958412
Epoch:  84 	 Loss:  0.028956569731235504
Epoch:  85 	 Loss:  0.028604740276932716
Epoch:  86 	 Loss:  0.028994601219892502
Epoch:  87 	 Loss:  0.02888096682727337
Epoch:  88 	 Loss:  0.029169145971536636
Epoch:  89 	 Loss:  0.028877221047878265
Epoch:  90 	 Loss:  0.028759153559803963
Epoch:  91 	 Loss:  0.029249774292111397
Epoch:  92 	 Loss:  0.028543105348944664
Epoch:  93 	 Loss:  0.028481891378760338
Epoch:  94 	 Loss:  0.028399884700775146
Epoch:  95 	 Loss:  0.028014324605464935
Epoch:  96 	 Loss:  0.028102554380893707
Epoch:  97 	 Loss:  0.029227783903479576
Epoch:  98 	 Loss:  0.029222043231129646
Epoch:  99 	 Loss:  0.028654858469963074
Epoch:  100 	 Loss:  0.029271332547068596
wandb: Waiting for W&B process to finish, PID 1656161
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143424-wg7rt5vf/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143424-wg7rt5vf/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02927
wandb:        _step 800000
wandb:     _runtime 82
wandb:   _timestamp 1612899346
wandb:         loss 0.02927
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–‡â–‡â–ˆâ–…â–…â–„â–…â–ƒâ–†â–ƒâ–‚â–‡â–†â–ˆâ–ˆâ–…â–…â–ˆâ–‡â–ˆâ–ˆâ–†â–‚â–…â–…â–†â–†â–…â–†â–â–†â–ˆâ–ˆâ–†â–…â–…â–ƒâ–‚â–ˆâ–ˆ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced easy-sweep-11: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/wg7rt5vf
wandb: Agent Starting Run: d45abh5y with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:35:52.849509: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:35:52.854604: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run frosty-sweep-12
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/d45abh5y
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_143550-d45abh5y
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02853420190513134
Epoch:  2 	 Loss:  0.027763085439801216
Epoch:  3 	 Loss:  0.028729209676384926
Epoch:  4 	 Loss:  0.028487494215369225
Epoch:  5 	 Loss:  0.029109811410307884
Epoch:  6 	 Loss:  0.027863241732120514
Epoch:  7 	 Loss:  0.02832730859518051
Epoch:  8 	 Loss:  0.02814180590212345
Epoch:  9 	 Loss:  0.028489721938967705
Epoch:  10 	 Loss:  0.028492087498307228
Epoch:  11 	 Loss:  0.027993585914373398
Epoch:  12 	 Loss:  0.028555816039443016
Epoch:  13 	 Loss:  0.028445487841963768
Epoch:  14 	 Loss:  0.02871433086693287
Epoch:  15 	 Loss:  0.028812583535909653
Epoch:  16 	 Loss:  0.02814595401287079
Epoch:  17 	 Loss:  0.02875913493335247
Epoch:  18 	 Loss:  0.027853067964315414
Epoch:  19 	 Loss:  0.02797619253396988
Epoch:  20 	 Loss:  0.028198104351758957
Epoch:  21 	 Loss:  0.02883046492934227
Epoch:  22 	 Loss:  0.02762279473245144
Epoch:  23 	 Loss:  0.02834262326359749
Epoch:  24 	 Loss:  0.028650974854826927
Epoch:  25 	 Loss:  0.02852468751370907
Epoch:  26 	 Loss:  0.0279474388808012
Epoch:  27 	 Loss:  0.02834714762866497
Epoch:  28 	 Loss:  0.028435036540031433
Epoch:  29 	 Loss:  0.02833174169063568
Epoch:  30 	 Loss:  0.028624245896935463
Epoch:  31 	 Loss:  0.027868784964084625
Epoch:  32 	 Loss:  0.028712360188364983
Epoch:  33 	 Loss:  0.02814125455915928
Epoch:  34 	 Loss:  0.028284819796681404
Epoch:  35 	 Loss:  0.028675775974988937
Epoch:  36 	 Loss:  0.02846105583012104
Epoch:  37 	 Loss:  0.028370287269353867
Epoch:  38 	 Loss:  0.028562072664499283
Epoch:  39 	 Loss:  0.027795271947979927
Epoch:  40 	 Loss:  0.028827277943491936
Epoch:  41 	 Loss:  0.028467940166592598
Epoch:  42 	 Loss:  0.028014641255140305
Epoch:  43 	 Loss:  0.028113635256886482
Epoch:  44 	 Loss:  0.02817317098379135
Epoch:  45 	 Loss:  0.028071962296962738
Epoch:  46 	 Loss:  0.02815941721200943
Epoch:  47 	 Loss:  0.02820141799747944
Epoch:  48 	 Loss:  0.028227323666214943
Epoch:  49 	 Loss:  0.02828415483236313
Epoch:  50 	 Loss:  0.0280389916151762
Epoch:  51 	 Loss:  0.028312815353274345
Epoch:  52 	 Loss:  0.028078977018594742
Epoch:  53 	 Loss:  0.028733037412166595
Epoch:  54 	 Loss:  0.02789139747619629
Epoch:  55 	 Loss:  0.028243817389011383
Epoch:  56 	 Loss:  0.028210420161485672
Epoch:  57 	 Loss:  0.02844199351966381
Epoch:  58 	 Loss:  0.027914725244045258
Epoch:  59 	 Loss:  0.02793699875473976
Epoch:  60 	 Loss:  0.028211669996380806
Epoch:  61 	 Loss:  0.027822325006127357
Epoch:  62 	 Loss:  0.02831335738301277
Epoch:  63 	 Loss:  0.028336092829704285
Epoch:  64 	 Loss:  0.028130531311035156
Epoch:  65 	 Loss:  0.02827792428433895
Epoch:  66 	 Loss:  0.02779032662510872
Epoch:  67 	 Loss:  0.028778672218322754
Epoch:  68 	 Loss:  0.02808377705514431
Epoch:  69 	 Loss:  0.028824273496866226
Epoch:  70 	 Loss:  0.028158633038401604
Epoch:  71 	 Loss:  0.02774517424404621
Epoch:  72 	 Loss:  0.028326066210865974
Epoch:  73 	 Loss:  0.028271129354834557
Epoch:  74 	 Loss:  0.028917504474520683
Epoch:  75 	 Loss:  0.02859954535961151
Epoch:  76 	 Loss:  0.02822066657245159
Epoch:  77 	 Loss:  0.028107712045311928
Epoch:  78 	 Loss:  0.028375547379255295
Epoch:  79 	 Loss:  0.027658531442284584
Epoch:  80 	 Loss:  0.028526093810796738
Epoch:  81 	 Loss:  0.027692999690771103
Epoch:  82 	 Loss:  0.028522269800305367
Epoch:  83 	 Loss:  0.028769250959157944
Epoch:  84 	 Loss:  0.028610888868570328
Epoch:  85 	 Loss:  0.02807251363992691
Epoch:  86 	 Loss:  0.028492698445916176
Epoch:  87 	 Loss:  0.028876520693302155
Epoch:  88 	 Loss:  0.02854876220226288
Epoch:  89 	 Loss:  0.028530046343803406
Epoch:  90 	 Loss:  0.02776988036930561
Epoch:  91 	 Loss:  0.02864774316549301
Epoch:  92 	 Loss:  0.028151873499155045
Epoch:  93 	 Loss:  0.027984537184238434
Epoch:  94 	 Loss:  0.028208093717694283
Epoch:  95 	 Loss:  0.028547130525112152
Epoch:  96 	 Loss:  0.02770099602639675
Epoch:  97 	 Loss:  0.027837546542286873
Epoch:  98 	 Loss:  0.028176505118608475
Epoch:  99 	 Loss:  0.028071898967027664
Epoch:  100 	 Loss:  0.027544140815734863
wandb: Waiting for W&B process to finish, PID 1659857
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143550-d45abh5y/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143550-d45abh5y/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02754
wandb:        _step 800000
wandb:     _runtime 85
wandb:   _timestamp 1612899436
wandb:         loss 0.02754
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–‡â–ƒâ–„â–ƒâ–†â–„â–ƒâ–ˆâ–…â–ƒâ–†â–ƒâ–…â–†â–‚â–†â–„â–„â–…â–…â–ƒâ–„â–ƒâ–‚â–„â–‡â–ˆâ–…â–ˆâ–„â–‚â–†â–†â–ˆâ–†â–„â–„â–‚â–
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced frosty-sweep-12: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/d45abh5y
wandb: Agent Starting Run: vu1y4t2a with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: adam
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:37:22.490062: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:37:22.497351: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run effortless-sweep-13
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/vu1y4t2a
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_143720-vu1y4t2a
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02725175768136978
Epoch:  2 	 Loss:  0.027129875496029854
Epoch:  3 	 Loss:  0.027484875172376633
Epoch:  4 	 Loss:  0.027144605293869972
Epoch:  5 	 Loss:  0.027231605723500252
Epoch:  6 	 Loss:  0.026479138061404228
Epoch:  7 	 Loss:  0.026835894212126732
Epoch:  8 	 Loss:  0.027317499741911888
Epoch:  9 	 Loss:  0.02697775885462761
Epoch:  10 	 Loss:  0.02713475190103054
Epoch:  11 	 Loss:  0.026943303644657135
Epoch:  12 	 Loss:  0.0274414774030447
Epoch:  13 	 Loss:  0.027575906366109848
Epoch:  14 	 Loss:  0.026888100430369377
Epoch:  15 	 Loss:  0.027633752673864365
Epoch:  16 	 Loss:  0.027439575642347336
Epoch:  17 	 Loss:  0.027358369901776314
Epoch:  18 	 Loss:  0.02707429975271225
Epoch:  19 	 Loss:  0.027682023122906685
Epoch:  20 	 Loss:  0.027413666248321533
Epoch:  21 	 Loss:  0.027378564700484276
Epoch:  22 	 Loss:  0.027501756325364113
Epoch:  23 	 Loss:  0.027602026239037514
Epoch:  24 	 Loss:  0.027363305911421776
Epoch:  25 	 Loss:  0.026940826326608658
Epoch:  26 	 Loss:  0.027166511863470078
Epoch:  27 	 Loss:  0.027484752237796783
Epoch:  28 	 Loss:  0.027300558984279633
Epoch:  29 	 Loss:  0.027313703671097755
Epoch:  30 	 Loss:  0.027284666895866394
Epoch:  31 	 Loss:  0.02729777991771698
Epoch:  32 	 Loss:  0.026705315336585045
Epoch:  33 	 Loss:  0.02786102145910263
Epoch:  34 	 Loss:  0.026801003143191338
Epoch:  35 	 Loss:  0.027674242854118347
Epoch:  36 	 Loss:  0.02736853063106537
Epoch:  37 	 Loss:  0.027440542355179787
Epoch:  38 	 Loss:  0.02750837244093418
Epoch:  39 	 Loss:  0.02758067473769188
Epoch:  40 	 Loss:  0.027475733309984207
Epoch:  41 	 Loss:  0.027019189670681953
Epoch:  42 	 Loss:  0.027531664818525314
Epoch:  43 	 Loss:  0.02728884480893612
Epoch:  44 	 Loss:  0.027348022907972336
Epoch:  45 	 Loss:  0.02732732519507408
Epoch:  46 	 Loss:  0.02721908874809742
Epoch:  47 	 Loss:  0.02685714326798916
Epoch:  48 	 Loss:  0.027169296517968178
Epoch:  49 	 Loss:  0.027362214401364326
Epoch:  50 	 Loss:  0.027526360005140305
Epoch:  51 	 Loss:  0.02752619981765747
Epoch:  52 	 Loss:  0.027795402333140373
Epoch:  53 	 Loss:  0.027123477309942245
Epoch:  54 	 Loss:  0.026985758915543556
Epoch:  55 	 Loss:  0.026920583099126816
Epoch:  56 	 Loss:  0.027217350900173187
Epoch:  57 	 Loss:  0.02673431485891342
Epoch:  58 	 Loss:  0.027348345145583153
Epoch:  59 	 Loss:  0.02737981081008911
Epoch:  60 	 Loss:  0.02715381607413292
Epoch:  61 	 Loss:  0.027415916323661804
Epoch:  62 	 Loss:  0.027034001424908638
Epoch:  63 	 Loss:  0.02724331244826317
Epoch:  64 	 Loss:  0.027034973725676537
Epoch:  65 	 Loss:  0.026724210008978844
Epoch:  66 	 Loss:  0.027458084747195244
Epoch:  67 	 Loss:  0.027326175943017006
Epoch:  68 	 Loss:  0.026953261345624924
Epoch:  69 	 Loss:  0.027692662551999092
Epoch:  70 	 Loss:  0.027207093313336372
Epoch:  71 	 Loss:  0.02744191884994507
Epoch:  72 	 Loss:  0.02766084298491478
Epoch:  73 	 Loss:  0.026820847764611244
Epoch:  74 	 Loss:  0.027153491973876953
Epoch:  75 	 Loss:  0.027317918837070465
Epoch:  76 	 Loss:  0.02671637013554573
Epoch:  77 	 Loss:  0.0272662453353405
Epoch:  78 	 Loss:  0.027240285649895668
Epoch:  79 	 Loss:  0.02712254226207733
Epoch:  80 	 Loss:  0.027470597997307777
Epoch:  81 	 Loss:  0.027928736060857773
Epoch:  82 	 Loss:  0.02729772962629795
Epoch:  83 	 Loss:  0.027212927117943764
Epoch:  84 	 Loss:  0.027470959350466728
Epoch:  85 	 Loss:  0.026573721319437027
Epoch:  86 	 Loss:  0.027167439460754395
Epoch:  87 	 Loss:  0.027294479310512543
Epoch:  88 	 Loss:  0.02739151194691658
Epoch:  89 	 Loss:  0.027226392179727554
Epoch:  90 	 Loss:  0.027059156447649002
Epoch:  91 	 Loss:  0.027380215004086494
Epoch:  92 	 Loss:  0.027427416294813156
Epoch:  93 	 Loss:  0.027562201023101807
Epoch:  94 	 Loss:  0.027167951688170433
Epoch:  95 	 Loss:  0.027076348662376404
Epoch:  96 	 Loss:  0.026822084560990334
Epoch:  97 	 Loss:  0.026980042457580566
Epoch:  98 	 Loss:  0.027365054935216904
Epoch:  99 	 Loss:  0.027453090995550156
Epoch:  100 	 Loss:  0.02717525139451027
wandb: Waiting for W&B process to finish, PID 1663554
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143720-vu1y4t2a/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143720-vu1y4t2a/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02718
wandb:        _step 800000
wandb:     _runtime 88
wandb:   _timestamp 1612899528
wandb:         loss 0.02718
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–‡â–â–†â–„â–‡â–‡â–„â–†â–‡â–…â–†â–†â–ƒâ–†â–‡â–„â–†â–…â–†â–‡â–„â–…â–†â–†â–„â–†â–ˆâ–ˆâ–…â–†â–…â–†â–‡â–†â–…â–†â–…â–„â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced effortless-sweep-13: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/vu1y4t2a
wandb: Agent Starting Run: lllj0sqr with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:38:54.737579: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:38:54.744297: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run fallen-sweep-14
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/lllj0sqr
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_143852-lllj0sqr
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02850089594721794
Epoch:  2 	 Loss:  0.029885178431868553
Epoch:  3 	 Loss:  0.028642864897847176
Epoch:  4 	 Loss:  0.029250415042042732
Epoch:  5 	 Loss:  0.02924235351383686
Epoch:  6 	 Loss:  0.028880402445793152
Epoch:  7 	 Loss:  0.02933267503976822
Epoch:  8 	 Loss:  0.028872784227132797
Epoch:  9 	 Loss:  0.0288433488458395
Epoch:  10 	 Loss:  0.028869634494185448
Epoch:  11 	 Loss:  0.02944328263401985
Epoch:  12 	 Loss:  0.02932400070130825
Epoch:  13 	 Loss:  0.028486426919698715
Epoch:  14 	 Loss:  0.029263891279697418
Epoch:  15 	 Loss:  0.029643164947628975
Epoch:  16 	 Loss:  0.028184106573462486
Epoch:  17 	 Loss:  0.02879061922430992
Epoch:  18 	 Loss:  0.029051147401332855
Epoch:  19 	 Loss:  0.02883121557533741
Epoch:  20 	 Loss:  0.029394900426268578
Epoch:  21 	 Loss:  0.02936679497361183
Epoch:  22 	 Loss:  0.02897777408361435
Epoch:  23 	 Loss:  0.029411809518933296
Epoch:  24 	 Loss:  0.029398281127214432
Epoch:  25 	 Loss:  0.029644805938005447
Epoch:  26 	 Loss:  0.029305264353752136
Epoch:  27 	 Loss:  0.02951771579682827
Epoch:  28 	 Loss:  0.02964731864631176
Epoch:  29 	 Loss:  0.0291748009622097
Epoch:  30 	 Loss:  0.029796039685606956
Epoch:  31 	 Loss:  0.02902401052415371
Epoch:  32 	 Loss:  0.029186483472585678
Epoch:  33 	 Loss:  0.02905205637216568
Epoch:  34 	 Loss:  0.029163019731640816
Epoch:  35 	 Loss:  0.028126779943704605
Epoch:  36 	 Loss:  0.029367785900831223
Epoch:  37 	 Loss:  0.0291909109801054
Epoch:  38 	 Loss:  0.02899901568889618
Epoch:  39 	 Loss:  0.02950814925134182
Epoch:  40 	 Loss:  0.029995692893862724
Epoch:  41 	 Loss:  0.02914438769221306
Epoch:  42 	 Loss:  0.02864973433315754
Epoch:  43 	 Loss:  0.028328312560915947
Epoch:  44 	 Loss:  0.029419101774692535
Epoch:  45 	 Loss:  0.029054289683699608
Epoch:  46 	 Loss:  0.029123501852154732
Epoch:  47 	 Loss:  0.028862683102488518
Epoch:  48 	 Loss:  0.02915664203464985
Epoch:  49 	 Loss:  0.02953348122537136
Epoch:  50 	 Loss:  0.028594769537448883
Epoch:  51 	 Loss:  0.02922089770436287
Epoch:  52 	 Loss:  0.029586808755993843
Epoch:  53 	 Loss:  0.029312271624803543
Epoch:  54 	 Loss:  0.02869250252842903
Epoch:  55 	 Loss:  0.029237475246191025
Epoch:  56 	 Loss:  0.02877727337181568
Epoch:  57 	 Loss:  0.029340729117393494
Epoch:  58 	 Loss:  0.029359446838498116
Epoch:  59 	 Loss:  0.028972763568162918
Epoch:  60 	 Loss:  0.029270287603139877
Epoch:  61 	 Loss:  0.029113173484802246
Epoch:  62 	 Loss:  0.029333623126149178
Epoch:  63 	 Loss:  0.02866935357451439
Epoch:  64 	 Loss:  0.028546597808599472
Epoch:  65 	 Loss:  0.029132403433322906
Epoch:  66 	 Loss:  0.029255742207169533
Epoch:  67 	 Loss:  0.029321476817131042
Epoch:  68 	 Loss:  0.028930338099598885
Epoch:  69 	 Loss:  0.028871523216366768
Epoch:  70 	 Loss:  0.029300451278686523
Epoch:  71 	 Loss:  0.02892898954451084
Epoch:  72 	 Loss:  0.028836147859692574
Epoch:  73 	 Loss:  0.02922970987856388
Epoch:  74 	 Loss:  0.02932559885084629
Epoch:  75 	 Loss:  0.029146282002329826
Epoch:  76 	 Loss:  0.028739338740706444
Epoch:  77 	 Loss:  0.02947554737329483
Epoch:  78 	 Loss:  0.02880067564547062
Epoch:  79 	 Loss:  0.029062747955322266
Epoch:  80 	 Loss:  0.02940988540649414
Epoch:  81 	 Loss:  0.029323989525437355
Epoch:  82 	 Loss:  0.029033981263637543
Epoch:  83 	 Loss:  0.028898069635033607
Epoch:  84 	 Loss:  0.0295251552015543
Epoch:  85 	 Loss:  0.028826016932725906
Epoch:  86 	 Loss:  0.0291917584836483
Epoch:  87 	 Loss:  0.029046397656202316
Epoch:  88 	 Loss:  0.02949398383498192
Epoch:  89 	 Loss:  0.029190069064497948
Epoch:  90 	 Loss:  0.029565101489424706
Epoch:  91 	 Loss:  0.029301103204488754
Epoch:  92 	 Loss:  0.028831522911787033
Epoch:  93 	 Loss:  0.02884995937347412
Epoch:  94 	 Loss:  0.02853388525545597
Epoch:  95 	 Loss:  0.029093191027641296
Epoch:  96 	 Loss:  0.028438512235879898
Epoch:  97 	 Loss:  0.029263760894536972
Epoch:  98 	 Loss:  0.029489215463399887
Epoch:  99 	 Loss:  0.02948257140815258
Epoch:  100 	 Loss:  0.029104892164468765
wandb: Waiting for W&B process to finish, PID 1667250
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143852-lllj0sqr/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_143852-lllj0sqr/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0291
wandb:        _step 800000
wandb:     _runtime 88
wandb:   _timestamp 1612899620
wandb:         loss 0.0291
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–ƒâ–ƒâ–„â–„â–‡â–‚â–â–…â–‡â–‡â–†â–ˆâ–…â–†â–‡â–‡â–†â–‡â–…â–‡â–†â–ƒâ–„â–…â–…â–ƒâ–†â–„â–„â–†â–‡â–…â–…â–‡â–…â–†â–„â–ƒâ–†â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced fallen-sweep-14: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/lllj0sqr
wandb: Agent Starting Run: p7m1dsnu with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:40:26.991851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:40:26.999044: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run feasible-sweep-15
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/p7m1dsnu
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_144025-p7m1dsnu
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.029052013531327248
Epoch:  2 	 Loss:  0.029242321848869324
Epoch:  3 	 Loss:  0.02940123900771141
Epoch:  4 	 Loss:  0.029010355472564697
Epoch:  5 	 Loss:  0.029259558767080307
Epoch:  6 	 Loss:  0.029776297509670258
Epoch:  7 	 Loss:  0.02920222282409668
Epoch:  8 	 Loss:  0.029514532536268234
Epoch:  9 	 Loss:  0.028893524780869484
Epoch:  10 	 Loss:  0.028908027336001396
Epoch:  11 	 Loss:  0.029241543263196945
Epoch:  12 	 Loss:  0.029328716918826103
Epoch:  13 	 Loss:  0.028451139107346535
Epoch:  14 	 Loss:  0.02927807718515396
Epoch:  15 	 Loss:  0.029139311984181404
Epoch:  16 	 Loss:  0.02854272536933422
Epoch:  17 	 Loss:  0.029285820201039314
Epoch:  18 	 Loss:  0.029355710372328758
Epoch:  19 	 Loss:  0.028999080881476402
Epoch:  20 	 Loss:  0.029199669137597084
Epoch:  21 	 Loss:  0.02863839827477932
Epoch:  22 	 Loss:  0.028764944523572922
Epoch:  23 	 Loss:  0.029716160148382187
Epoch:  24 	 Loss:  0.02945776656270027
Epoch:  25 	 Loss:  0.029006127268075943
Epoch:  26 	 Loss:  0.02970019169151783
Epoch:  27 	 Loss:  0.028847606852650642
Epoch:  28 	 Loss:  0.029444487765431404
Epoch:  29 	 Loss:  0.028972001746296883
Epoch:  30 	 Loss:  0.029039345681667328
Epoch:  31 	 Loss:  0.02937321923673153
Epoch:  32 	 Loss:  0.029179934412240982
Epoch:  33 	 Loss:  0.02905205637216568
Epoch:  34 	 Loss:  0.02930215746164322
Epoch:  35 	 Loss:  0.029061853885650635
Epoch:  36 	 Loss:  0.029122594743967056
Epoch:  37 	 Loss:  0.028776690363883972
Epoch:  38 	 Loss:  0.029537687078118324
Epoch:  39 	 Loss:  0.029381055384874344
Epoch:  40 	 Loss:  0.02914406545460224
Epoch:  41 	 Loss:  0.028800230473279953
Epoch:  42 	 Loss:  0.02923496812582016
Epoch:  43 	 Loss:  0.029426202178001404
Epoch:  44 	 Loss:  0.02956484444439411
Epoch:  45 	 Loss:  0.029253436252474785
Epoch:  46 	 Loss:  0.029534079134464264
Epoch:  47 	 Loss:  0.028868362307548523
Epoch:  48 	 Loss:  0.028974762186408043
Epoch:  49 	 Loss:  0.029504625126719475
Epoch:  50 	 Loss:  0.0294682327657938
Epoch:  51 	 Loss:  0.029424557462334633
Epoch:  52 	 Loss:  0.02916414849460125
Epoch:  53 	 Loss:  0.029025843366980553
Epoch:  54 	 Loss:  0.029494162648916245
Epoch:  55 	 Loss:  0.02981339395046234
Epoch:  56 	 Loss:  0.02901618741452694
Epoch:  57 	 Loss:  0.029603105038404465
Epoch:  58 	 Loss:  0.02926206961274147
Epoch:  59 	 Loss:  0.028740620240569115
Epoch:  60 	 Loss:  0.02796950563788414
Epoch:  61 	 Loss:  0.029294993728399277
Epoch:  62 	 Loss:  0.02982647530734539
Epoch:  63 	 Loss:  0.029567088931798935
Epoch:  64 	 Loss:  0.029805874451994896
Epoch:  65 	 Loss:  0.029054930433630943
Epoch:  66 	 Loss:  0.02967851422727108
Epoch:  67 	 Loss:  0.029381271451711655
Epoch:  68 	 Loss:  0.02945220097899437
Epoch:  69 	 Loss:  0.02957257814705372
Epoch:  70 	 Loss:  0.02874782308936119
Epoch:  71 	 Loss:  0.029120858758687973
Epoch:  72 	 Loss:  0.02959486097097397
Epoch:  73 	 Loss:  0.028978567570447922
Epoch:  74 	 Loss:  0.029183296486735344
Epoch:  75 	 Loss:  0.029534820467233658
Epoch:  76 	 Loss:  0.02955213375389576
Epoch:  77 	 Loss:  0.029373902827501297
Epoch:  78 	 Loss:  0.029064636677503586
Epoch:  79 	 Loss:  0.029310133308172226
Epoch:  80 	 Loss:  0.02998095192015171
Epoch:  81 	 Loss:  0.029518434777855873
Epoch:  82 	 Loss:  0.02922000363469124
Epoch:  83 	 Loss:  0.029444890096783638
Epoch:  84 	 Loss:  0.029500531032681465
Epoch:  85 	 Loss:  0.02926008030772209
Epoch:  86 	 Loss:  0.028747709468007088
Epoch:  87 	 Loss:  0.02897011674940586
Epoch:  88 	 Loss:  0.029453052207827568
Epoch:  89 	 Loss:  0.02946012280881405
Epoch:  90 	 Loss:  0.028629504144191742
Epoch:  91 	 Loss:  0.030064068734645844
Epoch:  92 	 Loss:  0.029436197131872177
Epoch:  93 	 Loss:  0.02934095822274685
Epoch:  94 	 Loss:  0.029275888577103615
Epoch:  95 	 Loss:  0.02948680706322193
Epoch:  96 	 Loss:  0.029777247458696365
Epoch:  97 	 Loss:  0.028844356536865234
Epoch:  98 	 Loss:  0.029426001012325287
Epoch:  99 	 Loss:  0.029101509600877762
Epoch:  100 	 Loss:  0.028789745643734932
wandb: Waiting for W&B process to finish, PID 1670945
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144025-p7m1dsnu/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144025-p7m1dsnu/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02879
wandb:        _step 800000
wandb:     _runtime 84
wandb:   _timestamp 1612899709
wandb:         loss 0.02879
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–†â–ˆâ–†â–…â–â–â–†â–‚â–ˆâ–‡â–†â–†â–…â–„â–†â–ƒâ–‡â–‡â–†â–†â–†â–„â–‚â–…â–ˆâ–†â–‡â–‡â–…â–†â–…â–…â–†â–„â–†â–†â–…â–ƒâ–ƒ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced feasible-sweep-15: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/p7m1dsnu
wandb: Agent Starting Run: l3dh10wf with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:41:55.245756: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:41:55.250465: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run charmed-sweep-16
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/l3dh10wf
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_144153-l3dh10wf
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02852519042789936
Epoch:  2 	 Loss:  0.027644986286759377
Epoch:  3 	 Loss:  0.02841309830546379
Epoch:  4 	 Loss:  0.028329290449619293
Epoch:  5 	 Loss:  0.02825774997472763
Epoch:  6 	 Loss:  0.027923312038183212
Epoch:  7 	 Loss:  0.028196770697832108
Epoch:  8 	 Loss:  0.02819814532995224
Epoch:  9 	 Loss:  0.028245819732546806
Epoch:  10 	 Loss:  0.028506197035312653
Epoch:  11 	 Loss:  0.027843402698636055
Epoch:  12 	 Loss:  0.02820810303092003
Epoch:  13 	 Loss:  0.027450213208794594
Epoch:  14 	 Loss:  0.02800861746072769
Epoch:  15 	 Loss:  0.02861008793115616
Epoch:  16 	 Loss:  0.028033830225467682
Epoch:  17 	 Loss:  0.028049921616911888
Epoch:  18 	 Loss:  0.028830494731664658
Epoch:  19 	 Loss:  0.027935031801462173
Epoch:  20 	 Loss:  0.02776286192238331
Epoch:  21 	 Loss:  0.027904676273465157
Epoch:  22 	 Loss:  0.028383320197463036
Epoch:  23 	 Loss:  0.028318166732788086
Epoch:  24 	 Loss:  0.028872884809970856
Epoch:  25 	 Loss:  0.02858969382941723
Epoch:  26 	 Loss:  0.02796877734363079
Epoch:  27 	 Loss:  0.027370816096663475
Epoch:  28 	 Loss:  0.02824721299111843
Epoch:  29 	 Loss:  0.02819063887000084
Epoch:  30 	 Loss:  0.028218235820531845
Epoch:  31 	 Loss:  0.028276965022087097
Epoch:  32 	 Loss:  0.027742568403482437
Epoch:  33 	 Loss:  0.02823583036661148
Epoch:  34 	 Loss:  0.0288224034011364
Epoch:  35 	 Loss:  0.02863110974431038
Epoch:  36 	 Loss:  0.028032710775732994
Epoch:  37 	 Loss:  0.028779707849025726
Epoch:  38 	 Loss:  0.02845945581793785
Epoch:  39 	 Loss:  0.02832750976085663
Epoch:  40 	 Loss:  0.028025345876812935
Epoch:  41 	 Loss:  0.02845202013850212
Epoch:  42 	 Loss:  0.028473740443587303
Epoch:  43 	 Loss:  0.028220143169164658
Epoch:  44 	 Loss:  0.028346629813313484
Epoch:  45 	 Loss:  0.029177270829677582
Epoch:  46 	 Loss:  0.0281500443816185
Epoch:  47 	 Loss:  0.028292866423726082
Epoch:  48 	 Loss:  0.028355080634355545
Epoch:  49 	 Loss:  0.027941854670643806
Epoch:  50 	 Loss:  0.028073878958821297
Epoch:  51 	 Loss:  0.02812707982957363
Epoch:  52 	 Loss:  0.028547629714012146
Epoch:  53 	 Loss:  0.02790842019021511
Epoch:  54 	 Loss:  0.027939217165112495
Epoch:  55 	 Loss:  0.028307275846600533
Epoch:  56 	 Loss:  0.028213322162628174
Epoch:  57 	 Loss:  0.02879437431693077
Epoch:  58 	 Loss:  0.027558403089642525
Epoch:  59 	 Loss:  0.02823965810239315
Epoch:  60 	 Loss:  0.028209887444972992
Epoch:  61 	 Loss:  0.028191974386572838
Epoch:  62 	 Loss:  0.028530441224575043
Epoch:  63 	 Loss:  0.028002239763736725
Epoch:  64 	 Loss:  0.027607059106230736
Epoch:  65 	 Loss:  0.028598446398973465
Epoch:  66 	 Loss:  0.027993930503726006
Epoch:  67 	 Loss:  0.027538198977708817
Epoch:  68 	 Loss:  0.028564227744936943
Epoch:  69 	 Loss:  0.027838055044412613
Epoch:  70 	 Loss:  0.028376080095767975
Epoch:  71 	 Loss:  0.028025444597005844
Epoch:  72 	 Loss:  0.02796110138297081
Epoch:  73 	 Loss:  0.028301725164055824
Epoch:  74 	 Loss:  0.028683336451649666
Epoch:  75 	 Loss:  0.028153441846370697
Epoch:  76 	 Loss:  0.028652196750044823
Epoch:  77 	 Loss:  0.028668247163295746
Epoch:  78 	 Loss:  0.02805187925696373
Epoch:  79 	 Loss:  0.027971157804131508
Epoch:  80 	 Loss:  0.027810927480459213
Epoch:  81 	 Loss:  0.027967914938926697
Epoch:  82 	 Loss:  0.028560267761349678
Epoch:  83 	 Loss:  0.02873469702899456
Epoch:  84 	 Loss:  0.028016632422804832
Epoch:  85 	 Loss:  0.02820803038775921
Epoch:  86 	 Loss:  0.027929851785302162
Epoch:  87 	 Loss:  0.028578704223036766
Epoch:  88 	 Loss:  0.02858320064842701
Epoch:  89 	 Loss:  0.028213845565915108
Epoch:  90 	 Loss:  0.028309708461165428
Epoch:  91 	 Loss:  0.028259849175810814
Epoch:  92 	 Loss:  0.028199754655361176
Epoch:  93 	 Loss:  0.028233079239726067
Epoch:  94 	 Loss:  0.028328079730272293
Epoch:  95 	 Loss:  0.028156720101833344
Epoch:  96 	 Loss:  0.027926005423069
Epoch:  97 	 Loss:  0.028883224353194237
Epoch:  98 	 Loss:  0.027396738529205322
Epoch:  99 	 Loss:  0.02798473834991455
Epoch:  100 	 Loss:  0.027621589601039886
wandb: Waiting for W&B process to finish, PID 1674635
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144153-l3dh10wf/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144153-l3dh10wf/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02762
wandb:        _step 800000
wandb:     _runtime 85
wandb:   _timestamp 1612899798
wandb:         loss 0.02762
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–†â–ƒâ–…â–ƒâ–â–„â–ˆâ–ƒâ–…â–„â–…â–…â–ˆâ–„â–…â–†â–…â–„â–ƒâ–„â–ƒâ–…â–…â–…â–‚â–â–ƒâ–ƒâ–‡â–‡â–„â–†â–„â–‡â–…â–…â–…â–ˆâ–‚
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced charmed-sweep-16: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/l3dh10wf
wandb: Agent Starting Run: zks8261n with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:43:24.758933: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:43:24.763693: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run morning-sweep-17
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/zks8261n
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_144322-zks8261n
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028161589056253433
Epoch:  2 	 Loss:  0.02829608879983425
Epoch:  3 	 Loss:  0.028257982805371284
Epoch:  4 	 Loss:  0.02825167030096054
Epoch:  5 	 Loss:  0.028217293322086334
Epoch:  6 	 Loss:  0.028343742713332176
Epoch:  7 	 Loss:  0.028667444363236427
Epoch:  8 	 Loss:  0.027882782742381096
Epoch:  9 	 Loss:  0.027837948873639107
Epoch:  10 	 Loss:  0.02847503311932087
Epoch:  11 	 Loss:  0.02800603397190571
Epoch:  12 	 Loss:  0.028317734599113464
Epoch:  13 	 Loss:  0.028465334326028824
Epoch:  14 	 Loss:  0.028258735314011574
Epoch:  15 	 Loss:  0.028293007984757423
Epoch:  16 	 Loss:  0.02803691476583481
Epoch:  17 	 Loss:  0.02748182602226734
Epoch:  18 	 Loss:  0.028461644425988197
Epoch:  19 	 Loss:  0.027929682284593582
Epoch:  20 	 Loss:  0.027870818972587585
Epoch:  21 	 Loss:  0.02815476804971695
Epoch:  22 	 Loss:  0.02825050614774227
Epoch:  23 	 Loss:  0.028482895344495773
Epoch:  24 	 Loss:  0.02829311415553093
Epoch:  25 	 Loss:  0.028464512899518013
Epoch:  26 	 Loss:  0.027890870347619057
Epoch:  27 	 Loss:  0.027637513354420662
Epoch:  28 	 Loss:  0.028445551171898842
Epoch:  29 	 Loss:  0.028248850256204605
Epoch:  30 	 Loss:  0.02801503613591194
Epoch:  31 	 Loss:  0.028505636379122734
Epoch:  32 	 Loss:  0.028248131275177002
Epoch:  33 	 Loss:  0.028551293537020683
Epoch:  34 	 Loss:  0.027995305135846138
Epoch:  35 	 Loss:  0.027497543022036552
Epoch:  36 	 Loss:  0.028461961075663567
Epoch:  37 	 Loss:  0.028510933741927147
Epoch:  38 	 Loss:  0.02841241843998432
Epoch:  39 	 Loss:  0.02845897153019905
Epoch:  40 	 Loss:  0.02828148566186428
Epoch:  41 	 Loss:  0.027979565784335136
Epoch:  42 	 Loss:  0.028296565636992455
Epoch:  43 	 Loss:  0.027642065659165382
Epoch:  44 	 Loss:  0.027996961027383804
Epoch:  45 	 Loss:  0.02830062434077263
Epoch:  46 	 Loss:  0.027706114575266838
Epoch:  47 	 Loss:  0.028311440721154213
Epoch:  48 	 Loss:  0.02838897332549095
Epoch:  49 	 Loss:  0.028192300349473953
Epoch:  50 	 Loss:  0.027705462649464607
Epoch:  51 	 Loss:  0.027912456542253494
Epoch:  52 	 Loss:  0.02762138843536377
Epoch:  53 	 Loss:  0.028041092678904533
Epoch:  54 	 Loss:  0.02801908366382122
Epoch:  55 	 Loss:  0.028671246021986008
Epoch:  56 	 Loss:  0.02883337065577507
Epoch:  57 	 Loss:  0.02785271778702736
Epoch:  58 	 Loss:  0.02748289331793785
Epoch:  59 	 Loss:  0.0279872864484787
Epoch:  60 	 Loss:  0.02812379226088524
Epoch:  61 	 Loss:  0.028251372277736664
Epoch:  62 	 Loss:  0.02822861261665821
Epoch:  63 	 Loss:  0.027944499626755714
Epoch:  64 	 Loss:  0.027799682691693306
Epoch:  65 	 Loss:  0.027470184490084648
Epoch:  66 	 Loss:  0.028683213517069817
Epoch:  67 	 Loss:  0.028403107076883316
Epoch:  68 	 Loss:  0.02768951654434204
Epoch:  69 	 Loss:  0.028443412855267525
Epoch:  70 	 Loss:  0.028048643842339516
Epoch:  71 	 Loss:  0.02780546061694622
Epoch:  72 	 Loss:  0.02843262068927288
Epoch:  73 	 Loss:  0.02804054133594036
Epoch:  74 	 Loss:  0.02797112986445427
Epoch:  75 	 Loss:  0.02795097790658474
Epoch:  76 	 Loss:  0.02804763987660408
Epoch:  77 	 Loss:  0.02753417380154133
Epoch:  78 	 Loss:  0.028568239882588387
Epoch:  79 	 Loss:  0.028397802263498306
Epoch:  80 	 Loss:  0.028501510620117188
Epoch:  81 	 Loss:  0.028067639097571373
Epoch:  82 	 Loss:  0.02759753353893757
Epoch:  83 	 Loss:  0.028288327157497406
Epoch:  84 	 Loss:  0.027401873841881752
Epoch:  85 	 Loss:  0.02808290347456932
Epoch:  86 	 Loss:  0.027590062469244003
Epoch:  87 	 Loss:  0.02864157222211361
Epoch:  88 	 Loss:  0.02788662537932396
Epoch:  89 	 Loss:  0.028434114530682564
Epoch:  90 	 Loss:  0.028682991862297058
Epoch:  91 	 Loss:  0.028273629024624825
Epoch:  92 	 Loss:  0.02822873555123806
Epoch:  93 	 Loss:  0.028783854097127914
Epoch:  94 	 Loss:  0.028258122503757477
Epoch:  95 	 Loss:  0.02804788574576378
Epoch:  96 	 Loss:  0.02809762954711914
Epoch:  97 	 Loss:  0.028101351112127304
Epoch:  98 	 Loss:  0.02855129912495613
Epoch:  99 	 Loss:  0.028082747012376785
Epoch:  100 	 Loss:  0.028185326606035233
wandb: Waiting for W&B process to finish, PID 1678327
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144322-zks8261n/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144322-zks8261n/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02819
wandb:        _step 800000
wandb:     _runtime 79
wandb:   _timestamp 1612899881
wandb:         loss 0.02819
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–…â–†â–ƒâ–„â–†â–„â–†â–…â–†â–ƒâ–†â–†â–„â–†â–†â–„â–„â–‚â–…â–ƒâ–„â–ˆâ–„â–…â–ƒâ–†â–†â–†â–„â–‚â–†â–‚â–â–‡â–†â–…â–…â–„â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced morning-sweep-17: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/zks8261n
wandb: Agent Starting Run: ugyq37q6 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:44:47.406765: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:44:47.413148: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run gallant-sweep-18
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/ugyq37q6
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_144445-ugyq37q6
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028168953955173492
Epoch:  2 	 Loss:  0.028085755184292793
Epoch:  3 	 Loss:  0.027689268812537193
Epoch:  4 	 Loss:  0.027540476992726326
Epoch:  5 	 Loss:  0.02790941670536995
Epoch:  6 	 Loss:  0.02806718833744526
Epoch:  7 	 Loss:  0.028564324602484703
Epoch:  8 	 Loss:  0.02763904072344303
Epoch:  9 	 Loss:  0.02867981046438217
Epoch:  10 	 Loss:  0.02773764356970787
Epoch:  11 	 Loss:  0.02802184969186783
Epoch:  12 	 Loss:  0.027915555983781815
Epoch:  13 	 Loss:  0.028922172263264656
Epoch:  14 	 Loss:  0.027547309175133705
Epoch:  15 	 Loss:  0.02818390540778637
Epoch:  16 	 Loss:  0.027982300147414207
Epoch:  17 	 Loss:  0.02803783118724823
Epoch:  18 	 Loss:  0.027951326221227646
Epoch:  19 	 Loss:  0.028315387666225433
Epoch:  20 	 Loss:  0.02787354215979576
Epoch:  21 	 Loss:  0.028563465923070908
Epoch:  22 	 Loss:  0.0287284217774868
Epoch:  23 	 Loss:  0.028684139251708984
Epoch:  24 	 Loss:  0.02816748060286045
Epoch:  25 	 Loss:  0.02818111702799797
Epoch:  26 	 Loss:  0.027763264253735542
Epoch:  27 	 Loss:  0.028672711923718452
Epoch:  28 	 Loss:  0.02829805575311184
Epoch:  29 	 Loss:  0.028513673692941666
Epoch:  30 	 Loss:  0.027918338775634766
Epoch:  31 	 Loss:  0.0284621212631464
Epoch:  32 	 Loss:  0.028642717748880386
Epoch:  33 	 Loss:  0.028429536148905754
Epoch:  34 	 Loss:  0.02837212197482586
Epoch:  35 	 Loss:  0.02762376330792904
Epoch:  36 	 Loss:  0.028234537690877914
Epoch:  37 	 Loss:  0.027985604479908943
Epoch:  38 	 Loss:  0.028194710612297058
Epoch:  39 	 Loss:  0.028623254969716072
Epoch:  40 	 Loss:  0.02837434969842434
Epoch:  41 	 Loss:  0.028101369738578796
Epoch:  42 	 Loss:  0.028425084426999092
Epoch:  43 	 Loss:  0.02805945836007595
Epoch:  44 	 Loss:  0.028381774201989174
Epoch:  45 	 Loss:  0.028263449668884277
Epoch:  46 	 Loss:  0.027908412739634514
Epoch:  47 	 Loss:  0.02833646535873413
Epoch:  48 	 Loss:  0.028557514771819115
Epoch:  49 	 Loss:  0.028528502210974693
Epoch:  50 	 Loss:  0.027819721028208733
Epoch:  51 	 Loss:  0.028468521311879158
Epoch:  52 	 Loss:  0.028579169884324074
Epoch:  53 	 Loss:  0.02854788675904274
Epoch:  54 	 Loss:  0.02830362692475319
Epoch:  55 	 Loss:  0.027866212651133537
Epoch:  56 	 Loss:  0.028203582391142845
Epoch:  57 	 Loss:  0.02844988740980625
Epoch:  58 	 Loss:  0.028377918526530266
Epoch:  59 	 Loss:  0.028700465336441994
Epoch:  60 	 Loss:  0.02752402052283287
Epoch:  61 	 Loss:  0.02889295108616352
Epoch:  62 	 Loss:  0.028604520484805107
Epoch:  63 	 Loss:  0.028611550107598305
Epoch:  64 	 Loss:  0.028455903753638268
Epoch:  65 	 Loss:  0.02823968045413494
Epoch:  66 	 Loss:  0.028214994817972183
Epoch:  67 	 Loss:  0.028463270515203476
Epoch:  68 	 Loss:  0.028420304879546165
Epoch:  69 	 Loss:  0.028413083404302597
Epoch:  70 	 Loss:  0.028115423396229744
Epoch:  71 	 Loss:  0.028815414756536484
Epoch:  72 	 Loss:  0.02888195775449276
Epoch:  73 	 Loss:  0.028559327125549316
Epoch:  74 	 Loss:  0.028407054021954536
Epoch:  75 	 Loss:  0.02816450595855713
Epoch:  76 	 Loss:  0.02799449861049652
Epoch:  77 	 Loss:  0.027826504781842232
Epoch:  78 	 Loss:  0.02822759561240673
Epoch:  79 	 Loss:  0.028068389743566513
Epoch:  80 	 Loss:  0.028776990249753
Epoch:  81 	 Loss:  0.028610434383153915
Epoch:  82 	 Loss:  0.028180938214063644
Epoch:  83 	 Loss:  0.02834232896566391
Epoch:  84 	 Loss:  0.028029419481754303
Epoch:  85 	 Loss:  0.02823309600353241
Epoch:  86 	 Loss:  0.02783355861902237
Epoch:  87 	 Loss:  0.027575505897402763
Epoch:  88 	 Loss:  0.02822924591600895
Epoch:  89 	 Loss:  0.027922924607992172
Epoch:  90 	 Loss:  0.028553711250424385
Epoch:  91 	 Loss:  0.028892453759908676
Epoch:  92 	 Loss:  0.028670910745859146
Epoch:  93 	 Loss:  0.027655765414237976
Epoch:  94 	 Loss:  0.028510376811027527
Epoch:  95 	 Loss:  0.028538590297102928
Epoch:  96 	 Loss:  0.027687516063451767
Epoch:  97 	 Loss:  0.028901008889079094
Epoch:  98 	 Loss:  0.027828775346279144
Epoch:  99 	 Loss:  0.02845950610935688
Epoch:  100 	 Loss:  0.028566822409629822
wandb: Waiting for W&B process to finish, PID 1682011
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144445-ugyq37q6/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144445-ugyq37q6/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02857
wandb:        _step 800000
wandb:     _runtime 80
wandb:   _timestamp 1612899965
wandb:         loss 0.02857
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–‚â–„â–â–ƒâ–ˆâ–ƒâ–ƒâ–†â–‡â–‚â–…â–†â–…â–„â–†â–„â–…â–ƒâ–†â–†â–…â–„â–‡â–ˆâ–†â–†â–…â–ˆâ–…â–‚â–„â–„â–ƒâ–â–ƒâ–‡â–†â–ˆâ–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced gallant-sweep-18: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/ugyq37q6
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: gcgsa1um with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:46:21.700164: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:46:21.706647: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run twilight-sweep-19
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/gcgsa1um
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_144619-gcgsa1um
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027153925970196724
Epoch:  2 	 Loss:  0.027549918740987778
Epoch:  3 	 Loss:  0.027842769399285316
Epoch:  4 	 Loss:  0.02803727798163891
Epoch:  5 	 Loss:  0.02754404954612255
Epoch:  6 	 Loss:  0.02762569859623909
Epoch:  7 	 Loss:  0.027767112478613853
Epoch:  8 	 Loss:  0.027417929843068123
Epoch:  9 	 Loss:  0.0274562519043684
Epoch:  10 	 Loss:  0.027366720139980316
Epoch:  11 	 Loss:  0.027500705793499947
Epoch:  12 	 Loss:  0.027747342362999916
Epoch:  13 	 Loss:  0.027331430464982986
Epoch:  14 	 Loss:  0.02768879197537899
Epoch:  15 	 Loss:  0.027162978425621986
Epoch:  16 	 Loss:  0.027976907789707184
Epoch:  17 	 Loss:  0.027576038613915443
Epoch:  18 	 Loss:  0.02777421474456787
Epoch:  19 	 Loss:  0.02719195932149887
Epoch:  20 	 Loss:  0.027424247935414314
Epoch:  21 	 Loss:  0.027258433401584625
Epoch:  22 	 Loss:  0.027284571900963783
Epoch:  23 	 Loss:  0.027551110833883286
Epoch:  24 	 Loss:  0.027298469096422195
Epoch:  25 	 Loss:  0.027177371084690094
Epoch:  26 	 Loss:  0.027651924639940262
Epoch:  27 	 Loss:  0.026975810527801514
Epoch:  28 	 Loss:  0.027348674833774567
Epoch:  29 	 Loss:  0.02725105546414852
Epoch:  30 	 Loss:  0.027651643380522728
Epoch:  31 	 Loss:  0.027436789125204086
Epoch:  32 	 Loss:  0.027626583352684975
Epoch:  33 	 Loss:  0.0273764505982399
Epoch:  34 	 Loss:  0.02783045917749405
Epoch:  35 	 Loss:  0.02761605940759182
Epoch:  36 	 Loss:  0.027340713888406754
Epoch:  37 	 Loss:  0.027465110644698143
Epoch:  38 	 Loss:  0.02749149687588215
Epoch:  39 	 Loss:  0.027337143197655678
Epoch:  40 	 Loss:  0.02742111124098301
Epoch:  41 	 Loss:  0.02738524042069912
Epoch:  42 	 Loss:  0.02713451348245144
Epoch:  43 	 Loss:  0.027804827317595482
Epoch:  44 	 Loss:  0.028012903407216072
Epoch:  45 	 Loss:  0.027458081021904945
Epoch:  46 	 Loss:  0.02776521071791649
Epoch:  47 	 Loss:  0.02787381410598755
Epoch:  48 	 Loss:  0.02696988545358181
Epoch:  49 	 Loss:  0.0274044219404459
Epoch:  50 	 Loss:  0.02752581797540188
Epoch:  51 	 Loss:  0.0275412667542696
Epoch:  52 	 Loss:  0.027341168373823166
Epoch:  53 	 Loss:  0.027650631964206696
Epoch:  54 	 Loss:  0.027615373954176903
Epoch:  55 	 Loss:  0.027721211314201355
Epoch:  56 	 Loss:  0.02804931253194809
Epoch:  57 	 Loss:  0.02767583355307579
Epoch:  58 	 Loss:  0.02758699655532837
Epoch:  59 	 Loss:  0.027872104197740555
Epoch:  60 	 Loss:  0.02739114873111248
Epoch:  61 	 Loss:  0.027964429929852486
Epoch:  62 	 Loss:  0.027832213789224625
Epoch:  63 	 Loss:  0.02779596485197544
Epoch:  64 	 Loss:  0.027524584904313087
Epoch:  65 	 Loss:  0.02757224254310131
Epoch:  66 	 Loss:  0.0276615209877491
Epoch:  67 	 Loss:  0.027742987498641014
Epoch:  68 	 Loss:  0.027809377759695053
Epoch:  69 	 Loss:  0.027885951101779938
Epoch:  70 	 Loss:  0.027828969061374664
Epoch:  71 	 Loss:  0.027452437207102776
Epoch:  72 	 Loss:  0.027713345363736153
Epoch:  73 	 Loss:  0.02776825986802578
Epoch:  74 	 Loss:  0.027761198580265045
Epoch:  75 	 Loss:  0.0274847149848938
Epoch:  76 	 Loss:  0.027672862634062767
Epoch:  77 	 Loss:  0.027303576469421387
Epoch:  78 	 Loss:  0.027372924610972404
Epoch:  79 	 Loss:  0.02759789116680622
Epoch:  80 	 Loss:  0.02747032791376114
Epoch:  81 	 Loss:  0.027781806886196136
Epoch:  82 	 Loss:  0.027078386396169662
Epoch:  83 	 Loss:  0.027612799778580666
Epoch:  84 	 Loss:  0.02723749168217182
Epoch:  85 	 Loss:  0.027424464002251625
Epoch:  86 	 Loss:  0.027488194406032562
Epoch:  87 	 Loss:  0.027666641399264336
Epoch:  88 	 Loss:  0.027796290814876556
Epoch:  89 	 Loss:  0.027588581666350365
Epoch:  90 	 Loss:  0.02781771309673786
Epoch:  91 	 Loss:  0.026627572253346443
Epoch:  92 	 Loss:  0.027420777827501297
Epoch:  93 	 Loss:  0.027639850974082947
Epoch:  94 	 Loss:  0.026907989755272865
Epoch:  95 	 Loss:  0.027523530647158623
Epoch:  96 	 Loss:  0.027296410873532295
Epoch:  97 	 Loss:  0.027471886947751045
Epoch:  98 	 Loss:  0.02746061608195305
Epoch:  99 	 Loss:  0.0273952167481184
Epoch:  100 	 Loss:  0.02785082906484604
wandb: Waiting for W&B process to finish, PID 1685708
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144619-gcgsa1um/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144619-gcgsa1um/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02785
wandb:        _step 800000
wandb:     _runtime 80
wandb:   _timestamp 1612900059
wandb:         loss 0.02785
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–ƒâ–‡â–…â–„â–…â–„â–ˆâ–†â–ƒâ–…â–†â–„â–„â–‡â–„â–„â–„â–ˆâ–†â–„â–…â–…â–ˆâ–‡â–‡â–…â–†â–‡â–†â–†â–ƒâ–…â–‚â–ƒâ–†â–…â–„â–â–„â–‡
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced twilight-sweep-19: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/gcgsa1um
wandb: Agent Starting Run: j5sc6m67 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:47:46.204113: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:47:46.210502: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run smart-sweep-20
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/j5sc6m67
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_144744-j5sc6m67
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027381258085370064
Epoch:  2 	 Loss:  0.026792846620082855
Epoch:  3 	 Loss:  0.026980921626091003
Epoch:  4 	 Loss:  0.027574991807341576
Epoch:  5 	 Loss:  0.027460461482405663
Epoch:  6 	 Loss:  0.02724485658109188
Epoch:  7 	 Loss:  0.02744615636765957
Epoch:  8 	 Loss:  0.027151452377438545
Epoch:  9 	 Loss:  0.026973839849233627
Epoch:  10 	 Loss:  0.027284754440188408
Epoch:  11 	 Loss:  0.02712581679224968
Epoch:  12 	 Loss:  0.027105143293738365
Epoch:  13 	 Loss:  0.02746458537876606
Epoch:  14 	 Loss:  0.027297932654619217
Epoch:  15 	 Loss:  0.027538811787962914
Epoch:  16 	 Loss:  0.027437275275588036
Epoch:  17 	 Loss:  0.027331769466400146
Epoch:  18 	 Loss:  0.027328507974743843
Epoch:  19 	 Loss:  0.02695571444928646
Epoch:  20 	 Loss:  0.027194218710064888
Epoch:  21 	 Loss:  0.027271665632724762
Epoch:  22 	 Loss:  0.0271535012871027
Epoch:  23 	 Loss:  0.02747015655040741
Epoch:  24 	 Loss:  0.027307381853461266
Epoch:  25 	 Loss:  0.026954663917422295
Epoch:  26 	 Loss:  0.027529185637831688
Epoch:  27 	 Loss:  0.027610814198851585
Epoch:  28 	 Loss:  0.027442418038845062
Epoch:  29 	 Loss:  0.027121158316731453
Epoch:  30 	 Loss:  0.027227558195590973
Epoch:  31 	 Loss:  0.02694673277437687
Epoch:  32 	 Loss:  0.02737586386501789
Epoch:  33 	 Loss:  0.027242319658398628
Epoch:  34 	 Loss:  0.02717992477118969
Epoch:  35 	 Loss:  0.02670368365943432
Epoch:  36 	 Loss:  0.027007974684238434
Epoch:  37 	 Loss:  0.02708948217332363
Epoch:  38 	 Loss:  0.027272284030914307
Epoch:  39 	 Loss:  0.02747480198740959
Epoch:  40 	 Loss:  0.02712784893810749
Epoch:  41 	 Loss:  0.02700832113623619
Epoch:  42 	 Loss:  0.027100820094347
Epoch:  43 	 Loss:  0.026863085106015205
Epoch:  44 	 Loss:  0.02714635618031025
Epoch:  45 	 Loss:  0.026680899783968925
Epoch:  46 	 Loss:  0.026942169293761253
Epoch:  47 	 Loss:  0.026681313291192055
Epoch:  48 	 Loss:  0.027523178607225418
Epoch:  49 	 Loss:  0.027364537119865417
Epoch:  50 	 Loss:  0.027174022048711777
Epoch:  51 	 Loss:  0.027242271229624748
Epoch:  52 	 Loss:  0.027190763503313065
Epoch:  53 	 Loss:  0.02706770785152912
Epoch:  54 	 Loss:  0.02736731246113777
Epoch:  55 	 Loss:  0.027423394843935966
Epoch:  56 	 Loss:  0.027503792196512222
Epoch:  57 	 Loss:  0.02718012034893036
Epoch:  58 	 Loss:  0.027564052492380142
Epoch:  59 	 Loss:  0.027054905891418457
Epoch:  60 	 Loss:  0.026701556518673897
Epoch:  61 	 Loss:  0.027122437953948975
Epoch:  62 	 Loss:  0.02743339166045189
Epoch:  63 	 Loss:  0.026903418824076653
Epoch:  64 	 Loss:  0.0274680033326149
Epoch:  65 	 Loss:  0.026886316016316414
Epoch:  66 	 Loss:  0.0271050613373518
Epoch:  67 	 Loss:  0.027067558839917183
Epoch:  68 	 Loss:  0.0273783840239048
Epoch:  69 	 Loss:  0.02716454491019249
Epoch:  70 	 Loss:  0.026905260980129242
Epoch:  71 	 Loss:  0.027058269828557968
Epoch:  72 	 Loss:  0.02738082781434059
Epoch:  73 	 Loss:  0.02724399045109749
Epoch:  74 	 Loss:  0.027682211250066757
Epoch:  75 	 Loss:  0.027205247431993484
Epoch:  76 	 Loss:  0.027413908392190933
Epoch:  77 	 Loss:  0.02764788828790188
Epoch:  78 	 Loss:  0.027341391891241074
Epoch:  79 	 Loss:  0.02703378163278103
Epoch:  80 	 Loss:  0.02673429809510708
Epoch:  81 	 Loss:  0.026981372386217117
Epoch:  82 	 Loss:  0.027360979467630386
Epoch:  83 	 Loss:  0.02734328620135784
Epoch:  84 	 Loss:  0.02716606855392456
Epoch:  85 	 Loss:  0.026989344507455826
Epoch:  86 	 Loss:  0.027045318856835365
Epoch:  87 	 Loss:  0.02707495540380478
Epoch:  88 	 Loss:  0.027279408648610115
Epoch:  89 	 Loss:  0.026642780750989914
Epoch:  90 	 Loss:  0.02745710499584675
Epoch:  91 	 Loss:  0.027328353375196457
Epoch:  92 	 Loss:  0.026954013854265213
Epoch:  93 	 Loss:  0.02698228321969509
Epoch:  94 	 Loss:  0.027036108076572418
Epoch:  95 	 Loss:  0.027307039126753807
Epoch:  96 	 Loss:  0.027437623590230942
Epoch:  97 	 Loss:  0.02701125107705593
Epoch:  98 	 Loss:  0.026927556842565536
Epoch:  99 	 Loss:  0.027543723583221436
Epoch:  100 	 Loss:  0.027059948071837425
wandb: Waiting for W&B process to finish, PID 1689391
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144744-j5sc6m67/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144744-j5sc6m67/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02706
wandb:        _step 800000
wandb:     _runtime 80
wandb:   _timestamp 1612900144
wandb:         loss 0.02706
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–ƒâ–…â–„â–„â–‡â–†â–†â–…â–‡â–‡â–†â–ƒâ–…â–ƒâ–‡â–ƒâ–„â–ƒâ–†â–…â–†â–‡â–„â–„â–‡â–„â–…â–†â–ˆâ–ˆâ–„â–†â–…â–„â–â–ƒâ–„â–ƒâ–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced smart-sweep-20: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/j5sc6m67
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: f28lksx0 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:49:21.101543: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:49:21.109040: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run hardy-sweep-21
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/f28lksx0
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_144919-f28lksx0
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028365543112158775
Epoch:  2 	 Loss:  0.02811008132994175
Epoch:  3 	 Loss:  0.02874310128390789
Epoch:  4 	 Loss:  0.027959629893302917
Epoch:  5 	 Loss:  0.02835431508719921
Epoch:  6 	 Loss:  0.028703337535262108
Epoch:  7 	 Loss:  0.028274888172745705
Epoch:  8 	 Loss:  0.02849770151078701
Epoch:  9 	 Loss:  0.028445836156606674
Epoch:  10 	 Loss:  0.028861334547400475
Epoch:  11 	 Loss:  0.027765311300754547
Epoch:  12 	 Loss:  0.028600269928574562
Epoch:  13 	 Loss:  0.02814473584294319
Epoch:  14 	 Loss:  0.028422195464372635
Epoch:  15 	 Loss:  0.028310097754001617
Epoch:  16 	 Loss:  0.028376730158925056
Epoch:  17 	 Loss:  0.02830824814736843
Epoch:  18 	 Loss:  0.02822701260447502
Epoch:  19 	 Loss:  0.02830725908279419
Epoch:  20 	 Loss:  0.028419451788067818
Epoch:  21 	 Loss:  0.028814394026994705
Epoch:  22 	 Loss:  0.02814379520714283
Epoch:  23 	 Loss:  0.027888046577572823
Epoch:  24 	 Loss:  0.028470663353800774
Epoch:  25 	 Loss:  0.028171304613351822
Epoch:  26 	 Loss:  0.02808319963514805
Epoch:  27 	 Loss:  0.02834072895348072
Epoch:  28 	 Loss:  0.02786906249821186
Epoch:  29 	 Loss:  0.02856292948126793
Epoch:  30 	 Loss:  0.02830306440591812
Epoch:  31 	 Loss:  0.028187865391373634
Epoch:  32 	 Loss:  0.028534237295389175
Epoch:  33 	 Loss:  0.028203850612044334
Epoch:  34 	 Loss:  0.027985114604234695
Epoch:  35 	 Loss:  0.028264805674552917
Epoch:  36 	 Loss:  0.028016764670610428
Epoch:  37 	 Loss:  0.02861301600933075
Epoch:  38 	 Loss:  0.028044339269399643
Epoch:  39 	 Loss:  0.029142022132873535
Epoch:  40 	 Loss:  0.02852189913392067
Epoch:  41 	 Loss:  0.02890137955546379
Epoch:  42 	 Loss:  0.02844492718577385
Epoch:  43 	 Loss:  0.02811536006629467
Epoch:  44 	 Loss:  0.02837904915213585
Epoch:  45 	 Loss:  0.027926700189709663
Epoch:  46 	 Loss:  0.028638221323490143
Epoch:  47 	 Loss:  0.028241772204637527
Epoch:  48 	 Loss:  0.02842099405825138
Epoch:  49 	 Loss:  0.028392352163791656
Epoch:  50 	 Loss:  0.02783903293311596
Epoch:  51 	 Loss:  0.028022784739732742
Epoch:  52 	 Loss:  0.028139974921941757
Epoch:  53 	 Loss:  0.02799145132303238
Epoch:  54 	 Loss:  0.02806396223604679
Epoch:  55 	 Loss:  0.029042532667517662
Epoch:  56 	 Loss:  0.028552576899528503
Epoch:  57 	 Loss:  0.028262929990887642
Epoch:  58 	 Loss:  0.02783437818288803
Epoch:  59 	 Loss:  0.028121713548898697
Epoch:  60 	 Loss:  0.028407812118530273
Epoch:  61 	 Loss:  0.028131520375609398
Epoch:  62 	 Loss:  0.028453892096877098
Epoch:  63 	 Loss:  0.02849947102367878
Epoch:  64 	 Loss:  0.02863193489611149
Epoch:  65 	 Loss:  0.027441276237368584
Epoch:  66 	 Loss:  0.028469786047935486
Epoch:  67 	 Loss:  0.028561953455209732
Epoch:  68 	 Loss:  0.028098803013563156
Epoch:  69 	 Loss:  0.028318073600530624
Epoch:  70 	 Loss:  0.028167512267827988
Epoch:  71 	 Loss:  0.028115203604102135
Epoch:  72 	 Loss:  0.028251422569155693
Epoch:  73 	 Loss:  0.02836662530899048
Epoch:  74 	 Loss:  0.028374847024679184
Epoch:  75 	 Loss:  0.028218461200594902
Epoch:  76 	 Loss:  0.028646014630794525
Epoch:  77 	 Loss:  0.028038062155246735
Epoch:  78 	 Loss:  0.0284600630402565
Epoch:  79 	 Loss:  0.028117148205637932
Epoch:  80 	 Loss:  0.028314020484685898
Epoch:  81 	 Loss:  0.02805192954838276
Epoch:  82 	 Loss:  0.02813011035323143
Epoch:  83 	 Loss:  0.02697356790304184
Epoch:  84 	 Loss:  0.02829817682504654
Epoch:  85 	 Loss:  0.028202565386891365
Epoch:  86 	 Loss:  0.028600241988897324
Epoch:  87 	 Loss:  0.02821827493607998
Epoch:  88 	 Loss:  0.027897389605641365
Epoch:  89 	 Loss:  0.028281014412641525
Epoch:  90 	 Loss:  0.028552796691656113
Epoch:  91 	 Loss:  0.029006462544202805
Epoch:  92 	 Loss:  0.027958575636148453
Epoch:  93 	 Loss:  0.028433511033654213
Epoch:  94 	 Loss:  0.028597500175237656
Epoch:  95 	 Loss:  0.028397446498274803
Epoch:  96 	 Loss:  0.02798217162489891
Epoch:  97 	 Loss:  0.0288153737783432
Epoch:  98 	 Loss:  0.028141038492321968
Epoch:  99 	 Loss:  0.028181543573737144
Epoch:  100 	 Loss:  0.02836919203400612
wandb: Waiting for W&B process to finish, PID 1693093
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144919-f28lksx0/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_144919-f28lksx0/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02837
wandb:        _step 800000
wandb:     _runtime 82
wandb:   _timestamp 1612900241
wandb:         loss 0.02837
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–†â–†â–…â–â–ƒâ–„â–ƒâ–†â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ˆâ–‡â–„â–…â–„â–‚â–ƒâ–…â–ƒâ–ƒâ–…â–…â–„â–ƒâ–„â–‚â–ƒâ–ƒâ–„â–ƒâ–„â–‚â–…â–†â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced hardy-sweep-21: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/f28lksx0
wandb: Agent Starting Run: qlesacfn with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:50:48.188438: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:50:48.193552: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run robust-sweep-22
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/qlesacfn
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_145046-qlesacfn
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028173936530947685
Epoch:  2 	 Loss:  0.0295637845993042
Epoch:  3 	 Loss:  0.028338639065623283
Epoch:  4 	 Loss:  0.028707122430205345
Epoch:  5 	 Loss:  0.028988705947995186
Epoch:  6 	 Loss:  0.02853458747267723
Epoch:  7 	 Loss:  0.028976961970329285
Epoch:  8 	 Loss:  0.028227366507053375
Epoch:  9 	 Loss:  0.028513457626104355
Epoch:  10 	 Loss:  0.028706839308142662
Epoch:  11 	 Loss:  0.02890746295452118
Epoch:  12 	 Loss:  0.028684502467513084
Epoch:  13 	 Loss:  0.027841925621032715
Epoch:  14 	 Loss:  0.028225211426615715
Epoch:  15 	 Loss:  0.028696518391370773
Epoch:  16 	 Loss:  0.028516214340925217
Epoch:  17 	 Loss:  0.028809107840061188
Epoch:  18 	 Loss:  0.028950437903404236
Epoch:  19 	 Loss:  0.028983550146222115
Epoch:  20 	 Loss:  0.028393959626555443
Epoch:  21 	 Loss:  0.028897637501358986
Epoch:  22 	 Loss:  0.029063977301120758
Epoch:  23 	 Loss:  0.028638720512390137
Epoch:  24 	 Loss:  0.02875583991408348
Epoch:  25 	 Loss:  0.029310928657650948
Epoch:  26 	 Loss:  0.028629407286643982
Epoch:  27 	 Loss:  0.028175413608551025
Epoch:  28 	 Loss:  0.028194352984428406
Epoch:  29 	 Loss:  0.028720278292894363
Epoch:  30 	 Loss:  0.028313906863331795
Epoch:  31 	 Loss:  0.02850971184670925
Epoch:  32 	 Loss:  0.028376849368214607
Epoch:  33 	 Loss:  0.028696248307824135
Epoch:  34 	 Loss:  0.0286936704069376
Epoch:  35 	 Loss:  0.029200609773397446
Epoch:  36 	 Loss:  0.028636176139116287
Epoch:  37 	 Loss:  0.029070641845464706
Epoch:  38 	 Loss:  0.028406234458088875
Epoch:  39 	 Loss:  0.029262350872159004
Epoch:  40 	 Loss:  0.02912227064371109
Epoch:  41 	 Loss:  0.028333719819784164
Epoch:  42 	 Loss:  0.02881714515388012
Epoch:  43 	 Loss:  0.028951194137334824
Epoch:  44 	 Loss:  0.029010778293013573
Epoch:  45 	 Loss:  0.029148399829864502
Epoch:  46 	 Loss:  0.028839917853474617
Epoch:  47 	 Loss:  0.028833452612161636
Epoch:  48 	 Loss:  0.028868436813354492
Epoch:  49 	 Loss:  0.028629031032323837
Epoch:  50 	 Loss:  0.028696343302726746
Epoch:  51 	 Loss:  0.028707202523946762
Epoch:  52 	 Loss:  0.028369998559355736
Epoch:  53 	 Loss:  0.028654083609580994
Epoch:  54 	 Loss:  0.02872355468571186
Epoch:  55 	 Loss:  0.02842005342245102
Epoch:  56 	 Loss:  0.028490085154771805
Epoch:  57 	 Loss:  0.02841632068157196
Epoch:  58 	 Loss:  0.02898900769650936
Epoch:  59 	 Loss:  0.02912125550210476
Epoch:  60 	 Loss:  0.0285046324133873
Epoch:  61 	 Loss:  0.028244266286492348
Epoch:  62 	 Loss:  0.028606440871953964
Epoch:  63 	 Loss:  0.02897922694683075
Epoch:  64 	 Loss:  0.02900320664048195
Epoch:  65 	 Loss:  0.029158370569348335
Epoch:  66 	 Loss:  0.028765903785824776
Epoch:  67 	 Loss:  0.028686514124274254
Epoch:  68 	 Loss:  0.029017828404903412
Epoch:  69 	 Loss:  0.02836962789297104
Epoch:  70 	 Loss:  0.028908513486385345
Epoch:  71 	 Loss:  0.028218576684594154
Epoch:  72 	 Loss:  0.029181260615587234
Epoch:  73 	 Loss:  0.027963532134890556
Epoch:  74 	 Loss:  0.028919747099280357
Epoch:  75 	 Loss:  0.028648005798459053
Epoch:  76 	 Loss:  0.02885817177593708
Epoch:  77 	 Loss:  0.029125383123755455
Epoch:  78 	 Loss:  0.0284655150026083
Epoch:  79 	 Loss:  0.029176129028201103
Epoch:  80 	 Loss:  0.028948325663805008
Epoch:  81 	 Loss:  0.02898082695901394
Epoch:  82 	 Loss:  0.028974756598472595
Epoch:  83 	 Loss:  0.0292974840849638
Epoch:  84 	 Loss:  0.028497351333498955
Epoch:  85 	 Loss:  0.02877933718264103
Epoch:  86 	 Loss:  0.028416821733117104
Epoch:  87 	 Loss:  0.028691276907920837
Epoch:  88 	 Loss:  0.028604429215192795
Epoch:  89 	 Loss:  0.02850232645869255
Epoch:  90 	 Loss:  0.028625445440411568
Epoch:  91 	 Loss:  0.028522124513983727
Epoch:  92 	 Loss:  0.02843504212796688
Epoch:  93 	 Loss:  0.028273632749915123
Epoch:  94 	 Loss:  0.029015246778726578
Epoch:  95 	 Loss:  0.028923405334353447
Epoch:  96 	 Loss:  0.02892838418483734
Epoch:  97 	 Loss:  0.028673065826296806
Epoch:  98 	 Loss:  0.028600366786122322
Epoch:  99 	 Loss:  0.029274463653564453
Epoch:  100 	 Loss:  0.028116540983319283
wandb: Waiting for W&B process to finish, PID 1696780
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145046-qlesacfn/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145046-qlesacfn/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02812
wandb:        _step 800000
wandb:     _runtime 84
wandb:   _timestamp 1612900330
wandb:         loss 0.02812
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–ƒâ–ƒâ–„â–ƒâ–†â–â–„â–†â–†â–…â–…â–ƒâ–„â–…â–…â–ˆâ–ƒâ–‡â–†â–…â–…â–…â–„â–‡â–ƒâ–‡â–…â–„â–ˆâ–†â–‡â–ˆâ–‡â–„â–…â–„â–„â–‡â–…â–‚
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced robust-sweep-22: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/qlesacfn
wandb: Agent Starting Run: 1b2amplg with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:52:16.751713: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:52:16.756655: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run pleasant-sweep-23
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/1b2amplg
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_145214-1b2amplg
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02722725085914135
Epoch:  2 	 Loss:  0.027370864525437355
Epoch:  3 	 Loss:  0.02738223597407341
Epoch:  4 	 Loss:  0.02751597948372364
Epoch:  5 	 Loss:  0.02693614922463894
Epoch:  6 	 Loss:  0.02772754803299904
Epoch:  7 	 Loss:  0.027180658653378487
Epoch:  8 	 Loss:  0.02780117467045784
Epoch:  9 	 Loss:  0.027255045250058174
Epoch:  10 	 Loss:  0.027591517195105553
Epoch:  11 	 Loss:  0.0272330641746521
Epoch:  12 	 Loss:  0.027157146483659744
Epoch:  13 	 Loss:  0.027733759954571724
Epoch:  14 	 Loss:  0.027834126725792885
Epoch:  15 	 Loss:  0.027584778144955635
Epoch:  16 	 Loss:  0.027618784457445145
Epoch:  17 	 Loss:  0.02759423665702343
Epoch:  18 	 Loss:  0.02778800018131733
Epoch:  19 	 Loss:  0.02772803045809269
Epoch:  20 	 Loss:  0.026686694473028183
Epoch:  21 	 Loss:  0.0270353052765131
Epoch:  22 	 Loss:  0.02741272933781147
Epoch:  23 	 Loss:  0.027692236006259918
Epoch:  24 	 Loss:  0.026648594066500664
Epoch:  25 	 Loss:  0.027371695265173912
Epoch:  26 	 Loss:  0.027720093727111816
Epoch:  27 	 Loss:  0.027352241799235344
Epoch:  28 	 Loss:  0.027350250631570816
Epoch:  29 	 Loss:  0.026857610791921616
Epoch:  30 	 Loss:  0.027984457090497017
Epoch:  31 	 Loss:  0.027378443628549576
Epoch:  32 	 Loss:  0.027738815173506737
Epoch:  33 	 Loss:  0.026964958757162094
Epoch:  34 	 Loss:  0.02746983990073204
Epoch:  35 	 Loss:  0.02721293643116951
Epoch:  36 	 Loss:  0.027322832494974136
Epoch:  37 	 Loss:  0.027573566883802414
Epoch:  38 	 Loss:  0.027476295828819275
Epoch:  39 	 Loss:  0.02689524181187153
Epoch:  40 	 Loss:  0.027470644563436508
Epoch:  41 	 Loss:  0.02742461860179901
Epoch:  42 	 Loss:  0.027380384504795074
Epoch:  43 	 Loss:  0.027516616508364677
Epoch:  44 	 Loss:  0.027595020830631256
Epoch:  45 	 Loss:  0.02767830714583397
Epoch:  46 	 Loss:  0.02728447876870632
Epoch:  47 	 Loss:  0.027443235740065575
Epoch:  48 	 Loss:  0.027881860733032227
Epoch:  49 	 Loss:  0.027505673468112946
Epoch:  50 	 Loss:  0.02688463218510151
Epoch:  51 	 Loss:  0.02775106579065323
Epoch:  52 	 Loss:  0.027470005676150322
Epoch:  53 	 Loss:  0.027219321578741074
Epoch:  54 	 Loss:  0.027581186965107918
Epoch:  55 	 Loss:  0.02716025523841381
Epoch:  56 	 Loss:  0.027432935312390327
Epoch:  57 	 Loss:  0.02780000865459442
Epoch:  58 	 Loss:  0.027175521478056908
Epoch:  59 	 Loss:  0.02714567445218563
Epoch:  60 	 Loss:  0.02791217714548111
Epoch:  61 	 Loss:  0.027390360832214355
Epoch:  62 	 Loss:  0.027700835838913918
Epoch:  63 	 Loss:  0.027572061866521835
Epoch:  64 	 Loss:  0.027274107560515404
Epoch:  65 	 Loss:  0.027278266847133636
Epoch:  66 	 Loss:  0.027423648163676262
Epoch:  67 	 Loss:  0.027409767732024193
Epoch:  68 	 Loss:  0.027292197570204735
Epoch:  69 	 Loss:  0.027698460966348648
Epoch:  70 	 Loss:  0.027715394273400307
Epoch:  71 	 Loss:  0.027591805905103683
Epoch:  72 	 Loss:  0.027487924322485924
Epoch:  73 	 Loss:  0.027512745931744576
Epoch:  74 	 Loss:  0.027505356818437576
Epoch:  75 	 Loss:  0.027545511722564697
Epoch:  76 	 Loss:  0.0274802315980196
Epoch:  77 	 Loss:  0.027701685205101967
Epoch:  78 	 Loss:  0.02746841497719288
Epoch:  79 	 Loss:  0.02768561989068985
Epoch:  80 	 Loss:  0.027395304292440414
Epoch:  81 	 Loss:  0.02766648679971695
Epoch:  82 	 Loss:  0.027334850281476974
Epoch:  83 	 Loss:  0.027589453384280205
Epoch:  84 	 Loss:  0.0269846823066473
Epoch:  85 	 Loss:  0.0274776853621006
Epoch:  86 	 Loss:  0.0277940034866333
Epoch:  87 	 Loss:  0.027758704498410225
Epoch:  88 	 Loss:  0.02774300053715706
Epoch:  89 	 Loss:  0.02778889611363411
Epoch:  90 	 Loss:  0.027610760182142258
Epoch:  91 	 Loss:  0.02747294120490551
Epoch:  92 	 Loss:  0.027629274874925613
Epoch:  93 	 Loss:  0.027682693675160408
Epoch:  94 	 Loss:  0.0276247076690197
Epoch:  95 	 Loss:  0.027684923261404037
Epoch:  96 	 Loss:  0.027567729353904724
Epoch:  97 	 Loss:  0.027685031294822693
Epoch:  98 	 Loss:  0.02700912393629551
Epoch:  99 	 Loss:  0.027503399178385735
Epoch:  100 	 Loss:  0.02706647291779518
wandb: Waiting for W&B process to finish, PID 1700471
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145214-1b2amplg/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145214-1b2amplg/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02707
wandb:        _step 800000
wandb:     _runtime 83
wandb:   _timestamp 1612900417
wandb:         loss 0.02707
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–…â–‡â–ˆâ–„â–‡â–‡â–ˆâ–‚â–‡â–‡â–…â–…â–…â–„â–â–…â–†â–„â–†â–ˆâ–†â–…â–ƒâ–…â–„â–…â–‡â–†â–†â–‡â–‡â–„â–‚â–ˆâ–ˆâ–‡â–‡â–‡â–‚
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced pleasant-sweep-23: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/1b2amplg
wandb: Agent Starting Run: y0pcspfb with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:53:43.975861: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:53:43.983396: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run logical-sweep-24
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/y0pcspfb
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_145342-y0pcspfb
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.0265486016869545
Epoch:  2 	 Loss:  0.027334565296769142
Epoch:  3 	 Loss:  0.027403106912970543
Epoch:  4 	 Loss:  0.02737296000123024
Epoch:  5 	 Loss:  0.027259495109319687
Epoch:  6 	 Loss:  0.026537029072642326
Epoch:  7 	 Loss:  0.027131175622344017
Epoch:  8 	 Loss:  0.02706029638648033
Epoch:  9 	 Loss:  0.027350667864084244
Epoch:  10 	 Loss:  0.02751811034977436
Epoch:  11 	 Loss:  0.026992321014404297
Epoch:  12 	 Loss:  0.02699541486799717
Epoch:  13 	 Loss:  0.02725619450211525
Epoch:  14 	 Loss:  0.027014626190066338
Epoch:  15 	 Loss:  0.027024392038583755
Epoch:  16 	 Loss:  0.027353405952453613
Epoch:  17 	 Loss:  0.027232063934206963
Epoch:  18 	 Loss:  0.02715599164366722
Epoch:  19 	 Loss:  0.0274041760712862
Epoch:  20 	 Loss:  0.027452418580651283
Epoch:  21 	 Loss:  0.02722415141761303
Epoch:  22 	 Loss:  0.027291513979434967
Epoch:  23 	 Loss:  0.027044503018260002
Epoch:  24 	 Loss:  0.027111245319247246
Epoch:  25 	 Loss:  0.027582287788391113
Epoch:  26 	 Loss:  0.02696962095797062
Epoch:  27 	 Loss:  0.027190322056412697
Epoch:  28 	 Loss:  0.02709454670548439
Epoch:  29 	 Loss:  0.027338316664099693
Epoch:  30 	 Loss:  0.027730226516723633
Epoch:  31 	 Loss:  0.026986652985215187
Epoch:  32 	 Loss:  0.027441225945949554
Epoch:  33 	 Loss:  0.027381088584661484
Epoch:  34 	 Loss:  0.02758365496993065
Epoch:  35 	 Loss:  0.027427246794104576
Epoch:  36 	 Loss:  0.0272830780595541
Epoch:  37 	 Loss:  0.02699333056807518
Epoch:  38 	 Loss:  0.027346134185791016
Epoch:  39 	 Loss:  0.027052778750658035
Epoch:  40 	 Loss:  0.027270179241895676
Epoch:  41 	 Loss:  0.027344433590769768
Epoch:  42 	 Loss:  0.02784862369298935
Epoch:  43 	 Loss:  0.02746889553964138
Epoch:  44 	 Loss:  0.027145670726895332
Epoch:  45 	 Loss:  0.027415351942181587
Epoch:  46 	 Loss:  0.027479231357574463
Epoch:  47 	 Loss:  0.027601754292845726
Epoch:  48 	 Loss:  0.027649249881505966
Epoch:  49 	 Loss:  0.02735917828977108
Epoch:  50 	 Loss:  0.027101052924990654
Epoch:  51 	 Loss:  0.027718691155314445
Epoch:  52 	 Loss:  0.027359947562217712
Epoch:  53 	 Loss:  0.027441667392849922
Epoch:  54 	 Loss:  0.027443870902061462
Epoch:  55 	 Loss:  0.02754967473447323
Epoch:  56 	 Loss:  0.027580663561820984
Epoch:  57 	 Loss:  0.028011566027998924
Epoch:  58 	 Loss:  0.02748950570821762
Epoch:  59 	 Loss:  0.026879336684942245
Epoch:  60 	 Loss:  0.027479322627186775
Epoch:  61 	 Loss:  0.02712671458721161
Epoch:  62 	 Loss:  0.027247585356235504
Epoch:  63 	 Loss:  0.027113258838653564
Epoch:  64 	 Loss:  0.027397915720939636
Epoch:  65 	 Loss:  0.027195224538445473
Epoch:  66 	 Loss:  0.027609186246991158
Epoch:  67 	 Loss:  0.02685118094086647
Epoch:  68 	 Loss:  0.0272219218313694
Epoch:  69 	 Loss:  0.027917977422475815
Epoch:  70 	 Loss:  0.026853708550333977
Epoch:  71 	 Loss:  0.027121538296341896
Epoch:  72 	 Loss:  0.027399396523833275
Epoch:  73 	 Loss:  0.027554994449019432
Epoch:  74 	 Loss:  0.02722901478409767
Epoch:  75 	 Loss:  0.02746790274977684
Epoch:  76 	 Loss:  0.027442868798971176
Epoch:  77 	 Loss:  0.027319563552737236
Epoch:  78 	 Loss:  0.027610553428530693
Epoch:  79 	 Loss:  0.027318334206938744
Epoch:  80 	 Loss:  0.027305785566568375
Epoch:  81 	 Loss:  0.027843503281474113
Epoch:  82 	 Loss:  0.027232913300395012
Epoch:  83 	 Loss:  0.026955686509609222
Epoch:  84 	 Loss:  0.027448076754808426
Epoch:  85 	 Loss:  0.027094649150967598
Epoch:  86 	 Loss:  0.02729203924536705
Epoch:  87 	 Loss:  0.02743573673069477
Epoch:  88 	 Loss:  0.02715878188610077
Epoch:  89 	 Loss:  0.027903463691473007
Epoch:  90 	 Loss:  0.02747057005763054
Epoch:  91 	 Loss:  0.02728535607457161
Epoch:  92 	 Loss:  0.02718638814985752
Epoch:  93 	 Loss:  0.027698108926415443
Epoch:  94 	 Loss:  0.027349131181836128
Epoch:  95 	 Loss:  0.027619268745183945
Epoch:  96 	 Loss:  0.02737865410745144
Epoch:  97 	 Loss:  0.02720021829009056
Epoch:  98 	 Loss:  0.02784259058535099
Epoch:  99 	 Loss:  0.026964958757162094
Epoch:  100 	 Loss:  0.027143504470586777
wandb: Waiting for W&B process to finish, PID 1704161
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145342-y0pcspfb/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145342-y0pcspfb/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02714
wandb:        _step 800000
wandb:     _runtime 83
wandb:   _timestamp 1612900505
wandb:         loss 0.02714
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–â–…â–â–„â–ƒâ–…â–…â–„â–„â–„â–ƒâ–„â–ƒâ–†â–…â–„â–…â–„â–†â–…â–‡â–†â–†â–ƒâ–„â–…â–ƒâ–ˆâ–…â–…â–…â–…â–…â–†â–†â–ˆâ–„â–…â–„â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced logical-sweep-24: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/y0pcspfb
wandb: Agent Starting Run: 31vjh3gh with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: adam
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:55:15.530780: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:55:15.537536: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run ethereal-sweep-25
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/31vjh3gh
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_145513-31vjh3gh
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02811967022716999
Epoch:  2 	 Loss:  0.028011854737997055
Epoch:  3 	 Loss:  0.028047116473317146
Epoch:  4 	 Loss:  0.02816038951277733
Epoch:  5 	 Loss:  0.027710961177945137
Epoch:  6 	 Loss:  0.02730538323521614
Epoch:  7 	 Loss:  0.028298484161496162
Epoch:  8 	 Loss:  0.02832019329071045
Epoch:  9 	 Loss:  0.028171241283416748
Epoch:  10 	 Loss:  0.027956727892160416
Epoch:  11 	 Loss:  0.028328023850917816
Epoch:  12 	 Loss:  0.027817463502287865
Epoch:  13 	 Loss:  0.027731841430068016
Epoch:  14 	 Loss:  0.02820780500769615
Epoch:  15 	 Loss:  0.027694454416632652
Epoch:  16 	 Loss:  0.028249382972717285
Epoch:  17 	 Loss:  0.028307007625699043
Epoch:  18 	 Loss:  0.027674442157149315
Epoch:  19 	 Loss:  0.028320657089352608
Epoch:  20 	 Loss:  0.027867624536156654
Epoch:  21 	 Loss:  0.02877887897193432
Epoch:  22 	 Loss:  0.028142176568508148
Epoch:  23 	 Loss:  0.027798740193247795
Epoch:  24 	 Loss:  0.0275022704154253
Epoch:  25 	 Loss:  0.027963954955339432
Epoch:  26 	 Loss:  0.02859332039952278
Epoch:  27 	 Loss:  0.02800825424492359
Epoch:  28 	 Loss:  0.02765210159122944
Epoch:  29 	 Loss:  0.028367089107632637
Epoch:  30 	 Loss:  0.02756641060113907
Epoch:  31 	 Loss:  0.027393344789743423
Epoch:  32 	 Loss:  0.027819808572530746
Epoch:  33 	 Loss:  0.027715783566236496
Epoch:  34 	 Loss:  0.02855055220425129
Epoch:  35 	 Loss:  0.028550945222377777
Epoch:  36 	 Loss:  0.028080372139811516
Epoch:  37 	 Loss:  0.028070170432329178
Epoch:  38 	 Loss:  0.0281805619597435
Epoch:  39 	 Loss:  0.028202267363667488
Epoch:  40 	 Loss:  0.028191015124320984
Epoch:  41 	 Loss:  0.0275418758392334
Epoch:  42 	 Loss:  0.0280656386166811
Epoch:  43 	 Loss:  0.027573127299547195
Epoch:  44 	 Loss:  0.027787717059254646
Epoch:  45 	 Loss:  0.02816295623779297
Epoch:  46 	 Loss:  0.027750803157687187
Epoch:  47 	 Loss:  0.028263315558433533
Epoch:  48 	 Loss:  0.02772928960621357
Epoch:  49 	 Loss:  0.027795640751719475
Epoch:  50 	 Loss:  0.02756504714488983
Epoch:  51 	 Loss:  0.027940085157752037
Epoch:  52 	 Loss:  0.02779054269194603
Epoch:  53 	 Loss:  0.028192905709147453
Epoch:  54 	 Loss:  0.028191817924380302
Epoch:  55 	 Loss:  0.028218960389494896
Epoch:  56 	 Loss:  0.028287507593631744
Epoch:  57 	 Loss:  0.02808988466858864
Epoch:  58 	 Loss:  0.027844693511724472
Epoch:  59 	 Loss:  0.0276318546384573
Epoch:  60 	 Loss:  0.02836354821920395
Epoch:  61 	 Loss:  0.0277844350785017
Epoch:  62 	 Loss:  0.028190817683935165
Epoch:  63 	 Loss:  0.027961868792772293
Epoch:  64 	 Loss:  0.02810535579919815
Epoch:  65 	 Loss:  0.02734796330332756
Epoch:  66 	 Loss:  0.02831215038895607
Epoch:  67 	 Loss:  0.02825203165411949
Epoch:  68 	 Loss:  0.027889948338270187
Epoch:  69 	 Loss:  0.028023673221468925
Epoch:  70 	 Loss:  0.028360072523355484
Epoch:  71 	 Loss:  0.027568554505705833
Epoch:  72 	 Loss:  0.027974283322691917
Epoch:  73 	 Loss:  0.0282418392598629
Epoch:  74 	 Loss:  0.02796112932264805
Epoch:  75 	 Loss:  0.027725709602236748
Epoch:  76 	 Loss:  0.02831950969994068
Epoch:  77 	 Loss:  0.028448360040783882
Epoch:  78 	 Loss:  0.028270991519093513
Epoch:  79 	 Loss:  0.02778235264122486
Epoch:  80 	 Loss:  0.02745015360414982
Epoch:  81 	 Loss:  0.027885330840945244
Epoch:  82 	 Loss:  0.028284989297389984
Epoch:  83 	 Loss:  0.02866249345242977
Epoch:  84 	 Loss:  0.02775556407868862
Epoch:  85 	 Loss:  0.02816121280193329
Epoch:  86 	 Loss:  0.02771403267979622
Epoch:  87 	 Loss:  0.028397779911756516
Epoch:  88 	 Loss:  0.02766207419335842
Epoch:  89 	 Loss:  0.027800701558589935
Epoch:  90 	 Loss:  0.02860252745449543
Epoch:  91 	 Loss:  0.02835138328373432
Epoch:  92 	 Loss:  0.02781141735613346
Epoch:  93 	 Loss:  0.028225114569067955
Epoch:  94 	 Loss:  0.027901360765099525
Epoch:  95 	 Loss:  0.027564411982893944
Epoch:  96 	 Loss:  0.02794256992638111
Epoch:  97 	 Loss:  0.027678057551383972
Epoch:  98 	 Loss:  0.02808179333806038
Epoch:  99 	 Loss:  0.027462607249617577
Epoch:  100 	 Loss:  0.028345365077257156
wandb: Waiting for W&B process to finish, PID 1707853
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145513-31vjh3gh/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145513-31vjh3gh/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02835
wandb:        _step 800000
wandb:     _runtime 84
wandb:   _timestamp 1612900597
wandb:         loss 0.02835
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–…â–â–†â–†â–ƒâ–…â–ƒâ–ˆâ–ƒâ–‡â–ƒâ–â–‡â–…â–…â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–†â–ƒâ–ƒâ–…â–…â–„â–„â–„â–†â–ƒâ–†â–ƒâ–†â–ƒâ–ƒâ–„â–ƒâ–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced ethereal-sweep-25: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/31vjh3gh
wandb: Agent Starting Run: drj7o18v with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:56:45.977877: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:56:45.984309: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run effortless-sweep-26
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/drj7o18v
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_145644-drj7o18v
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02741536684334278
Epoch:  2 	 Loss:  0.02804173342883587
Epoch:  3 	 Loss:  0.027613883838057518
Epoch:  4 	 Loss:  0.028042711317539215
Epoch:  5 	 Loss:  0.027613749727606773
Epoch:  6 	 Loss:  0.02793196029961109
Epoch:  7 	 Loss:  0.027993466705083847
Epoch:  8 	 Loss:  0.02757994830608368
Epoch:  9 	 Loss:  0.027945009991526604
Epoch:  10 	 Loss:  0.0275728702545166
Epoch:  11 	 Loss:  0.027702318504452705
Epoch:  12 	 Loss:  0.02743832767009735
Epoch:  13 	 Loss:  0.027374105527997017
Epoch:  14 	 Loss:  0.027344491332769394
Epoch:  15 	 Loss:  0.027387496083974838
Epoch:  16 	 Loss:  0.027695637196302414
Epoch:  17 	 Loss:  0.027265360578894615
Epoch:  18 	 Loss:  0.02771315537393093
Epoch:  19 	 Loss:  0.027475113049149513
Epoch:  20 	 Loss:  0.027647752314805984
Epoch:  21 	 Loss:  0.02739202417433262
Epoch:  22 	 Loss:  0.027281073853373528
Epoch:  23 	 Loss:  0.027441447600722313
Epoch:  24 	 Loss:  0.02739151194691658
Epoch:  25 	 Loss:  0.02732188068330288
Epoch:  26 	 Loss:  0.02756776288151741
Epoch:  27 	 Loss:  0.02784574031829834
Epoch:  28 	 Loss:  0.027652515098452568
Epoch:  29 	 Loss:  0.027581719681620598
Epoch:  30 	 Loss:  0.02704627253115177
Epoch:  31 	 Loss:  0.027603987604379654
Epoch:  32 	 Loss:  0.02796218916773796
Epoch:  33 	 Loss:  0.02746264636516571
Epoch:  34 	 Loss:  0.027650387957692146
Epoch:  35 	 Loss:  0.02771298587322235
Epoch:  36 	 Loss:  0.02725980430841446
Epoch:  37 	 Loss:  0.0275846216827631
Epoch:  38 	 Loss:  0.02755386009812355
Epoch:  39 	 Loss:  0.027042215690016747
Epoch:  40 	 Loss:  0.02777942828834057
Epoch:  41 	 Loss:  0.02772187814116478
Epoch:  42 	 Loss:  0.02756408601999283
Epoch:  43 	 Loss:  0.027426844462752342
Epoch:  44 	 Loss:  0.027955716475844383
Epoch:  45 	 Loss:  0.02774103730916977
Epoch:  46 	 Loss:  0.02740006148815155
Epoch:  47 	 Loss:  0.026732243597507477
Epoch:  48 	 Loss:  0.02766430750489235
Epoch:  49 	 Loss:  0.027474315837025642
Epoch:  50 	 Loss:  0.027632232755422592
Epoch:  51 	 Loss:  0.027229372411966324
Epoch:  52 	 Loss:  0.027574222534894943
Epoch:  53 	 Loss:  0.027684839442372322
Epoch:  54 	 Loss:  0.027743911370635033
Epoch:  55 	 Loss:  0.027151716873049736
Epoch:  56 	 Loss:  0.027265185490250587
Epoch:  57 	 Loss:  0.027830537408590317
Epoch:  58 	 Loss:  0.02736673876643181
Epoch:  59 	 Loss:  0.027489768341183662
Epoch:  60 	 Loss:  0.027925441041588783
Epoch:  61 	 Loss:  0.028209513053297997
Epoch:  62 	 Loss:  0.02763081155717373
Epoch:  63 	 Loss:  0.027150489389896393
Epoch:  64 	 Loss:  0.027883950620889664
Epoch:  65 	 Loss:  0.027454275637865067
Epoch:  66 	 Loss:  0.02773416042327881
Epoch:  67 	 Loss:  0.026836102828383446
Epoch:  68 	 Loss:  0.02736087329685688
Epoch:  69 	 Loss:  0.027712758630514145
Epoch:  70 	 Loss:  0.027434831485152245
Epoch:  71 	 Loss:  0.02775195986032486
Epoch:  72 	 Loss:  0.027624445036053658
Epoch:  73 	 Loss:  0.028009528294205666
Epoch:  74 	 Loss:  0.02779426798224449
Epoch:  75 	 Loss:  0.027703838422894478
Epoch:  76 	 Loss:  0.027179008349776268
Epoch:  77 	 Loss:  0.027322053909301758
Epoch:  78 	 Loss:  0.027310151606798172
Epoch:  79 	 Loss:  0.027926256880164146
Epoch:  80 	 Loss:  0.02725711092352867
Epoch:  81 	 Loss:  0.02739713154733181
Epoch:  82 	 Loss:  0.02756733074784279
Epoch:  83 	 Loss:  0.027469011023640633
Epoch:  84 	 Loss:  0.027612030506134033
Epoch:  85 	 Loss:  0.027009693905711174
Epoch:  86 	 Loss:  0.027840998023748398
Epoch:  87 	 Loss:  0.0272050853818655
Epoch:  88 	 Loss:  0.027165137231349945
Epoch:  89 	 Loss:  0.02744750678539276
Epoch:  90 	 Loss:  0.027614863589406013
Epoch:  91 	 Loss:  0.027448471635580063
Epoch:  92 	 Loss:  0.02743224799633026
Epoch:  93 	 Loss:  0.027170466259121895
Epoch:  94 	 Loss:  0.02746623381972313
Epoch:  95 	 Loss:  0.0266813226044178
Epoch:  96 	 Loss:  0.026681607589125633
Epoch:  97 	 Loss:  0.027683651074767113
Epoch:  98 	 Loss:  0.027924662455916405
Epoch:  99 	 Loss:  0.02761823870241642
Epoch:  100 	 Loss:  0.028263645246624947
wandb: Waiting for W&B process to finish, PID 1711582
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145644-drj7o18v/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145644-drj7o18v/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02826
wandb:        _step 800000
wandb:     _runtime 85
wandb:   _timestamp 1612900689
wandb:         loss 0.02826
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–…â–†â–…â–…â–„â–…â–…â–„â–„â–…â–…â–…â–…â–ƒâ–‚â–…â–†â–„â–„â–ƒâ–…â–ƒâ–„â–ˆâ–†â–â–…â–…â–†â–ƒâ–†â–…â–…â–ƒâ–„â–„â–„â–…â–ˆ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced effortless-sweep-26: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/drj7o18v
wandb: Agent Starting Run: 123vxob7 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:58:15.551767: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:58:15.556784: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run lucky-sweep-27
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/123vxob7
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_145813-123vxob7
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02828417904675007
Epoch:  2 	 Loss:  0.02784629352390766
Epoch:  3 	 Loss:  0.028601113706827164
Epoch:  4 	 Loss:  0.027456412091851234
Epoch:  5 	 Loss:  0.027396518737077713
Epoch:  6 	 Loss:  0.02838391438126564
Epoch:  7 	 Loss:  0.02843490056693554
Epoch:  8 	 Loss:  0.028512917459011078
Epoch:  9 	 Loss:  0.02792578563094139
Epoch:  10 	 Loss:  0.028710121288895607
Epoch:  11 	 Loss:  0.02740541659295559
Epoch:  12 	 Loss:  0.028090136125683784
Epoch:  13 	 Loss:  0.02848970890045166
Epoch:  14 	 Loss:  0.027850132435560226
Epoch:  15 	 Loss:  0.028358792886137962
Epoch:  16 	 Loss:  0.02850213274359703
Epoch:  17 	 Loss:  0.028182145208120346
Epoch:  18 	 Loss:  0.02811492048203945
Epoch:  19 	 Loss:  0.028293389827013016
Epoch:  20 	 Loss:  0.02843635343015194
Epoch:  21 	 Loss:  0.028399545699357986
Epoch:  22 	 Loss:  0.02825785242021084
Epoch:  23 	 Loss:  0.027623571455478668
Epoch:  24 	 Loss:  0.02847333997488022
Epoch:  25 	 Loss:  0.027788495644927025
Epoch:  26 	 Loss:  0.02762904018163681
Epoch:  27 	 Loss:  0.028763780370354652
Epoch:  28 	 Loss:  0.02783823572099209
Epoch:  29 	 Loss:  0.027729135006666183
Epoch:  30 	 Loss:  0.028427081182599068
Epoch:  31 	 Loss:  0.027361826971173286
Epoch:  32 	 Loss:  0.028072375804185867
Epoch:  33 	 Loss:  0.02819984219968319
Epoch:  34 	 Loss:  0.02801833674311638
Epoch:  35 	 Loss:  0.027820223942399025
Epoch:  36 	 Loss:  0.028075886890292168
Epoch:  37 	 Loss:  0.02820560149848461
Epoch:  38 	 Loss:  0.028080495074391365
Epoch:  39 	 Loss:  0.028374480083584785
Epoch:  40 	 Loss:  0.02821045182645321
Epoch:  41 	 Loss:  0.028132515028119087
Epoch:  42 	 Loss:  0.02777072601020336
Epoch:  43 	 Loss:  0.02743140049278736
Epoch:  44 	 Loss:  0.02791919931769371
Epoch:  45 	 Loss:  0.028128203004598618
Epoch:  46 	 Loss:  0.028067776933312416
Epoch:  47 	 Loss:  0.02842262014746666
Epoch:  48 	 Loss:  0.02792944386601448
Epoch:  49 	 Loss:  0.02812192402780056
Epoch:  50 	 Loss:  0.028358835726976395
Epoch:  51 	 Loss:  0.027967315167188644
Epoch:  52 	 Loss:  0.028297781944274902
Epoch:  53 	 Loss:  0.02820507436990738
Epoch:  54 	 Loss:  0.02804776281118393
Epoch:  55 	 Loss:  0.02802392654120922
Epoch:  56 	 Loss:  0.027891099452972412
Epoch:  57 	 Loss:  0.028352994471788406
Epoch:  58 	 Loss:  0.028060389682650566
Epoch:  59 	 Loss:  0.028077904134988785
Epoch:  60 	 Loss:  0.028296031057834625
Epoch:  61 	 Loss:  0.02803056873381138
Epoch:  62 	 Loss:  0.028364621102809906
Epoch:  63 	 Loss:  0.027894198894500732
Epoch:  64 	 Loss:  0.02858998067677021
Epoch:  65 	 Loss:  0.02776101790368557
Epoch:  66 	 Loss:  0.028160016983747482
Epoch:  67 	 Loss:  0.028640402480959892
Epoch:  68 	 Loss:  0.02780960127711296
Epoch:  69 	 Loss:  0.028101246803998947
Epoch:  70 	 Loss:  0.027984824031591415
Epoch:  71 	 Loss:  0.027920421212911606
Epoch:  72 	 Loss:  0.02870255336165428
Epoch:  73 	 Loss:  0.02827521413564682
Epoch:  74 	 Loss:  0.028529969975352287
Epoch:  75 	 Loss:  0.028023531660437584
Epoch:  76 	 Loss:  0.027864262461662292
Epoch:  77 	 Loss:  0.027917183935642242
Epoch:  78 	 Loss:  0.028770020231604576
Epoch:  79 	 Loss:  0.028102947399020195
Epoch:  80 	 Loss:  0.028098542243242264
Epoch:  81 	 Loss:  0.028823595494031906
Epoch:  82 	 Loss:  0.027702247723937035
Epoch:  83 	 Loss:  0.02828902006149292
Epoch:  84 	 Loss:  0.02885362319648266
Epoch:  85 	 Loss:  0.028188424184918404
Epoch:  86 	 Loss:  0.02834431082010269
Epoch:  87 	 Loss:  0.027869518846273422
Epoch:  88 	 Loss:  0.028172649443149567
Epoch:  89 	 Loss:  0.027488993480801582
Epoch:  90 	 Loss:  0.02836916409432888
Epoch:  91 	 Loss:  0.02815723791718483
Epoch:  92 	 Loss:  0.028094308450818062
Epoch:  93 	 Loss:  0.027891764417290688
Epoch:  94 	 Loss:  0.027994243428111076
Epoch:  95 	 Loss:  0.028218980878591537
Epoch:  96 	 Loss:  0.028460659086704254
Epoch:  97 	 Loss:  0.027821211144328117
Epoch:  98 	 Loss:  0.027829434722661972
Epoch:  99 	 Loss:  0.028442800045013428
Epoch:  100 	 Loss:  0.027608243748545647
wandb: Waiting for W&B process to finish, PID 1715276
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145813-123vxob7/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145813-123vxob7/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02761
wandb:        _step 800000
wandb:     _runtime 86
wandb:   _timestamp 1612900779
wandb:         loss 0.02761
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–‡â–†â–†â–â–†â–†â–…â–†â–‚â–‚â–ƒâ–â–„â–„â–†â–…â–„â–„â–…â–„â–„â–ƒâ–„â–„â–‡â–‡â–„â–‡â–†â–„â–„â–ƒâ–ˆâ–ƒâ–‚â–„â–„â–ƒâ–‚
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced lucky-sweep-27: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/123vxob7
wandb: Agent Starting Run: 1mo7jc5r with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 14:59:45.396689: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 14:59:45.403845: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run silver-sweep-28
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/1mo7jc5r
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_145943-1mo7jc5r
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027559421956539154
Epoch:  2 	 Loss:  0.027582043781876564
Epoch:  3 	 Loss:  0.027745403349399567
Epoch:  4 	 Loss:  0.027949173003435135
Epoch:  5 	 Loss:  0.027558986097574234
Epoch:  6 	 Loss:  0.027615629136562347
Epoch:  7 	 Loss:  0.02733670361340046
Epoch:  8 	 Loss:  0.027634862810373306
Epoch:  9 	 Loss:  0.028001610189676285
Epoch:  10 	 Loss:  0.027822213247418404
Epoch:  11 	 Loss:  0.027583548799157143
Epoch:  12 	 Loss:  0.027625583112239838
Epoch:  13 	 Loss:  0.028101645410060883
Epoch:  14 	 Loss:  0.02745780162513256
Epoch:  15 	 Loss:  0.02788940817117691
Epoch:  16 	 Loss:  0.027089720591902733
Epoch:  17 	 Loss:  0.028096990659832954
Epoch:  18 	 Loss:  0.027982253581285477
Epoch:  19 	 Loss:  0.028063878417015076
Epoch:  20 	 Loss:  0.027633892372250557
Epoch:  21 	 Loss:  0.027677064761519432
Epoch:  22 	 Loss:  0.0275946706533432
Epoch:  23 	 Loss:  0.02753405272960663
Epoch:  24 	 Loss:  0.027590099722146988
Epoch:  25 	 Loss:  0.02743808552622795
Epoch:  26 	 Loss:  0.02749723754823208
Epoch:  27 	 Loss:  0.02814292348921299
Epoch:  28 	 Loss:  0.02770129032433033
Epoch:  29 	 Loss:  0.02765420638024807
Epoch:  30 	 Loss:  0.027617618441581726
Epoch:  31 	 Loss:  0.027979668229818344
Epoch:  32 	 Loss:  0.027558164671063423
Epoch:  33 	 Loss:  0.027874061837792397
Epoch:  34 	 Loss:  0.027486098930239677
Epoch:  35 	 Loss:  0.027662452310323715
Epoch:  36 	 Loss:  0.027237484231591225
Epoch:  37 	 Loss:  0.02792019210755825
Epoch:  38 	 Loss:  0.027896055951714516
Epoch:  39 	 Loss:  0.028081700205802917
Epoch:  40 	 Loss:  0.02772049605846405
Epoch:  41 	 Loss:  0.027724962681531906
Epoch:  42 	 Loss:  0.02770829200744629
Epoch:  43 	 Loss:  0.02779645100235939
Epoch:  44 	 Loss:  0.02801538072526455
Epoch:  45 	 Loss:  0.027744250372052193
Epoch:  46 	 Loss:  0.027781009674072266
Epoch:  47 	 Loss:  0.028082000091671944
Epoch:  48 	 Loss:  0.027968939393758774
Epoch:  49 	 Loss:  0.027907472103834152
Epoch:  50 	 Loss:  0.027682969346642494
Epoch:  51 	 Loss:  0.027648862451314926
Epoch:  52 	 Loss:  0.028086230158805847
Epoch:  53 	 Loss:  0.027209805324673653
Epoch:  54 	 Loss:  0.02739560231566429
Epoch:  55 	 Loss:  0.027900388464331627
Epoch:  56 	 Loss:  0.027721192687749863
Epoch:  57 	 Loss:  0.02822674997150898
Epoch:  58 	 Loss:  0.027558689936995506
Epoch:  59 	 Loss:  0.027598554268479347
Epoch:  60 	 Loss:  0.027179066091775894
Epoch:  61 	 Loss:  0.02759847231209278
Epoch:  62 	 Loss:  0.028071124106645584
Epoch:  63 	 Loss:  0.027944225817918777
Epoch:  64 	 Loss:  0.027735205367207527
Epoch:  65 	 Loss:  0.028149021789431572
Epoch:  66 	 Loss:  0.02770555391907692
Epoch:  67 	 Loss:  0.027886386960744858
Epoch:  68 	 Loss:  0.027624662965536118
Epoch:  69 	 Loss:  0.027874644845724106
Epoch:  70 	 Loss:  0.027817651629447937
Epoch:  71 	 Loss:  0.02745375595986843
Epoch:  72 	 Loss:  0.027470702305436134
Epoch:  73 	 Loss:  0.027969595044851303
Epoch:  74 	 Loss:  0.027797963470220566
Epoch:  75 	 Loss:  0.027521450072526932
Epoch:  76 	 Loss:  0.027532050386071205
Epoch:  77 	 Loss:  0.02806304581463337
Epoch:  78 	 Loss:  0.02791835181415081
Epoch:  79 	 Loss:  0.02792097069323063
Epoch:  80 	 Loss:  0.02720310166478157
Epoch:  81 	 Loss:  0.028003962710499763
Epoch:  82 	 Loss:  0.027465369552373886
Epoch:  83 	 Loss:  0.027940338477492332
Epoch:  84 	 Loss:  0.027966856956481934
Epoch:  85 	 Loss:  0.027406221255660057
Epoch:  86 	 Loss:  0.02781338058412075
Epoch:  87 	 Loss:  0.027994778007268906
Epoch:  88 	 Loss:  0.028100408613681793
Epoch:  89 	 Loss:  0.027865780517458916
Epoch:  90 	 Loss:  0.027386900037527084
Epoch:  91 	 Loss:  0.027328532189130783
Epoch:  92 	 Loss:  0.027918200939893723
Epoch:  93 	 Loss:  0.027841756120324135
Epoch:  94 	 Loss:  0.02782914601266384
Epoch:  95 	 Loss:  0.02774401195347309
Epoch:  96 	 Loss:  0.028115831315517426
Epoch:  97 	 Loss:  0.02743031457066536
Epoch:  98 	 Loss:  0.028099078685045242
Epoch:  99 	 Loss:  0.027447769418358803
Epoch:  100 	 Loss:  0.027832772582769394
wandb: Waiting for W&B process to finish, PID 1718970
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145943-1mo7jc5r/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_145943-1mo7jc5r/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02783
wandb:        _step 800000
wandb:     _runtime 86
wandb:   _timestamp 1612900869
wandb:         loss 0.02783
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–†â–…â–…â–„â–ˆâ–â–‡â–…â–„â–„â–…â–‡â–„â–‚â–ˆâ–…â–‡â–†â–‡â–…â–ƒâ–…â–…â–…â–…â–‡â–†â–„â–†â–ˆâ–‡â–„â–‡â–‡â–†â–‡â–†â–ƒâ–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced silver-sweep-28: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/1mo7jc5r
wandb: Agent Starting Run: 0p4f4jbl with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:01:15.209152: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:01:15.214749: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run vital-sweep-29
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/0p4f4jbl
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_150113-0p4f4jbl
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027818413451313972
Epoch:  2 	 Loss:  0.027342576533555984
Epoch:  3 	 Loss:  0.02745332196354866
Epoch:  4 	 Loss:  0.027875931933522224
Epoch:  5 	 Loss:  0.027990931645035744
Epoch:  6 	 Loss:  0.027379073202610016
Epoch:  7 	 Loss:  0.027807286009192467
Epoch:  8 	 Loss:  0.02754933014512062
Epoch:  9 	 Loss:  0.027535846456885338
Epoch:  10 	 Loss:  0.027836130931973457
Epoch:  11 	 Loss:  0.02808057703077793
Epoch:  12 	 Loss:  0.027871424332261086
Epoch:  13 	 Loss:  0.02809138409793377
Epoch:  14 	 Loss:  0.027703141793608665
Epoch:  15 	 Loss:  0.027268406003713608
Epoch:  16 	 Loss:  0.028124546632170677
Epoch:  17 	 Loss:  0.02782950922846794
Epoch:  18 	 Loss:  0.027748094871640205
Epoch:  19 	 Loss:  0.027607399970293045
Epoch:  20 	 Loss:  0.027528895065188408
Epoch:  21 	 Loss:  0.027613947167992592
Epoch:  22 	 Loss:  0.02735944837331772
Epoch:  23 	 Loss:  0.02805151417851448
Epoch:  24 	 Loss:  0.028154905885457993
Epoch:  25 	 Loss:  0.02767542190849781
Epoch:  26 	 Loss:  0.027509814128279686
Epoch:  27 	 Loss:  0.027652395889163017
Epoch:  28 	 Loss:  0.027516547590494156
Epoch:  29 	 Loss:  0.02771364338696003
Epoch:  30 	 Loss:  0.027661945670843124
Epoch:  31 	 Loss:  0.027796877548098564
Epoch:  32 	 Loss:  0.027046000584959984
Epoch:  33 	 Loss:  0.027951423078775406
Epoch:  34 	 Loss:  0.027348894625902176
Epoch:  35 	 Loss:  0.027675485238432884
Epoch:  36 	 Loss:  0.027855318039655685
Epoch:  37 	 Loss:  0.02759295143187046
Epoch:  38 	 Loss:  0.02770555019378662
Epoch:  39 	 Loss:  0.027669323608279228
Epoch:  40 	 Loss:  0.027532825246453285
Epoch:  41 	 Loss:  0.0274957362562418
Epoch:  42 	 Loss:  0.02765779383480549
Epoch:  43 	 Loss:  0.02758157253265381
Epoch:  44 	 Loss:  0.02770438976585865
Epoch:  45 	 Loss:  0.0277982410043478
Epoch:  46 	 Loss:  0.02729421854019165
Epoch:  47 	 Loss:  0.027910487726330757
Epoch:  48 	 Loss:  0.027649467810988426
Epoch:  49 	 Loss:  0.02743145078420639
Epoch:  50 	 Loss:  0.027755718678236008
Epoch:  51 	 Loss:  0.02802060730755329
Epoch:  52 	 Loss:  0.028011905029416084
Epoch:  53 	 Loss:  0.02783193811774254
Epoch:  54 	 Loss:  0.027954069897532463
Epoch:  55 	 Loss:  0.027618953958153725
Epoch:  56 	 Loss:  0.028225284069776535
Epoch:  57 	 Loss:  0.027850283309817314
Epoch:  58 	 Loss:  0.027660585939884186
Epoch:  59 	 Loss:  0.027564341202378273
Epoch:  60 	 Loss:  0.027124185115098953
Epoch:  61 	 Loss:  0.02777838706970215
Epoch:  62 	 Loss:  0.02752303145825863
Epoch:  63 	 Loss:  0.02843622863292694
Epoch:  64 	 Loss:  0.027664385735988617
Epoch:  65 	 Loss:  0.0273484755307436
Epoch:  66 	 Loss:  0.027481339871883392
Epoch:  67 	 Loss:  0.027295075356960297
Epoch:  68 	 Loss:  0.027640875428915024
Epoch:  69 	 Loss:  0.027826767414808273
Epoch:  70 	 Loss:  0.027709349989891052
Epoch:  71 	 Loss:  0.027731729671359062
Epoch:  72 	 Loss:  0.027542218565940857
Epoch:  73 	 Loss:  0.027901900932192802
Epoch:  74 	 Loss:  0.027398893609642982
Epoch:  75 	 Loss:  0.027092117816209793
Epoch:  76 	 Loss:  0.027902768924832344
Epoch:  77 	 Loss:  0.0274655818939209
Epoch:  78 	 Loss:  0.027853617444634438
Epoch:  79 	 Loss:  0.02823677286505699
Epoch:  80 	 Loss:  0.027410488575696945
Epoch:  81 	 Loss:  0.02799326181411743
Epoch:  82 	 Loss:  0.02812081016600132
Epoch:  83 	 Loss:  0.028269793838262558
Epoch:  84 	 Loss:  0.02778446115553379
Epoch:  85 	 Loss:  0.027715660631656647
Epoch:  86 	 Loss:  0.027462519705295563
Epoch:  87 	 Loss:  0.027668790891766548
Epoch:  88 	 Loss:  0.02787642367184162
Epoch:  89 	 Loss:  0.02780451998114586
Epoch:  90 	 Loss:  0.028018204495310783
Epoch:  91 	 Loss:  0.02786744199693203
Epoch:  92 	 Loss:  0.027064647525548935
Epoch:  93 	 Loss:  0.027467159554362297
Epoch:  94 	 Loss:  0.027744976803660393
Epoch:  95 	 Loss:  0.0276006031781435
Epoch:  96 	 Loss:  0.02759324200451374
Epoch:  97 	 Loss:  0.02770581655204296
Epoch:  98 	 Loss:  0.027325116097927094
Epoch:  99 	 Loss:  0.027721988037228584
Epoch:  100 	 Loss:  0.027255265042185783
wandb: Waiting for W&B process to finish, PID 1722671
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150113-0p4f4jbl/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150113-0p4f4jbl/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02726
wandb:        _step 800000
wandb:     _runtime 80
wandb:   _timestamp 1612900953
wandb:         loss 0.02726
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–ƒâ–ƒâ–„â–‡â–‡â–‡â–…â–„â–‡â–„â–„â–…â–ƒâ–†â–…â–„â–…â–‚â–ƒâ–‡â–†â–ˆâ–„â–…â–…â–‚â–†â–„â–ƒâ–ƒâ–ˆâ–‡â–…â–…â–…â–â–…â–…â–‚
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced vital-sweep-29: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/0p4f4jbl
wandb: Agent Starting Run: i95v54gq with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:02:39.872085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:02:39.877199: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run breezy-sweep-30
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/i95v54gq
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_150237-i95v54gq
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027502018958330154
Epoch:  2 	 Loss:  0.02762138471007347
Epoch:  3 	 Loss:  0.027706215158104897
Epoch:  4 	 Loss:  0.02781950868666172
Epoch:  5 	 Loss:  0.02746119350194931
Epoch:  6 	 Loss:  0.0272512249648571
Epoch:  7 	 Loss:  0.027561770752072334
Epoch:  8 	 Loss:  0.02779102325439453
Epoch:  9 	 Loss:  0.02788066305220127
Epoch:  10 	 Loss:  0.027365244925022125
Epoch:  11 	 Loss:  0.027896931394934654
Epoch:  12 	 Loss:  0.027292603626847267
Epoch:  13 	 Loss:  0.027584072202444077
Epoch:  14 	 Loss:  0.027762403711676598
Epoch:  15 	 Loss:  0.02757967822253704
Epoch:  16 	 Loss:  0.027605947107076645
Epoch:  17 	 Loss:  0.027436312288045883
Epoch:  18 	 Loss:  0.02803848870098591
Epoch:  19 	 Loss:  0.02755143865942955
Epoch:  20 	 Loss:  0.02718747965991497
Epoch:  21 	 Loss:  0.027337918058037758
Epoch:  22 	 Loss:  0.027581924572587013
Epoch:  23 	 Loss:  0.02709188684821129
Epoch:  24 	 Loss:  0.02796684391796589
Epoch:  25 	 Loss:  0.028002528473734856
Epoch:  26 	 Loss:  0.027631521224975586
Epoch:  27 	 Loss:  0.027610572054982185
Epoch:  28 	 Loss:  0.02763473428785801
Epoch:  29 	 Loss:  0.028281280770897865
Epoch:  30 	 Loss:  0.027460945770144463
Epoch:  31 	 Loss:  0.027887243777513504
Epoch:  32 	 Loss:  0.027031375095248222
Epoch:  33 	 Loss:  0.02789759263396263
Epoch:  34 	 Loss:  0.027428196743130684
Epoch:  35 	 Loss:  0.02802058681845665
Epoch:  36 	 Loss:  0.027672892436385155
Epoch:  37 	 Loss:  0.02774045616388321
Epoch:  38 	 Loss:  0.02767138183116913
Epoch:  39 	 Loss:  0.02785254828631878
Epoch:  40 	 Loss:  0.027329981327056885
Epoch:  41 	 Loss:  0.02774384804069996
Epoch:  42 	 Loss:  0.027683747932314873
Epoch:  43 	 Loss:  0.027921689674258232
Epoch:  44 	 Loss:  0.027231400832533836
Epoch:  45 	 Loss:  0.027392493560910225
Epoch:  46 	 Loss:  0.028139466419816017
Epoch:  47 	 Loss:  0.027791017666459084
Epoch:  48 	 Loss:  0.027790380641818047
Epoch:  49 	 Loss:  0.02803388424217701
Epoch:  50 	 Loss:  0.02810828946530819
Epoch:  51 	 Loss:  0.02777678705751896
Epoch:  52 	 Loss:  0.027200009673833847
Epoch:  53 	 Loss:  0.027829665690660477
Epoch:  54 	 Loss:  0.026937004178762436
Epoch:  55 	 Loss:  0.02804814837872982
Epoch:  56 	 Loss:  0.02786795049905777
Epoch:  57 	 Loss:  0.02804509922862053
Epoch:  58 	 Loss:  0.027424266561865807
Epoch:  59 	 Loss:  0.027668792754411697
Epoch:  60 	 Loss:  0.027877315878868103
Epoch:  61 	 Loss:  0.028050171211361885
Epoch:  62 	 Loss:  0.027559055015444756
Epoch:  63 	 Loss:  0.027268635109066963
Epoch:  64 	 Loss:  0.027856023982167244
Epoch:  65 	 Loss:  0.027663903310894966
Epoch:  66 	 Loss:  0.02765859104692936
Epoch:  67 	 Loss:  0.027763165533542633
Epoch:  68 	 Loss:  0.02762099914252758
Epoch:  69 	 Loss:  0.027752330526709557
Epoch:  70 	 Loss:  0.02808566577732563
Epoch:  71 	 Loss:  0.02811616286635399
Epoch:  72 	 Loss:  0.027514467015862465
Epoch:  73 	 Loss:  0.027206772938370705
Epoch:  74 	 Loss:  0.027190199121832848
Epoch:  75 	 Loss:  0.027695706114172935
Epoch:  76 	 Loss:  0.027988998219370842
Epoch:  77 	 Loss:  0.02774827741086483
Epoch:  78 	 Loss:  0.0277117770165205
Epoch:  79 	 Loss:  0.027725815773010254
Epoch:  80 	 Loss:  0.027414821088314056
Epoch:  81 	 Loss:  0.02787098102271557
Epoch:  82 	 Loss:  0.027513163164258003
Epoch:  83 	 Loss:  0.027530215680599213
Epoch:  84 	 Loss:  0.027606230229139328
Epoch:  85 	 Loss:  0.02775297686457634
Epoch:  86 	 Loss:  0.028110938146710396
Epoch:  87 	 Loss:  0.028033237904310226
Epoch:  88 	 Loss:  0.02792649157345295
Epoch:  89 	 Loss:  0.02782360278069973
Epoch:  90 	 Loss:  0.028098270297050476
Epoch:  91 	 Loss:  0.02789430320262909
Epoch:  92 	 Loss:  0.02796979621052742
Epoch:  93 	 Loss:  0.02775123342871666
Epoch:  94 	 Loss:  0.027850227430462837
Epoch:  95 	 Loss:  0.028173213824629784
Epoch:  96 	 Loss:  0.028023745864629745
Epoch:  97 	 Loss:  0.02772463671863079
Epoch:  98 	 Loss:  0.02806873992085457
Epoch:  99 	 Loss:  0.02774936892092228
Epoch:  100 	 Loss:  0.02750498428940773
wandb: Waiting for W&B process to finish, PID 1726354
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150237-i95v54gq/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150237-i95v54gq/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0275
wandb:        _step 800000
wandb:     _runtime 80
wandb:   _timestamp 1612901037
wandb:         loss 0.0275
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–…â–ƒâ–†â–‡â–…â–…â–‡â–ƒâ–‚â–…â–…â–‡â–„â–…â–†â–†â–ƒâ–ˆâ–‡â–†â–â–†â–…â–‡â–†â–†â–†â–„â–‚â–†â–†â–„â–…â–‡â–†â–‡â–†â–†â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced breezy-sweep-30: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/i95v54gq
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: kn0vj5bo with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:04:13.825915: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:04:13.830516: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run decent-sweep-31
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/kn0vj5bo
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_150411-kn0vj5bo
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027468176558613777
Epoch:  2 	 Loss:  0.02746698074042797
Epoch:  3 	 Loss:  0.027496878057718277
Epoch:  4 	 Loss:  0.02794245071709156
Epoch:  5 	 Loss:  0.02801835536956787
Epoch:  6 	 Loss:  0.027287619188427925
Epoch:  7 	 Loss:  0.027481703087687492
Epoch:  8 	 Loss:  0.027739359065890312
Epoch:  9 	 Loss:  0.027757789939641953
Epoch:  10 	 Loss:  0.027790138497948647
Epoch:  11 	 Loss:  0.028019940480589867
Epoch:  12 	 Loss:  0.027887560427188873
Epoch:  13 	 Loss:  0.028370220214128494
Epoch:  14 	 Loss:  0.027998516336083412
Epoch:  15 	 Loss:  0.027993055060505867
Epoch:  16 	 Loss:  0.027802614495158195
Epoch:  17 	 Loss:  0.028052857145667076
Epoch:  18 	 Loss:  0.02794969640672207
Epoch:  19 	 Loss:  0.027914922684431076
Epoch:  20 	 Loss:  0.02760808728635311
Epoch:  21 	 Loss:  0.027884721755981445
Epoch:  22 	 Loss:  0.02780860662460327
Epoch:  23 	 Loss:  0.027465371415019035
Epoch:  24 	 Loss:  0.02823718637228012
Epoch:  25 	 Loss:  0.027512842789292336
Epoch:  26 	 Loss:  0.027366863563656807
Epoch:  27 	 Loss:  0.02761303074657917
Epoch:  28 	 Loss:  0.02764013595879078
Epoch:  29 	 Loss:  0.02772543579339981
Epoch:  30 	 Loss:  0.027193697169423103
Epoch:  31 	 Loss:  0.027845138683915138
Epoch:  32 	 Loss:  0.02708994783461094
Epoch:  33 	 Loss:  0.02832888998091221
Epoch:  34 	 Loss:  0.027732228860259056
Epoch:  35 	 Loss:  0.027776040136814117
Epoch:  36 	 Loss:  0.027648260816931725
Epoch:  37 	 Loss:  0.027562109753489494
Epoch:  38 	 Loss:  0.02779443934559822
Epoch:  39 	 Loss:  0.027776556089520454
Epoch:  40 	 Loss:  0.027737200260162354
Epoch:  41 	 Loss:  0.027647238224744797
Epoch:  42 	 Loss:  0.02754046395421028
Epoch:  43 	 Loss:  0.028225380927324295
Epoch:  44 	 Loss:  0.027749793604016304
Epoch:  45 	 Loss:  0.027857964858412743
Epoch:  46 	 Loss:  0.02766818180680275
Epoch:  47 	 Loss:  0.027675142511725426
Epoch:  48 	 Loss:  0.027779726311564445
Epoch:  49 	 Loss:  0.027666453272104263
Epoch:  50 	 Loss:  0.027742166072130203
Epoch:  51 	 Loss:  0.02760274149477482
Epoch:  52 	 Loss:  0.028079096227884293
Epoch:  53 	 Loss:  0.027391396462917328
Epoch:  54 	 Loss:  0.027576038613915443
Epoch:  55 	 Loss:  0.027906201779842377
Epoch:  56 	 Loss:  0.027820104733109474
Epoch:  57 	 Loss:  0.027619091793894768
Epoch:  58 	 Loss:  0.027312379330396652
Epoch:  59 	 Loss:  0.027192527428269386
Epoch:  60 	 Loss:  0.027618110179901123
Epoch:  61 	 Loss:  0.027630824595689774
Epoch:  62 	 Loss:  0.02770933322608471
Epoch:  63 	 Loss:  0.028039894998073578
Epoch:  64 	 Loss:  0.027857981622219086
Epoch:  65 	 Loss:  0.028181634843349457
Epoch:  66 	 Loss:  0.027647266164422035
Epoch:  67 	 Loss:  0.02781142108142376
Epoch:  68 	 Loss:  0.02722897008061409
Epoch:  69 	 Loss:  0.028215128928422928
Epoch:  70 	 Loss:  0.028025951236486435
Epoch:  71 	 Loss:  0.027658864855766296
Epoch:  72 	 Loss:  0.02814415842294693
Epoch:  73 	 Loss:  0.027583956718444824
Epoch:  74 	 Loss:  0.02744979038834572
Epoch:  75 	 Loss:  0.027547001838684082
Epoch:  76 	 Loss:  0.02810012362897396
Epoch:  77 	 Loss:  0.027534879744052887
Epoch:  78 	 Loss:  0.027346987277269363
Epoch:  79 	 Loss:  0.027607912197709084
Epoch:  80 	 Loss:  0.02749154530465603
Epoch:  81 	 Loss:  0.028015293180942535
Epoch:  82 	 Loss:  0.027402834966778755
Epoch:  83 	 Loss:  0.02761809155344963
Epoch:  84 	 Loss:  0.02771700732409954
Epoch:  85 	 Loss:  0.02739488147199154
Epoch:  86 	 Loss:  0.027600470930337906
Epoch:  87 	 Loss:  0.027788087725639343
Epoch:  88 	 Loss:  0.027988437563180923
Epoch:  89 	 Loss:  0.027611562982201576
Epoch:  90 	 Loss:  0.027782734483480453
Epoch:  91 	 Loss:  0.027808763086795807
Epoch:  92 	 Loss:  0.027576550841331482
Epoch:  93 	 Loss:  0.027713216841220856
Epoch:  94 	 Loss:  0.027807358652353287
Epoch:  95 	 Loss:  0.02752021513879299
Epoch:  96 	 Loss:  0.02750025875866413
Epoch:  97 	 Loss:  0.027573704719543457
Epoch:  98 	 Loss:  0.02837459370493889
Epoch:  99 	 Loss:  0.027943002060055733
Epoch:  100 	 Loss:  0.027913538739085197
wandb: Waiting for W&B process to finish, PID 1730051
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150411-kn0vj5bo/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150411-kn0vj5bo/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02791
wandb:        _step 800000
wandb:     _runtime 79
wandb:   _timestamp 1612901131
wandb:         loss 0.02791
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–ƒâ–ƒâ–‚â–„â–†â–ˆâ–…â–†â–…â–ƒâ–‚â–„â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–…â–â–„â–…â–…â–‡â–‡â–ƒâ–ƒâ–ƒâ–‚â–„â–…â–ƒâ–ƒâ–…â–ƒâ–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced decent-sweep-31: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/kn0vj5bo
wandb: Agent Starting Run: 4t5qahre with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:05:37.242636: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:05:37.247743: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run easy-sweep-32
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/4t5qahre
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_150535-4t5qahre
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02814318612217903
Epoch:  2 	 Loss:  0.028009822592139244
Epoch:  3 	 Loss:  0.028772592544555664
Epoch:  4 	 Loss:  0.028690313920378685
Epoch:  5 	 Loss:  0.028532234951853752
Epoch:  6 	 Loss:  0.028173841536045074
Epoch:  7 	 Loss:  0.028802650049328804
Epoch:  8 	 Loss:  0.02821396477520466
Epoch:  9 	 Loss:  0.028527667745947838
Epoch:  10 	 Loss:  0.027714990079402924
Epoch:  11 	 Loss:  0.028660325333476067
Epoch:  12 	 Loss:  0.0287062618881464
Epoch:  13 	 Loss:  0.02835978753864765
Epoch:  14 	 Loss:  0.02839803136885166
Epoch:  15 	 Loss:  0.028234269469976425
Epoch:  16 	 Loss:  0.028822757303714752
Epoch:  17 	 Loss:  0.028614643961191177
Epoch:  18 	 Loss:  0.028732268139719963
Epoch:  19 	 Loss:  0.028763633221387863
Epoch:  20 	 Loss:  0.02848958410322666
Epoch:  21 	 Loss:  0.02813832461833954
Epoch:  22 	 Loss:  0.027839655056595802
Epoch:  23 	 Loss:  0.028071541339159012
Epoch:  24 	 Loss:  0.027930717915296555
Epoch:  25 	 Loss:  0.028414437547326088
Epoch:  26 	 Loss:  0.028268814086914062
Epoch:  27 	 Loss:  0.02807386964559555
Epoch:  28 	 Loss:  0.02832399494946003
Epoch:  29 	 Loss:  0.02845640666782856
Epoch:  30 	 Loss:  0.028586098924279213
Epoch:  31 	 Loss:  0.028073374181985855
Epoch:  32 	 Loss:  0.02892613410949707
Epoch:  33 	 Loss:  0.02852211892604828
Epoch:  34 	 Loss:  0.027894556522369385
Epoch:  35 	 Loss:  0.0283005703240633
Epoch:  36 	 Loss:  0.028284292668104172
Epoch:  37 	 Loss:  0.028886253014206886
Epoch:  38 	 Loss:  0.02803763560950756
Epoch:  39 	 Loss:  0.027833180502057076
Epoch:  40 	 Loss:  0.028749579563736916
Epoch:  41 	 Loss:  0.028277741745114326
Epoch:  42 	 Loss:  0.028454827144742012
Epoch:  43 	 Loss:  0.02827322483062744
Epoch:  44 	 Loss:  0.02811926230788231
Epoch:  45 	 Loss:  0.028024164959788322
Epoch:  46 	 Loss:  0.028649255633354187
Epoch:  47 	 Loss:  0.028337767347693443
Epoch:  48 	 Loss:  0.02847498655319214
Epoch:  49 	 Loss:  0.0284861009567976
Epoch:  50 	 Loss:  0.028180617839097977
Epoch:  51 	 Loss:  0.028332063928246498
Epoch:  52 	 Loss:  0.028521083295345306
Epoch:  53 	 Loss:  0.02905271016061306
Epoch:  54 	 Loss:  0.028304310515522957
Epoch:  55 	 Loss:  0.02798202820122242
Epoch:  56 	 Loss:  0.027667902410030365
Epoch:  57 	 Loss:  0.028215471655130386
Epoch:  58 	 Loss:  0.027976369485259056
Epoch:  59 	 Loss:  0.028715068474411964
Epoch:  60 	 Loss:  0.028053855523467064
Epoch:  61 	 Loss:  0.02824298106133938
Epoch:  62 	 Loss:  0.02882397547364235
Epoch:  63 	 Loss:  0.028277503326535225
Epoch:  64 	 Loss:  0.027945930138230324
Epoch:  65 	 Loss:  0.028728622943162918
Epoch:  66 	 Loss:  0.027936888858675957
Epoch:  67 	 Loss:  0.028522789478302002
Epoch:  68 	 Loss:  0.02831142581999302
Epoch:  69 	 Loss:  0.02801038697361946
Epoch:  70 	 Loss:  0.028539685532450676
Epoch:  71 	 Loss:  0.027807537466287613
Epoch:  72 	 Loss:  0.028266901150345802
Epoch:  73 	 Loss:  0.028167862445116043
Epoch:  74 	 Loss:  0.02843151055276394
Epoch:  75 	 Loss:  0.028688618913292885
Epoch:  76 	 Loss:  0.02799486555159092
Epoch:  77 	 Loss:  0.028548046946525574
Epoch:  78 	 Loss:  0.028240686282515526
Epoch:  79 	 Loss:  0.0288560651242733
Epoch:  80 	 Loss:  0.028661182150244713
Epoch:  81 	 Loss:  0.027942832559347153
Epoch:  82 	 Loss:  0.028436711058020592
Epoch:  83 	 Loss:  0.027319638058543205
Epoch:  84 	 Loss:  0.02879517897963524
Epoch:  85 	 Loss:  0.028369074687361717
Epoch:  86 	 Loss:  0.027872439473867416
Epoch:  87 	 Loss:  0.02845011278986931
Epoch:  88 	 Loss:  0.028632361441850662
Epoch:  89 	 Loss:  0.02805512771010399
Epoch:  90 	 Loss:  0.028229814022779465
Epoch:  91 	 Loss:  0.028637615963816643
Epoch:  92 	 Loss:  0.027983957901597023
Epoch:  93 	 Loss:  0.0285864919424057
Epoch:  94 	 Loss:  0.02822510153055191
Epoch:  95 	 Loss:  0.02768910489976406
Epoch:  96 	 Loss:  0.02846258506178856
Epoch:  97 	 Loss:  0.028766419738531113
Epoch:  98 	 Loss:  0.028234783560037613
Epoch:  99 	 Loss:  0.028430525213479996
Epoch:  100 	 Loss:  0.02806740067899227
wandb: Waiting for W&B process to finish, PID 1733736
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150535-4t5qahre/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150535-4t5qahre/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02807
wandb:        _step 800000
wandb:     _runtime 80
wandb:   _timestamp 1612901215
wandb:         loss 0.02807
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–ˆâ–„â–„â–‡â–…â–ˆâ–‡â–„â–ƒâ–…â–…â–ƒâ–‚â–…â–‚â–…â–„â–‡â–†â–…â–…â–â–‡â–„â–ƒâ–†â–ƒâ–…â–…â–†â–ˆâ–†â–ˆâ–†â–ƒâ–ƒâ–„â–‡â–ƒ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced easy-sweep-32: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/4t5qahre
wandb: Agent Starting Run: 25ouwepo with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:07:01.940821: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:07:01.948090: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run major-sweep-33
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/25ouwepo
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_150659-25ouwepo
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02912321500480175
Epoch:  2 	 Loss:  0.029592055827379227
Epoch:  3 	 Loss:  0.029035186395049095
Epoch:  4 	 Loss:  0.029464570805430412
Epoch:  5 	 Loss:  0.02884642407298088
Epoch:  6 	 Loss:  0.02960215136408806
Epoch:  7 	 Loss:  0.02873249538242817
Epoch:  8 	 Loss:  0.02914496511220932
Epoch:  9 	 Loss:  0.02866857871413231
Epoch:  10 	 Loss:  0.02883525937795639
Epoch:  11 	 Loss:  0.02898920141160488
Epoch:  12 	 Loss:  0.029055792838335037
Epoch:  13 	 Loss:  0.0289145577698946
Epoch:  14 	 Loss:  0.029352642595767975
Epoch:  15 	 Loss:  0.02908611297607422
Epoch:  16 	 Loss:  0.029205285012722015
Epoch:  17 	 Loss:  0.029585255309939384
Epoch:  18 	 Loss:  0.0286006610840559
Epoch:  19 	 Loss:  0.029069731011986732
Epoch:  20 	 Loss:  0.028404800221323967
Epoch:  21 	 Loss:  0.02862573228776455
Epoch:  22 	 Loss:  0.029299959540367126
Epoch:  23 	 Loss:  0.029038969427347183
Epoch:  24 	 Loss:  0.029012290760874748
Epoch:  25 	 Loss:  0.028813736513257027
Epoch:  26 	 Loss:  0.029079919680953026
Epoch:  27 	 Loss:  0.02909117378294468
Epoch:  28 	 Loss:  0.028974225744605064
Epoch:  29 	 Loss:  0.0287014190107584
Epoch:  30 	 Loss:  0.029172204434871674
Epoch:  31 	 Loss:  0.028944775462150574
Epoch:  32 	 Loss:  0.029131218791007996
Epoch:  33 	 Loss:  0.02874836139380932
Epoch:  34 	 Loss:  0.029118550941348076
Epoch:  35 	 Loss:  0.029189499095082283
Epoch:  36 	 Loss:  0.028823044151067734
Epoch:  37 	 Loss:  0.02944115921854973
Epoch:  38 	 Loss:  0.02868645451962948
Epoch:  39 	 Loss:  0.028898125514388084
Epoch:  40 	 Loss:  0.028751837089657784
Epoch:  41 	 Loss:  0.02941705472767353
Epoch:  42 	 Loss:  0.02896088920533657
Epoch:  43 	 Loss:  0.029345806688070297
Epoch:  44 	 Loss:  0.02864379622042179
Epoch:  45 	 Loss:  0.029044734314084053
Epoch:  46 	 Loss:  0.02821921929717064
Epoch:  47 	 Loss:  0.028993286192417145
Epoch:  48 	 Loss:  0.028406789526343346
Epoch:  49 	 Loss:  0.029093516990542412
Epoch:  50 	 Loss:  0.02885439619421959
Epoch:  51 	 Loss:  0.02925008162856102
Epoch:  52 	 Loss:  0.028925927355885506
Epoch:  53 	 Loss:  0.02932988665997982
Epoch:  54 	 Loss:  0.02915743552148342
Epoch:  55 	 Loss:  0.029038015753030777
Epoch:  56 	 Loss:  0.028502602130174637
Epoch:  57 	 Loss:  0.029160048812627792
Epoch:  58 	 Loss:  0.02880007028579712
Epoch:  59 	 Loss:  0.028737632557749748
Epoch:  60 	 Loss:  0.029268380254507065
Epoch:  61 	 Loss:  0.0294187068939209
Epoch:  62 	 Loss:  0.029598211869597435
Epoch:  63 	 Loss:  0.02827790006995201
Epoch:  64 	 Loss:  0.02903524786233902
Epoch:  65 	 Loss:  0.028980543836951256
Epoch:  66 	 Loss:  0.02926391363143921
Epoch:  67 	 Loss:  0.02909926325082779
Epoch:  68 	 Loss:  0.0287106204777956
Epoch:  69 	 Loss:  0.028152039274573326
Epoch:  70 	 Loss:  0.02868526242673397
Epoch:  71 	 Loss:  0.02832874283194542
Epoch:  72 	 Loss:  0.029266124591231346
Epoch:  73 	 Loss:  0.029551418498158455
Epoch:  74 	 Loss:  0.029461393132805824
Epoch:  75 	 Loss:  0.029344668611884117
Epoch:  76 	 Loss:  0.028555255383253098
Epoch:  77 	 Loss:  0.029463373124599457
Epoch:  78 	 Loss:  0.02823306806385517
Epoch:  79 	 Loss:  0.028925688937306404
Epoch:  80 	 Loss:  0.02893982082605362
Epoch:  81 	 Loss:  0.029282221570611
Epoch:  82 	 Loss:  0.029055539518594742
Epoch:  83 	 Loss:  0.028850844129920006
Epoch:  84 	 Loss:  0.028546778485178947
Epoch:  85 	 Loss:  0.0297806765884161
Epoch:  86 	 Loss:  0.02866693027317524
Epoch:  87 	 Loss:  0.02890959568321705
Epoch:  88 	 Loss:  0.028974100947380066
Epoch:  89 	 Loss:  0.029331620782613754
Epoch:  90 	 Loss:  0.029222683981060982
Epoch:  91 	 Loss:  0.028874224051833153
Epoch:  92 	 Loss:  0.029851073399186134
Epoch:  93 	 Loss:  0.02930302359163761
Epoch:  94 	 Loss:  0.02928202971816063
Epoch:  95 	 Loss:  0.0289315115660429
Epoch:  96 	 Loss:  0.028821757063269615
Epoch:  97 	 Loss:  0.02906837686896324
Epoch:  98 	 Loss:  0.02902515046298504
Epoch:  99 	 Loss:  0.029290111735463142
Epoch:  100 	 Loss:  0.029431553557515144
wandb: Waiting for W&B process to finish, PID 1737422
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150659-25ouwepo/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150659-25ouwepo/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02943
wandb:        _step 800000
wandb:     _runtime 83
wandb:   _timestamp 1612901302
wandb:         loss 0.02943
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–…â–‡â–…â–„â–„â–…â–ƒâ–ƒâ–…â–…â–„â–„â–…â–„â–„â–†â–ƒâ–â–…â–†â–…â–‚â–ƒâ–†â–…â–…â–â–†â–†â–†â–„â–…â–ƒâ–„â–†â–ˆâ–†â–…â–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced major-sweep-33: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/25ouwepo
wandb: Agent Starting Run: 3nxl5c2z with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:08:28.990379: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:08:28.995201: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run misty-sweep-34
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/3nxl5c2z
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_150827-3nxl5c2z
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028141416609287262
Epoch:  2 	 Loss:  0.02762719616293907
Epoch:  3 	 Loss:  0.027745792642235756
Epoch:  4 	 Loss:  0.02767685241997242
Epoch:  5 	 Loss:  0.027814792469143867
Epoch:  6 	 Loss:  0.027801211923360825
Epoch:  7 	 Loss:  0.028059247881174088
Epoch:  8 	 Loss:  0.027922743931412697
Epoch:  9 	 Loss:  0.028149159625172615
Epoch:  10 	 Loss:  0.02823580615222454
Epoch:  11 	 Loss:  0.02822941541671753
Epoch:  12 	 Loss:  0.027890600264072418
Epoch:  13 	 Loss:  0.027840444818139076
Epoch:  14 	 Loss:  0.02765866369009018
Epoch:  15 	 Loss:  0.028025131672620773
Epoch:  16 	 Loss:  0.027522532269358635
Epoch:  17 	 Loss:  0.02776481956243515
Epoch:  18 	 Loss:  0.027861809358000755
Epoch:  19 	 Loss:  0.02805224247276783
Epoch:  20 	 Loss:  0.027470871806144714
Epoch:  21 	 Loss:  0.02767195925116539
Epoch:  22 	 Loss:  0.02727711759507656
Epoch:  23 	 Loss:  0.028240280225872993
Epoch:  24 	 Loss:  0.027958951890468597
Epoch:  25 	 Loss:  0.028208117932081223
Epoch:  26 	 Loss:  0.028102558106184006
Epoch:  27 	 Loss:  0.02806083858013153
Epoch:  28 	 Loss:  0.02821403369307518
Epoch:  29 	 Loss:  0.02798336185514927
Epoch:  30 	 Loss:  0.02806924283504486
Epoch:  31 	 Loss:  0.02806735411286354
Epoch:  32 	 Loss:  0.0279567688703537
Epoch:  33 	 Loss:  0.027919607236981392
Epoch:  34 	 Loss:  0.027709349989891052
Epoch:  35 	 Loss:  0.027745209634304047
Epoch:  36 	 Loss:  0.027926482260227203
Epoch:  37 	 Loss:  0.02785305865108967
Epoch:  38 	 Loss:  0.028087681159377098
Epoch:  39 	 Loss:  0.02778048999607563
Epoch:  40 	 Loss:  0.028774132952094078
Epoch:  41 	 Loss:  0.02788519486784935
Epoch:  42 	 Loss:  0.028172122314572334
Epoch:  43 	 Loss:  0.027880920097231865
Epoch:  44 	 Loss:  0.028004007413983345
Epoch:  45 	 Loss:  0.027128439396619797
Epoch:  46 	 Loss:  0.02824738807976246
Epoch:  47 	 Loss:  0.028146781027317047
Epoch:  48 	 Loss:  0.027711128816008568
Epoch:  49 	 Loss:  0.0277246180921793
Epoch:  50 	 Loss:  0.02782350778579712
Epoch:  51 	 Loss:  0.027609705924987793
Epoch:  52 	 Loss:  0.027545258402824402
Epoch:  53 	 Loss:  0.02796964906156063
Epoch:  54 	 Loss:  0.028596414253115654
Epoch:  55 	 Loss:  0.028077853843569756
Epoch:  56 	 Loss:  0.027683066204190254
Epoch:  57 	 Loss:  0.027537377551198006
Epoch:  58 	 Loss:  0.027753466740250587
Epoch:  59 	 Loss:  0.028613286092877388
Epoch:  60 	 Loss:  0.028028689324855804
Epoch:  61 	 Loss:  0.027932213619351387
Epoch:  62 	 Loss:  0.028158972039818764
Epoch:  63 	 Loss:  0.027886459603905678
Epoch:  64 	 Loss:  0.027997350320219994
Epoch:  65 	 Loss:  0.02802875265479088
Epoch:  66 	 Loss:  0.027682317420840263
Epoch:  67 	 Loss:  0.02764355204999447
Epoch:  68 	 Loss:  0.028110159561038017
Epoch:  69 	 Loss:  0.02791982889175415
Epoch:  70 	 Loss:  0.02789817750453949
Epoch:  71 	 Loss:  0.02763674221932888
Epoch:  72 	 Loss:  0.028016384690999985
Epoch:  73 	 Loss:  0.02831305004656315
Epoch:  74 	 Loss:  0.02807607129216194
Epoch:  75 	 Loss:  0.028225159272551537
Epoch:  76 	 Loss:  0.02790326252579689
Epoch:  77 	 Loss:  0.027869926765561104
Epoch:  78 	 Loss:  0.028021618723869324
Epoch:  79 	 Loss:  0.02804066427052021
Epoch:  80 	 Loss:  0.028540687635540962
Epoch:  81 	 Loss:  0.028016842901706696
Epoch:  82 	 Loss:  0.028336208313703537
Epoch:  83 	 Loss:  0.028110966086387634
Epoch:  84 	 Loss:  0.02816781960427761
Epoch:  85 	 Loss:  0.028116043657064438
Epoch:  86 	 Loss:  0.027919406071305275
Epoch:  87 	 Loss:  0.027731381356716156
Epoch:  88 	 Loss:  0.027381764724850655
Epoch:  89 	 Loss:  0.027965674176812172
Epoch:  90 	 Loss:  0.027853626757860184
Epoch:  91 	 Loss:  0.028835948556661606
Epoch:  92 	 Loss:  0.028408274054527283
Epoch:  93 	 Loss:  0.027788257226347923
Epoch:  94 	 Loss:  0.028161292895674706
Epoch:  95 	 Loss:  0.027656253427267075
Epoch:  96 	 Loss:  0.028116609901189804
Epoch:  97 	 Loss:  0.02797725982964039
Epoch:  98 	 Loss:  0.028229691088199615
Epoch:  99 	 Loss:  0.027692826464772224
Epoch:  100 	 Loss:  0.028106780722737312
wandb: Waiting for W&B process to finish, PID 1741116
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150827-3nxl5c2z/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150827-3nxl5c2z/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02811
wandb:        _step 800000
wandb:     _runtime 84
wandb:   _timestamp 1612901391
wandb:         loss 0.02811
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–‚â–ƒâ–„â–†â–ƒâ–â–ƒâ–‚â–†â–…â–…â–„â–‚â–„â–ƒâ–ƒâ–„â–†â–‚â–‚â–ˆâ–‚â–ˆâ–„â–„â–‚â–„â–„â–…â–ƒâ–„â–†â–…â–‚â–„â–‡â–…â–„â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced misty-sweep-34: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/3nxl5c2z
wandb: Agent Starting Run: rni0d4n7 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:09:57.431071: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:09:57.438366: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run light-sweep-35
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/rni0d4n7
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_150955-rni0d4n7
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.0275795366615057
Epoch:  2 	 Loss:  0.027959158644080162
Epoch:  3 	 Loss:  0.027928518131375313
Epoch:  4 	 Loss:  0.027938222512602806
Epoch:  5 	 Loss:  0.02779305912554264
Epoch:  6 	 Loss:  0.027635743841528893
Epoch:  7 	 Loss:  0.02776511386036873
Epoch:  8 	 Loss:  0.027584383264183998
Epoch:  9 	 Loss:  0.028216945007443428
Epoch:  10 	 Loss:  0.02824406884610653
Epoch:  11 	 Loss:  0.028402457013726234
Epoch:  12 	 Loss:  0.028287794440984726
Epoch:  13 	 Loss:  0.028392374515533447
Epoch:  14 	 Loss:  0.027964523062109947
Epoch:  15 	 Loss:  0.02814503386616707
Epoch:  16 	 Loss:  0.02778284251689911
Epoch:  17 	 Loss:  0.028166422620415688
Epoch:  18 	 Loss:  0.02831266075372696
Epoch:  19 	 Loss:  0.028004420921206474
Epoch:  20 	 Loss:  0.027507280930876732
Epoch:  21 	 Loss:  0.027574922889471054
Epoch:  22 	 Loss:  0.027919957414269447
Epoch:  23 	 Loss:  0.027677154168486595
Epoch:  24 	 Loss:  0.027633607387542725
Epoch:  25 	 Loss:  0.027967529371380806
Epoch:  26 	 Loss:  0.027916746214032173
Epoch:  27 	 Loss:  0.028463559225201607
Epoch:  28 	 Loss:  0.02800929546356201
Epoch:  29 	 Loss:  0.02819538116455078
Epoch:  30 	 Loss:  0.02751370519399643
Epoch:  31 	 Loss:  0.02818475291132927
Epoch:  32 	 Loss:  0.02767675556242466
Epoch:  33 	 Loss:  0.027689959853887558
Epoch:  34 	 Loss:  0.027827652171254158
Epoch:  35 	 Loss:  0.027003847062587738
Epoch:  36 	 Loss:  0.02747243270277977
Epoch:  37 	 Loss:  0.027583850547671318
Epoch:  38 	 Loss:  0.027526643127202988
Epoch:  39 	 Loss:  0.0276692733168602
Epoch:  40 	 Loss:  0.02771271951496601
Epoch:  41 	 Loss:  0.027763480320572853
Epoch:  42 	 Loss:  0.02753068320453167
Epoch:  43 	 Loss:  0.027510931715369225
Epoch:  44 	 Loss:  0.028201982378959656
Epoch:  45 	 Loss:  0.027374330908060074
Epoch:  46 	 Loss:  0.02809092216193676
Epoch:  47 	 Loss:  0.02760106883943081
Epoch:  48 	 Loss:  0.027509203180670738
Epoch:  49 	 Loss:  0.027422230690717697
Epoch:  50 	 Loss:  0.02764749526977539
Epoch:  51 	 Loss:  0.027507280930876732
Epoch:  52 	 Loss:  0.027763761579990387
Epoch:  53 	 Loss:  0.027585312724113464
Epoch:  54 	 Loss:  0.027630912140011787
Epoch:  55 	 Loss:  0.027854399755597115
Epoch:  56 	 Loss:  0.027654258534312248
Epoch:  57 	 Loss:  0.027921510860323906
Epoch:  58 	 Loss:  0.027571866288781166
Epoch:  59 	 Loss:  0.027507970109581947
Epoch:  60 	 Loss:  0.027697023004293442
Epoch:  61 	 Loss:  0.0273362435400486
Epoch:  62 	 Loss:  0.02788039855659008
Epoch:  63 	 Loss:  0.02803940884768963
Epoch:  64 	 Loss:  0.027056267485022545
Epoch:  65 	 Loss:  0.028078487142920494
Epoch:  66 	 Loss:  0.028191516175866127
Epoch:  67 	 Loss:  0.027419066056609154
Epoch:  68 	 Loss:  0.02774936892092228
Epoch:  69 	 Loss:  0.027874525636434555
Epoch:  70 	 Loss:  0.027582701295614243
Epoch:  71 	 Loss:  0.02753864787518978
Epoch:  72 	 Loss:  0.02801608480513096
Epoch:  73 	 Loss:  0.028179796412587166
Epoch:  74 	 Loss:  0.027811085805296898
Epoch:  75 	 Loss:  0.02780926786363125
Epoch:  76 	 Loss:  0.027829265221953392
Epoch:  77 	 Loss:  0.027702903375029564
Epoch:  78 	 Loss:  0.028034882619976997
Epoch:  79 	 Loss:  0.02798987366259098
Epoch:  80 	 Loss:  0.02800566516816616
Epoch:  81 	 Loss:  0.027943162247538567
Epoch:  82 	 Loss:  0.027944765985012054
Epoch:  83 	 Loss:  0.027544928714632988
Epoch:  84 	 Loss:  0.02821931056678295
Epoch:  85 	 Loss:  0.028239864856004715
Epoch:  86 	 Loss:  0.027597909793257713
Epoch:  87 	 Loss:  0.02761095203459263
Epoch:  88 	 Loss:  0.028453422710299492
Epoch:  89 	 Loss:  0.02781551703810692
Epoch:  90 	 Loss:  0.02763122320175171
Epoch:  91 	 Loss:  0.027753230184316635
Epoch:  92 	 Loss:  0.027962515130639076
Epoch:  93 	 Loss:  0.028129728510975838
Epoch:  94 	 Loss:  0.02775326557457447
Epoch:  95 	 Loss:  0.027868082746863365
Epoch:  96 	 Loss:  0.02745134010910988
Epoch:  97 	 Loss:  0.02795141562819481
Epoch:  98 	 Loss:  0.028137633576989174
Epoch:  99 	 Loss:  0.027662519365549088
Epoch:  100 	 Loss:  0.027669306844472885
wandb: Waiting for W&B process to finish, PID 1744808
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150955-rni0d4n7/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_150955-rni0d4n7/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02767
wandb:        _step 800000
wandb:     _runtime 85
wandb:   _timestamp 1612901480
wandb:         loss 0.02767
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–†â–„â–„â–ˆâ–ˆâ–…â–ˆâ–„â–„â–…â–†â–‡â–…â–ƒâ–„â–…â–‡â–†â–ƒâ–ƒâ–„â–„â–ƒâ–‚â–â–ƒâ–…â–†â–…â–„â–†â–†â–‡â–„â–…â–†â–…â–†â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced light-sweep-35: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/rni0d4n7
wandb: Agent Starting Run: 2381m75o with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 64
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:11:26.832335: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:11:26.838868: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run hearty-sweep-36
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/2381m75o
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_151124-2381m75o
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02894812822341919
Epoch:  2 	 Loss:  0.0283672995865345
Epoch:  3 	 Loss:  0.028968259692192078
Epoch:  4 	 Loss:  0.02961832284927368
Epoch:  5 	 Loss:  0.029027612879872322
Epoch:  6 	 Loss:  0.028320588171482086
Epoch:  7 	 Loss:  0.028265921398997307
Epoch:  8 	 Loss:  0.028607940301299095
Epoch:  9 	 Loss:  0.028765007853507996
Epoch:  10 	 Loss:  0.029317140579223633
Epoch:  11 	 Loss:  0.028952529653906822
Epoch:  12 	 Loss:  0.029203947633504868
Epoch:  13 	 Loss:  0.028464706614613533
Epoch:  14 	 Loss:  0.02873932011425495
Epoch:  15 	 Loss:  0.029061876237392426
Epoch:  16 	 Loss:  0.02893087826669216
Epoch:  17 	 Loss:  0.029147304594516754
Epoch:  18 	 Loss:  0.028430480509996414
Epoch:  19 	 Loss:  0.0286533422768116
Epoch:  20 	 Loss:  0.02952657639980316
Epoch:  21 	 Loss:  0.028713010251522064
Epoch:  22 	 Loss:  0.028838803991675377
Epoch:  23 	 Loss:  0.02901438996195793
Epoch:  24 	 Loss:  0.028903111815452576
Epoch:  25 	 Loss:  0.029039327055215836
Epoch:  26 	 Loss:  0.02900310605764389
Epoch:  27 	 Loss:  0.028894267976284027
Epoch:  28 	 Loss:  0.028530176728963852
Epoch:  29 	 Loss:  0.02910824865102768
Epoch:  30 	 Loss:  0.028891773894429207
Epoch:  31 	 Loss:  0.028582559898495674
Epoch:  32 	 Loss:  0.028990374878048897
Epoch:  33 	 Loss:  0.028623763471841812
Epoch:  34 	 Loss:  0.028688371181488037
Epoch:  35 	 Loss:  0.029171114787459373
Epoch:  36 	 Loss:  0.028819667175412178
Epoch:  37 	 Loss:  0.02874903380870819
Epoch:  38 	 Loss:  0.028650343418121338
Epoch:  39 	 Loss:  0.029103880748152733
Epoch:  40 	 Loss:  0.029351379722356796
Epoch:  41 	 Loss:  0.02953091450035572
Epoch:  42 	 Loss:  0.02846449241042137
Epoch:  43 	 Loss:  0.029332298785448074
Epoch:  44 	 Loss:  0.028710180893540382
Epoch:  45 	 Loss:  0.02885046787559986
Epoch:  46 	 Loss:  0.02866077981889248
Epoch:  47 	 Loss:  0.028391724452376366
Epoch:  48 	 Loss:  0.029241593554615974
Epoch:  49 	 Loss:  0.029173020273447037
Epoch:  50 	 Loss:  0.028753655031323433
Epoch:  51 	 Loss:  0.02826661244034767
Epoch:  52 	 Loss:  0.029146170243620872
Epoch:  53 	 Loss:  0.028695449233055115
Epoch:  54 	 Loss:  0.029066210612654686
Epoch:  55 	 Loss:  0.029096189886331558
Epoch:  56 	 Loss:  0.02850925363600254
Epoch:  57 	 Loss:  0.028875689953565598
Epoch:  58 	 Loss:  0.029143860563635826
Epoch:  59 	 Loss:  0.029167508706450462
Epoch:  60 	 Loss:  0.0287945456802845
Epoch:  61 	 Loss:  0.028872521594166756
Epoch:  62 	 Loss:  0.028453409671783447
Epoch:  63 	 Loss:  0.02921023592352867
Epoch:  64 	 Loss:  0.02860335074365139
Epoch:  65 	 Loss:  0.02863423526287079
Epoch:  66 	 Loss:  0.029060442000627518
Epoch:  67 	 Loss:  0.028508557006716728
Epoch:  68 	 Loss:  0.02870335802435875
Epoch:  69 	 Loss:  0.028293009847402573
Epoch:  70 	 Loss:  0.02903616428375244
Epoch:  71 	 Loss:  0.028799382969737053
Epoch:  72 	 Loss:  0.02916942723095417
Epoch:  73 	 Loss:  0.028556309640407562
Epoch:  74 	 Loss:  0.028758790343999863
Epoch:  75 	 Loss:  0.028945332393050194
Epoch:  76 	 Loss:  0.029155565425753593
Epoch:  77 	 Loss:  0.029191896319389343
Epoch:  78 	 Loss:  0.029157092794775963
Epoch:  79 	 Loss:  0.028583412989974022
Epoch:  80 	 Loss:  0.028761129826307297
Epoch:  81 	 Loss:  0.02911251224577427
Epoch:  82 	 Loss:  0.028852026909589767
Epoch:  83 	 Loss:  0.028854621574282646
Epoch:  84 	 Loss:  0.028848616406321526
Epoch:  85 	 Loss:  0.028951287269592285
Epoch:  86 	 Loss:  0.02936755307018757
Epoch:  87 	 Loss:  0.02860168367624283
Epoch:  88 	 Loss:  0.029257742688059807
Epoch:  89 	 Loss:  0.029042840003967285
Epoch:  90 	 Loss:  0.02833661250770092
Epoch:  91 	 Loss:  0.02901499532163143
Epoch:  92 	 Loss:  0.029585178941488266
Epoch:  93 	 Loss:  0.028905468061566353
Epoch:  94 	 Loss:  0.028716133907437325
Epoch:  95 	 Loss:  0.028573080897331238
Epoch:  96 	 Loss:  0.028388958424329758
Epoch:  97 	 Loss:  0.02925432287156582
Epoch:  98 	 Loss:  0.028923988342285156
Epoch:  99 	 Loss:  0.028499603271484375
Epoch:  100 	 Loss:  0.02859191782772541
wandb: Waiting for W&B process to finish, PID 1748500
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151124-2381m75o/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151124-2381m75o/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02859
wandb:        _step 800000
wandb:     _runtime 86
wandb:   _timestamp 1612901570
wandb:         loss 0.02859
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–…â–â–ƒâ–…â–‚â–…â–‚â–ƒâ–…â–…â–‚â–ƒâ–ƒâ–„â–…â–ˆâ–ƒâ–ƒâ–†â–â–…â–‚â–†â–„â–ƒâ–‚â–â–†â–„â–†â–ƒâ–„â–„â–ƒâ–…â–ˆâ–ƒâ–†â–ƒ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced hearty-sweep-36: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/2381m75o
wandb: Agent Starting Run: jbsx41zb with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: adam
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:12:56.448789: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:12:56.455511: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run swift-sweep-37
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/jbsx41zb
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_151254-jbsx41zb
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028741836547851562
Epoch:  2 	 Loss:  0.02889920584857464
Epoch:  3 	 Loss:  0.02845308743417263
Epoch:  4 	 Loss:  0.028375964611768723
Epoch:  5 	 Loss:  0.028459446504712105
Epoch:  6 	 Loss:  0.02822933718562126
Epoch:  7 	 Loss:  0.028491826727986336
Epoch:  8 	 Loss:  0.028561906889081
Epoch:  9 	 Loss:  0.028149710968136787
Epoch:  10 	 Loss:  0.02837979421019554
Epoch:  11 	 Loss:  0.02868400141596794
Epoch:  12 	 Loss:  0.02865668386220932
Epoch:  13 	 Loss:  0.028376340866088867
Epoch:  14 	 Loss:  0.02902994677424431
Epoch:  15 	 Loss:  0.028336286544799805
Epoch:  16 	 Loss:  0.028493637219071388
Epoch:  17 	 Loss:  0.028575245290994644
Epoch:  18 	 Loss:  0.028318092226982117
Epoch:  19 	 Loss:  0.02824893221259117
Epoch:  20 	 Loss:  0.02890901267528534
Epoch:  21 	 Loss:  0.028101112693548203
Epoch:  22 	 Loss:  0.028664011508226395
Epoch:  23 	 Loss:  0.028949173167347908
Epoch:  24 	 Loss:  0.028965648263692856
Epoch:  25 	 Loss:  0.028409525752067566
Epoch:  26 	 Loss:  0.029462581500411034
Epoch:  27 	 Loss:  0.028438059613108635
Epoch:  28 	 Loss:  0.028357524424791336
Epoch:  29 	 Loss:  0.028541477397084236
Epoch:  30 	 Loss:  0.028135325759649277
Epoch:  31 	 Loss:  0.028607133775949478
Epoch:  32 	 Loss:  0.02858138084411621
Epoch:  33 	 Loss:  0.028488310053944588
Epoch:  34 	 Loss:  0.02853744849562645
Epoch:  35 	 Loss:  0.02892388217151165
Epoch:  36 	 Loss:  0.02888287417590618
Epoch:  37 	 Loss:  0.029247472062706947
Epoch:  38 	 Loss:  0.02821328490972519
Epoch:  39 	 Loss:  0.028495660051703453
Epoch:  40 	 Loss:  0.02826697938144207
Epoch:  41 	 Loss:  0.02853493019938469
Epoch:  42 	 Loss:  0.027877261862158775
Epoch:  43 	 Loss:  0.029241519048810005
Epoch:  44 	 Loss:  0.02871672809123993
Epoch:  45 	 Loss:  0.02834649197757244
Epoch:  46 	 Loss:  0.027942318469285965
Epoch:  47 	 Loss:  0.029087845236063004
Epoch:  48 	 Loss:  0.028751768171787262
Epoch:  49 	 Loss:  0.028199654072523117
Epoch:  50 	 Loss:  0.02791447378695011
Epoch:  51 	 Loss:  0.028504764661192894
Epoch:  52 	 Loss:  0.029119648039340973
Epoch:  53 	 Loss:  0.02827044017612934
Epoch:  54 	 Loss:  0.028185129165649414
Epoch:  55 	 Loss:  0.02818405255675316
Epoch:  56 	 Loss:  0.02864520251750946
Epoch:  57 	 Loss:  0.028880953788757324
Epoch:  58 	 Loss:  0.02844533883035183
Epoch:  59 	 Loss:  0.02853792905807495
Epoch:  60 	 Loss:  0.028485223650932312
Epoch:  61 	 Loss:  0.028933631256222725
Epoch:  62 	 Loss:  0.028771070763468742
Epoch:  63 	 Loss:  0.028347400948405266
Epoch:  64 	 Loss:  0.02872435748577118
Epoch:  65 	 Loss:  0.028900984674692154
Epoch:  66 	 Loss:  0.02857593446969986
Epoch:  67 	 Loss:  0.028791967779397964
Epoch:  68 	 Loss:  0.028279360383749008
Epoch:  69 	 Loss:  0.02861913852393627
Epoch:  70 	 Loss:  0.02865145169198513
Epoch:  71 	 Loss:  0.028072291985154152
Epoch:  72 	 Loss:  0.028433743864297867
Epoch:  73 	 Loss:  0.027973884716629982
Epoch:  74 	 Loss:  0.02782386541366577
Epoch:  75 	 Loss:  0.028391016647219658
Epoch:  76 	 Loss:  0.02822207100689411
Epoch:  77 	 Loss:  0.028792230412364006
Epoch:  78 	 Loss:  0.02873322181403637
Epoch:  79 	 Loss:  0.029102977365255356
Epoch:  80 	 Loss:  0.02797006256878376
Epoch:  81 	 Loss:  0.028331467881798744
Epoch:  82 	 Loss:  0.0279901884496212
Epoch:  83 	 Loss:  0.028085235506296158
Epoch:  84 	 Loss:  0.028262345120310783
Epoch:  85 	 Loss:  0.028525447472929955
Epoch:  86 	 Loss:  0.0284651480615139
Epoch:  87 	 Loss:  0.027862297371029854
Epoch:  88 	 Loss:  0.028570203110575676
Epoch:  89 	 Loss:  0.028081385418772697
Epoch:  90 	 Loss:  0.02887401171028614
Epoch:  91 	 Loss:  0.027890494093298912
Epoch:  92 	 Loss:  0.028858598321676254
Epoch:  93 	 Loss:  0.028371956199407578
Epoch:  94 	 Loss:  0.028655918315052986
Epoch:  95 	 Loss:  0.028016608208417892
Epoch:  96 	 Loss:  0.028337322175502777
Epoch:  97 	 Loss:  0.02877289056777954
Epoch:  98 	 Loss:  0.028249552473425865
Epoch:  99 	 Loss:  0.02883283793926239
Epoch:  100 	 Loss:  0.028457241132855415
wandb: Waiting for W&B process to finish, PID 1752191
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151254-jbsx41zb/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151254-jbsx41zb/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02846
wandb:        _step 800000
wandb:     _runtime 87
wandb:   _timestamp 1612901661
wandb:         loss 0.02846
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–„â–ƒâ–„â–…â–ƒâ–„â–ƒâ–‚â–†â–ˆâ–ƒâ–„â–„â–†â–„â–„â–…â–‚â–ƒâ–„â–ƒâ–…â–„â–†â–…â–…â–„â–„â–â–…â–†â–‚â–ƒâ–â–‚â–…â–…â–…â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced swift-sweep-37: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/jbsx41zb
wandb: Agent Starting Run: zymj2vxo with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:14:27.599663: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:14:27.606024: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run giddy-sweep-38
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/zymj2vxo
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_151425-zymj2vxo
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02694249153137207
Epoch:  2 	 Loss:  0.027261583134531975
Epoch:  3 	 Loss:  0.026793470606207848
Epoch:  4 	 Loss:  0.02714301459491253
Epoch:  5 	 Loss:  0.027126280590891838
Epoch:  6 	 Loss:  0.0271407812833786
Epoch:  7 	 Loss:  0.026943091303110123
Epoch:  8 	 Loss:  0.02720988355576992
Epoch:  9 	 Loss:  0.027444161474704742
Epoch:  10 	 Loss:  0.02686481736600399
Epoch:  11 	 Loss:  0.02729756198823452
Epoch:  12 	 Loss:  0.027226733043789864
Epoch:  13 	 Loss:  0.026811575517058372
Epoch:  14 	 Loss:  0.027271095663309097
Epoch:  15 	 Loss:  0.026838120073080063
Epoch:  16 	 Loss:  0.027326496317982674
Epoch:  17 	 Loss:  0.02732950821518898
Epoch:  18 	 Loss:  0.0272163525223732
Epoch:  19 	 Loss:  0.02718903310596943
Epoch:  20 	 Loss:  0.02678270824253559
Epoch:  21 	 Loss:  0.02697637863457203
Epoch:  22 	 Loss:  0.026819216087460518
Epoch:  23 	 Loss:  0.027075592428445816
Epoch:  24 	 Loss:  0.02743120677769184
Epoch:  25 	 Loss:  0.027338912710547447
Epoch:  26 	 Loss:  0.026630103588104248
Epoch:  27 	 Loss:  0.02708367258310318
Epoch:  28 	 Loss:  0.026836330071091652
Epoch:  29 	 Loss:  0.027011118829250336
Epoch:  30 	 Loss:  0.027224091812968254
Epoch:  31 	 Loss:  0.02749626524746418
Epoch:  32 	 Loss:  0.027018217369914055
Epoch:  33 	 Loss:  0.02756105549633503
Epoch:  34 	 Loss:  0.02689850889146328
Epoch:  35 	 Loss:  0.027004020288586617
Epoch:  36 	 Loss:  0.027404069900512695
Epoch:  37 	 Loss:  0.026992572471499443
Epoch:  38 	 Loss:  0.02732757478952408
Epoch:  39 	 Loss:  0.02757422626018524
Epoch:  40 	 Loss:  0.027533726766705513
Epoch:  41 	 Loss:  0.02683199755847454
Epoch:  42 	 Loss:  0.026792649179697037
Epoch:  43 	 Loss:  0.026787877082824707
Epoch:  44 	 Loss:  0.027109798043966293
Epoch:  45 	 Loss:  0.027211299166083336
Epoch:  46 	 Loss:  0.02739197388291359
Epoch:  47 	 Loss:  0.02743547596037388
Epoch:  48 	 Loss:  0.026912834495306015
Epoch:  49 	 Loss:  0.027045520022511482
Epoch:  50 	 Loss:  0.027054887264966965
Epoch:  51 	 Loss:  0.026934577152132988
Epoch:  52 	 Loss:  0.027469996362924576
Epoch:  53 	 Loss:  0.027286836877465248
Epoch:  54 	 Loss:  0.027074014768004417
Epoch:  55 	 Loss:  0.02728971652686596
Epoch:  56 	 Loss:  0.027098316699266434
Epoch:  57 	 Loss:  0.027072906494140625
Epoch:  58 	 Loss:  0.027099208906292915
Epoch:  59 	 Loss:  0.027436908334493637
Epoch:  60 	 Loss:  0.02675827592611313
Epoch:  61 	 Loss:  0.027066564187407494
Epoch:  62 	 Loss:  0.027081722393631935
Epoch:  63 	 Loss:  0.027699295431375504
Epoch:  64 	 Loss:  0.027275260537862778
Epoch:  65 	 Loss:  0.027395101264119148
Epoch:  66 	 Loss:  0.027206318452954292
Epoch:  67 	 Loss:  0.027142491191625595
Epoch:  68 	 Loss:  0.026965100318193436
Epoch:  69 	 Loss:  0.02669934183359146
Epoch:  70 	 Loss:  0.02709469012916088
Epoch:  71 	 Loss:  0.02714361436665058
Epoch:  72 	 Loss:  0.027323907241225243
Epoch:  73 	 Loss:  0.0272582545876503
Epoch:  74 	 Loss:  0.027135517448186874
Epoch:  75 	 Loss:  0.027220914140343666
Epoch:  76 	 Loss:  0.026865249499678612
Epoch:  77 	 Loss:  0.026507575064897537
Epoch:  78 	 Loss:  0.027160268276929855
Epoch:  79 	 Loss:  0.026812810450792313
Epoch:  80 	 Loss:  0.02715689316391945
Epoch:  81 	 Loss:  0.027406372129917145
Epoch:  82 	 Loss:  0.027194596827030182
Epoch:  83 	 Loss:  0.026984717696905136
Epoch:  84 	 Loss:  0.027293860912322998
Epoch:  85 	 Loss:  0.027334056794643402
Epoch:  86 	 Loss:  0.027054203674197197
Epoch:  87 	 Loss:  0.027180785313248634
Epoch:  88 	 Loss:  0.027248816564679146
Epoch:  89 	 Loss:  0.027060361579060555
Epoch:  90 	 Loss:  0.026997989043593407
Epoch:  91 	 Loss:  0.02737952210009098
Epoch:  92 	 Loss:  0.026788799092173576
Epoch:  93 	 Loss:  0.027201414108276367
Epoch:  94 	 Loss:  0.02728945016860962
Epoch:  95 	 Loss:  0.02694740705192089
Epoch:  96 	 Loss:  0.026629969477653503
Epoch:  97 	 Loss:  0.027085768058896065
Epoch:  98 	 Loss:  0.026990564540028572
Epoch:  99 	 Loss:  0.02706565335392952
Epoch:  100 	 Loss:  0.02696789987385273
wandb: Waiting for W&B process to finish, PID 1755886
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151425-zymj2vxo/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151425-zymj2vxo/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02697
wandb:        _step 800000
wandb:     _runtime 87
wandb:   _timestamp 1612901752
wandb:         loss 0.02697
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–ƒâ–…â–†â–†â–ƒâ–†â–†â–„â–…â–‚â–ƒâ–‡â–„â–‡â–ˆâ–ƒâ–…â–‡â–…â–„â–…â–…â–‡â–…â–†â–…â–‚â–†â–…â–â–ƒâ–†â–†â–…â–…â–ƒâ–†â–…â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced giddy-sweep-38: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/zymj2vxo
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: x6rjsm5m with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:16:08.907022: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:16:08.912174: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run sunny-sweep-39
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/x6rjsm5m
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_151607-x6rjsm5m
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02707321010529995
Epoch:  2 	 Loss:  0.027414197102189064
Epoch:  3 	 Loss:  0.02738734893500805
Epoch:  4 	 Loss:  0.027488047257065773
Epoch:  5 	 Loss:  0.02730434574186802
Epoch:  6 	 Loss:  0.027368275448679924
Epoch:  7 	 Loss:  0.02708100713789463
Epoch:  8 	 Loss:  0.02712300978600979
Epoch:  9 	 Loss:  0.027488069608807564
Epoch:  10 	 Loss:  0.02718241885304451
Epoch:  11 	 Loss:  0.02711225114762783
Epoch:  12 	 Loss:  0.026988552883267403
Epoch:  13 	 Loss:  0.02690386213362217
Epoch:  14 	 Loss:  0.02703223191201687
Epoch:  15 	 Loss:  0.027305113151669502
Epoch:  16 	 Loss:  0.027504123747348785
Epoch:  17 	 Loss:  0.027504663914442062
Epoch:  18 	 Loss:  0.026627201586961746
Epoch:  19 	 Loss:  0.026975154876708984
Epoch:  20 	 Loss:  0.026951545849442482
Epoch:  21 	 Loss:  0.027386900037527084
Epoch:  22 	 Loss:  0.027686582878232002
Epoch:  23 	 Loss:  0.0270286425948143
Epoch:  24 	 Loss:  0.02702501229941845
Epoch:  25 	 Loss:  0.027455724775791168
Epoch:  26 	 Loss:  0.027676349505782127
Epoch:  27 	 Loss:  0.027213115245103836
Epoch:  28 	 Loss:  0.0265215914696455
Epoch:  29 	 Loss:  0.027104534208774567
Epoch:  30 	 Loss:  0.027220066636800766
Epoch:  31 	 Loss:  0.02789023332297802
Epoch:  32 	 Loss:  0.02735023759305477
Epoch:  33 	 Loss:  0.027575239539146423
Epoch:  34 	 Loss:  0.027774861082434654
Epoch:  35 	 Loss:  0.02742411568760872
Epoch:  36 	 Loss:  0.027395254001021385
Epoch:  37 	 Loss:  0.027234742417931557
Epoch:  38 	 Loss:  0.02708289586007595
Epoch:  39 	 Loss:  0.027210606262087822
Epoch:  40 	 Loss:  0.027356060221791267
Epoch:  41 	 Loss:  0.027189796790480614
Epoch:  42 	 Loss:  0.02697916515171528
Epoch:  43 	 Loss:  0.02744721993803978
Epoch:  44 	 Loss:  0.027215104550123215
Epoch:  45 	 Loss:  0.027293311432003975
Epoch:  46 	 Loss:  0.027002017945051193
Epoch:  47 	 Loss:  0.027186568826436996
Epoch:  48 	 Loss:  0.027598774060606956
Epoch:  49 	 Loss:  0.027471011504530907
Epoch:  50 	 Loss:  0.026808148249983788
Epoch:  51 	 Loss:  0.02720450423657894
Epoch:  52 	 Loss:  0.027078326791524887
Epoch:  53 	 Loss:  0.02731749601662159
Epoch:  54 	 Loss:  0.026991304010152817
Epoch:  55 	 Loss:  0.026682661846280098
Epoch:  56 	 Loss:  0.02757374942302704
Epoch:  57 	 Loss:  0.026722362264990807
Epoch:  58 	 Loss:  0.027180075645446777
Epoch:  59 	 Loss:  0.027259958907961845
Epoch:  60 	 Loss:  0.02726012095808983
Epoch:  61 	 Loss:  0.027127861976623535
Epoch:  62 	 Loss:  0.027053670957684517
Epoch:  63 	 Loss:  0.0264552291482687
Epoch:  64 	 Loss:  0.02733999863266945
Epoch:  65 	 Loss:  0.02733810991048813
Epoch:  66 	 Loss:  0.027406636625528336
Epoch:  67 	 Loss:  0.027297010645270348
Epoch:  68 	 Loss:  0.027212657034397125
Epoch:  69 	 Loss:  0.027463234961032867
Epoch:  70 	 Loss:  0.02756601944565773
Epoch:  71 	 Loss:  0.02716941572725773
Epoch:  72 	 Loss:  0.02699369564652443
Epoch:  73 	 Loss:  0.027432505041360855
Epoch:  74 	 Loss:  0.02728060446679592
Epoch:  75 	 Loss:  0.026885537430644035
Epoch:  76 	 Loss:  0.027411727234721184
Epoch:  77 	 Loss:  0.026730166748166084
Epoch:  78 	 Loss:  0.02706349827349186
Epoch:  79 	 Loss:  0.027822237461805344
Epoch:  80 	 Loss:  0.02674787864089012
Epoch:  81 	 Loss:  0.027369197458028793
Epoch:  82 	 Loss:  0.027122585102915764
Epoch:  83 	 Loss:  0.02729044109582901
Epoch:  84 	 Loss:  0.02672790177166462
Epoch:  85 	 Loss:  0.0273624025285244
Epoch:  86 	 Loss:  0.027237428352236748
Epoch:  87 	 Loss:  0.02714066207408905
Epoch:  88 	 Loss:  0.027611466124653816
Epoch:  89 	 Loss:  0.027183610945940018
Epoch:  90 	 Loss:  0.027145009487867355
Epoch:  91 	 Loss:  0.02702944539487362
Epoch:  92 	 Loss:  0.027131443843245506
Epoch:  93 	 Loss:  0.027186255902051926
Epoch:  94 	 Loss:  0.026944035664200783
Epoch:  95 	 Loss:  0.027485376223921776
Epoch:  96 	 Loss:  0.02736772410571575
Epoch:  97 	 Loss:  0.027479708194732666
Epoch:  98 	 Loss:  0.027495967224240303
Epoch:  99 	 Loss:  0.027136174961924553
Epoch:  100 	 Loss:  0.027193542569875717
wandb: Waiting for W&B process to finish, PID 1759592
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151607-x6rjsm5m/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151607-x6rjsm5m/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02719
wandb:        _step 800000
wandb:     _runtime 85
wandb:   _timestamp 1612901852
wandb:         loss 0.02719
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–…â–…â–„â–„â–ƒâ–†â–‚â–…â–„â–‡â–â–ˆâ–‡â–…â–…â–„â–…â–ƒâ–†â–„â–ƒâ–†â–…â–„â–…â–…â–†â–ƒâ–…â–‚â–ˆâ–„â–‚â–„â–„â–„â–ƒâ–†â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced sunny-sweep-39: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/x6rjsm5m
wandb: Agent Starting Run: sq8bow9q with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:17:38.809612: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:17:38.814993: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run gentle-sweep-40
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/sq8bow9q
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_151736-sq8bow9q
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02707780711352825
Epoch:  2 	 Loss:  0.02709665521979332
Epoch:  3 	 Loss:  0.027462128549814224
Epoch:  4 	 Loss:  0.027018006891012192
Epoch:  5 	 Loss:  0.02690720185637474
Epoch:  6 	 Loss:  0.02699184976518154
Epoch:  7 	 Loss:  0.02686386927962303
Epoch:  8 	 Loss:  0.026981303468346596
Epoch:  9 	 Loss:  0.026736140251159668
Epoch:  10 	 Loss:  0.027117008343338966
Epoch:  11 	 Loss:  0.02725703828036785
Epoch:  12 	 Loss:  0.027003558352589607
Epoch:  13 	 Loss:  0.027039358392357826
Epoch:  14 	 Loss:  0.026784200221300125
Epoch:  15 	 Loss:  0.026754098013043404
Epoch:  16 	 Loss:  0.026854028925299644
Epoch:  17 	 Loss:  0.02722005546092987
Epoch:  18 	 Loss:  0.026823703199625015
Epoch:  19 	 Loss:  0.026907796040177345
Epoch:  20 	 Loss:  0.027295753359794617
Epoch:  21 	 Loss:  0.027283912524580956
Epoch:  22 	 Loss:  0.02739538438618183
Epoch:  23 	 Loss:  0.02660151571035385
Epoch:  24 	 Loss:  0.027168525382876396
Epoch:  25 	 Loss:  0.026984842494130135
Epoch:  26 	 Loss:  0.026950940489768982
Epoch:  27 	 Loss:  0.026895567774772644
Epoch:  28 	 Loss:  0.026943828910589218
Epoch:  29 	 Loss:  0.02728912979364395
Epoch:  30 	 Loss:  0.026558594778180122
Epoch:  31 	 Loss:  0.027269698679447174
Epoch:  32 	 Loss:  0.026502957567572594
Epoch:  33 	 Loss:  0.02714509703218937
Epoch:  34 	 Loss:  0.027375930920243263
Epoch:  35 	 Loss:  0.02704899199306965
Epoch:  36 	 Loss:  0.0268105436116457
Epoch:  37 	 Loss:  0.027172528207302094
Epoch:  38 	 Loss:  0.02737460657954216
Epoch:  39 	 Loss:  0.026748908683657646
Epoch:  40 	 Loss:  0.027228236198425293
Epoch:  41 	 Loss:  0.02700323425233364
Epoch:  42 	 Loss:  0.02695378288626671
Epoch:  43 	 Loss:  0.02733910083770752
Epoch:  44 	 Loss:  0.026890965178608894
Epoch:  45 	 Loss:  0.02743455395102501
Epoch:  46 	 Loss:  0.02695976383984089
Epoch:  47 	 Loss:  0.027050243690609932
Epoch:  48 	 Loss:  0.026759440079331398
Epoch:  49 	 Loss:  0.02729322388768196
Epoch:  50 	 Loss:  0.02684381790459156
Epoch:  51 	 Loss:  0.026864437386393547
Epoch:  52 	 Loss:  0.02707044407725334
Epoch:  53 	 Loss:  0.027151094749569893
Epoch:  54 	 Loss:  0.02739899978041649
Epoch:  55 	 Loss:  0.027546236291527748
Epoch:  56 	 Loss:  0.02671797014772892
Epoch:  57 	 Loss:  0.02695162035524845
Epoch:  58 	 Loss:  0.027217373251914978
Epoch:  59 	 Loss:  0.026953333988785744
Epoch:  60 	 Loss:  0.027074282988905907
Epoch:  61 	 Loss:  0.026856375858187675
Epoch:  62 	 Loss:  0.027011757716536522
Epoch:  63 	 Loss:  0.026965314522385597
Epoch:  64 	 Loss:  0.027243683114647865
Epoch:  65 	 Loss:  0.026677075773477554
Epoch:  66 	 Loss:  0.027119401842355728
Epoch:  67 	 Loss:  0.027010224759578705
Epoch:  68 	 Loss:  0.027202289551496506
Epoch:  69 	 Loss:  0.02706199511885643
Epoch:  70 	 Loss:  0.02647300250828266
Epoch:  71 	 Loss:  0.0265663992613554
Epoch:  72 	 Loss:  0.027141056954860687
Epoch:  73 	 Loss:  0.02687557227909565
Epoch:  74 	 Loss:  0.026985492557287216
Epoch:  75 	 Loss:  0.027006005868315697
Epoch:  76 	 Loss:  0.02665574662387371
Epoch:  77 	 Loss:  0.02673620916903019
Epoch:  78 	 Loss:  0.026651231572031975
Epoch:  79 	 Loss:  0.026975607499480247
Epoch:  80 	 Loss:  0.026572227478027344
Epoch:  81 	 Loss:  0.026864610612392426
Epoch:  82 	 Loss:  0.026849640533328056
Epoch:  83 	 Loss:  0.027105070650577545
Epoch:  84 	 Loss:  0.027152525261044502
Epoch:  85 	 Loss:  0.026864584535360336
Epoch:  86 	 Loss:  0.027182843536138535
Epoch:  87 	 Loss:  0.026829885318875313
Epoch:  88 	 Loss:  0.026816565543413162
Epoch:  89 	 Loss:  0.026968207210302353
Epoch:  90 	 Loss:  0.026970431208610535
Epoch:  91 	 Loss:  0.026518138125538826
Epoch:  92 	 Loss:  0.02628411538898945
Epoch:  93 	 Loss:  0.02725747413933277
Epoch:  94 	 Loss:  0.02703913301229477
Epoch:  95 	 Loss:  0.026576632633805275
Epoch:  96 	 Loss:  0.027230877429246902
Epoch:  97 	 Loss:  0.02707814611494541
Epoch:  98 	 Loss:  0.02709382399916649
Epoch:  99 	 Loss:  0.02713688835501671
Epoch:  100 	 Loss:  0.026571057736873627
wandb: Waiting for W&B process to finish, PID 1763324
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151736-sq8bow9q/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151736-sq8bow9q/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02657
wandb:        _step 800000
wandb:     _runtime 85
wandb:   _timestamp 1612901941
wandb:         loss 0.02657
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–ˆâ–…â–…â–‡â–…â–„â–„â–‡â–ƒâ–…â–…â–‡â–‡â–„â–„â–…â–…â–…â–‡â–„â–ˆâ–„â–…â–„â–‡â–…â–†â–†â–…â–„â–…â–„â–†â–„â–…â–â–…â–†â–ƒ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced gentle-sweep-40: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/sq8bow9q
wandb: Agent Starting Run: doh966t6 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:19:08.007555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:19:08.013959: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run sleek-sweep-41
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/doh966t6
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_151905-doh966t6
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.026799477636814117
Epoch:  2 	 Loss:  0.027350369840860367
Epoch:  3 	 Loss:  0.026308240368962288
Epoch:  4 	 Loss:  0.027392908930778503
Epoch:  5 	 Loss:  0.026924265548586845
Epoch:  6 	 Loss:  0.027094583958387375
Epoch:  7 	 Loss:  0.027187297120690346
Epoch:  8 	 Loss:  0.026659680530428886
Epoch:  9 	 Loss:  0.02638179436326027
Epoch:  10 	 Loss:  0.02699493058025837
Epoch:  11 	 Loss:  0.026774434372782707
Epoch:  12 	 Loss:  0.027377989143133163
Epoch:  13 	 Loss:  0.02723785862326622
Epoch:  14 	 Loss:  0.02669038623571396
Epoch:  15 	 Loss:  0.026917247101664543
Epoch:  16 	 Loss:  0.026866551488637924
Epoch:  17 	 Loss:  0.02643672376871109
Epoch:  18 	 Loss:  0.02722969278693199
Epoch:  19 	 Loss:  0.027087010443210602
Epoch:  20 	 Loss:  0.026920748874545097
Epoch:  21 	 Loss:  0.027136093005537987
Epoch:  22 	 Loss:  0.026729580014944077
Epoch:  23 	 Loss:  0.026562847197055817
Epoch:  24 	 Loss:  0.026651501655578613
Epoch:  25 	 Loss:  0.02706856094300747
Epoch:  26 	 Loss:  0.027336036786437035
Epoch:  27 	 Loss:  0.02662644349038601
Epoch:  28 	 Loss:  0.027322204783558846
Epoch:  29 	 Loss:  0.0270376019179821
Epoch:  30 	 Loss:  0.026655035093426704
Epoch:  31 	 Loss:  0.0268615260720253
Epoch:  32 	 Loss:  0.02715311571955681
Epoch:  33 	 Loss:  0.027301104739308357
Epoch:  34 	 Loss:  0.02721218764781952
Epoch:  35 	 Loss:  0.026869159191846848
Epoch:  36 	 Loss:  0.026929933577775955
Epoch:  37 	 Loss:  0.026622066274285316
Epoch:  38 	 Loss:  0.027297671884298325
Epoch:  39 	 Loss:  0.027110040187835693
Epoch:  40 	 Loss:  0.02699422836303711
Epoch:  41 	 Loss:  0.02714216336607933
Epoch:  42 	 Loss:  0.026812942698597908
Epoch:  43 	 Loss:  0.027027243748307228
Epoch:  44 	 Loss:  0.026867402717471123
Epoch:  45 	 Loss:  0.027116088196635246
Epoch:  46 	 Loss:  0.02710048109292984
Epoch:  47 	 Loss:  0.027263104915618896
Epoch:  48 	 Loss:  0.027000343427062035
Epoch:  49 	 Loss:  0.027073079720139503
Epoch:  50 	 Loss:  0.026877297088503838
Epoch:  51 	 Loss:  0.027181267738342285
Epoch:  52 	 Loss:  0.027242325246334076
Epoch:  53 	 Loss:  0.026824450120329857
Epoch:  54 	 Loss:  0.0269613116979599
Epoch:  55 	 Loss:  0.026563387364149094
Epoch:  56 	 Loss:  0.02691430225968361
Epoch:  57 	 Loss:  0.027105744928121567
Epoch:  58 	 Loss:  0.026903625577688217
Epoch:  59 	 Loss:  0.027158737182617188
Epoch:  60 	 Loss:  0.026403583586215973
Epoch:  61 	 Loss:  0.026831094175577164
Epoch:  62 	 Loss:  0.026599746197462082
Epoch:  63 	 Loss:  0.02740226313471794
Epoch:  64 	 Loss:  0.026930488646030426
Epoch:  65 	 Loss:  0.026962123811244965
Epoch:  66 	 Loss:  0.026809994131326675
Epoch:  67 	 Loss:  0.026674000546336174
Epoch:  68 	 Loss:  0.02683149464428425
Epoch:  69 	 Loss:  0.02670394815504551
Epoch:  70 	 Loss:  0.02724447473883629
Epoch:  71 	 Loss:  0.027143729850649834
Epoch:  72 	 Loss:  0.026888107880949974
Epoch:  73 	 Loss:  0.02710464783012867
Epoch:  74 	 Loss:  0.026877090334892273
Epoch:  75 	 Loss:  0.02710064873099327
Epoch:  76 	 Loss:  0.02654123306274414
Epoch:  77 	 Loss:  0.027254343032836914
Epoch:  78 	 Loss:  0.026843639090657234
Epoch:  79 	 Loss:  0.026981934905052185
Epoch:  80 	 Loss:  0.026733970269560814
Epoch:  81 	 Loss:  0.027112716808915138
Epoch:  82 	 Loss:  0.026725713163614273
Epoch:  83 	 Loss:  0.02667500264942646
Epoch:  84 	 Loss:  0.027103308588266373
Epoch:  85 	 Loss:  0.026941120624542236
Epoch:  86 	 Loss:  0.02713196538388729
Epoch:  87 	 Loss:  0.026655862107872963
Epoch:  88 	 Loss:  0.026239357888698578
Epoch:  89 	 Loss:  0.026736993342638016
Epoch:  90 	 Loss:  0.027176260948181152
Epoch:  91 	 Loss:  0.026601901277899742
Epoch:  92 	 Loss:  0.027055079117417336
Epoch:  93 	 Loss:  0.02677396684885025
Epoch:  94 	 Loss:  0.026930062100291252
Epoch:  95 	 Loss:  0.02668836899101734
Epoch:  96 	 Loss:  0.026714324951171875
Epoch:  97 	 Loss:  0.026441624388098717
Epoch:  98 	 Loss:  0.027059825137257576
Epoch:  99 	 Loss:  0.02664855867624283
Epoch:  100 	 Loss:  0.02682538516819477
wandb: Waiting for W&B process to finish, PID 1767015
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151905-doh966t6/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_151905-doh966t6/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02683
wandb:        _step 800000
wandb:     _runtime 74
wandb:   _timestamp 1612902020
wandb:         loss 0.02683
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–â–†â–ƒâ–„â–‡â–…â–‡â–‡â–ƒâ–ˆâ–ˆâ–…â–‡â–…â–†â–‡â–…â–†â–†â–‡â–…â–…â–‡â–…â–…â–ƒâ–„â–…â–…â–‡â–†â–„â–†â–ƒâ–„â–†â–…â–‚â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced sleek-sweep-41: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/doh966t6
wandb: Agent Starting Run: 9ivdyk5s with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:20:26.930991: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:20:26.937425: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run crisp-sweep-42
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/9ivdyk5s
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_152024-9ivdyk5s
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027203794568777084
Epoch:  2 	 Loss:  0.026792015880346298
Epoch:  3 	 Loss:  0.02688843570649624
Epoch:  4 	 Loss:  0.026781966909766197
Epoch:  5 	 Loss:  0.02752578817307949
Epoch:  6 	 Loss:  0.027137169614434242
Epoch:  7 	 Loss:  0.026873713359236717
Epoch:  8 	 Loss:  0.02740837074816227
Epoch:  9 	 Loss:  0.02708420902490616
Epoch:  10 	 Loss:  0.02757055126130581
Epoch:  11 	 Loss:  0.026993494480848312
Epoch:  12 	 Loss:  0.027565184980630875
Epoch:  13 	 Loss:  0.027420012280344963
Epoch:  14 	 Loss:  0.027395354583859444
Epoch:  15 	 Loss:  0.027230266481637955
Epoch:  16 	 Loss:  0.027393240481615067
Epoch:  17 	 Loss:  0.027579670771956444
Epoch:  18 	 Loss:  0.027306826785206795
Epoch:  19 	 Loss:  0.027490362524986267
Epoch:  20 	 Loss:  0.02705124206840992
Epoch:  21 	 Loss:  0.027104496955871582
Epoch:  22 	 Loss:  0.02751915156841278
Epoch:  23 	 Loss:  0.027492893859744072
Epoch:  24 	 Loss:  0.027044814079999924
Epoch:  25 	 Loss:  0.02736668288707733
Epoch:  26 	 Loss:  0.026930030435323715
Epoch:  27 	 Loss:  0.02750268764793873
Epoch:  28 	 Loss:  0.027190253138542175
Epoch:  29 	 Loss:  0.027559606358408928
Epoch:  30 	 Loss:  0.02743607386946678
Epoch:  31 	 Loss:  0.027526384219527245
Epoch:  32 	 Loss:  0.026924023404717445
Epoch:  33 	 Loss:  0.027482453733682632
Epoch:  34 	 Loss:  0.027040766552090645
Epoch:  35 	 Loss:  0.026955466717481613
Epoch:  36 	 Loss:  0.02730812318623066
Epoch:  37 	 Loss:  0.02693951688706875
Epoch:  38 	 Loss:  0.026858942583203316
Epoch:  39 	 Loss:  0.02712753601372242
Epoch:  40 	 Loss:  0.02747037634253502
Epoch:  41 	 Loss:  0.02715008519589901
Epoch:  42 	 Loss:  0.02722022496163845
Epoch:  43 	 Loss:  0.02763647772371769
Epoch:  44 	 Loss:  0.027158021926879883
Epoch:  45 	 Loss:  0.026803920045495033
Epoch:  46 	 Loss:  0.027112459763884544
Epoch:  47 	 Loss:  0.02719786949455738
Epoch:  48 	 Loss:  0.026983072981238365
Epoch:  49 	 Loss:  0.02733900025486946
Epoch:  50 	 Loss:  0.027287088334560394
Epoch:  51 	 Loss:  0.02722334861755371
Epoch:  52 	 Loss:  0.027009474113583565
Epoch:  53 	 Loss:  0.027535080909729004
Epoch:  54 	 Loss:  0.02764173038303852
Epoch:  55 	 Loss:  0.027094539254903793
Epoch:  56 	 Loss:  0.027549900114536285
Epoch:  57 	 Loss:  0.02739378996193409
Epoch:  58 	 Loss:  0.0268135704100132
Epoch:  59 	 Loss:  0.027236223220825195
Epoch:  60 	 Loss:  0.02679758332669735
Epoch:  61 	 Loss:  0.027352333068847656
Epoch:  62 	 Loss:  0.02711787447333336
Epoch:  63 	 Loss:  0.026912638917565346
Epoch:  64 	 Loss:  0.027061007916927338
Epoch:  65 	 Loss:  0.027345845475792885
Epoch:  66 	 Loss:  0.027150755748152733
Epoch:  67 	 Loss:  0.027060357853770256
Epoch:  68 	 Loss:  0.027545351535081863
Epoch:  69 	 Loss:  0.027167553082108498
Epoch:  70 	 Loss:  0.027263280004262924
Epoch:  71 	 Loss:  0.0270107239484787
Epoch:  72 	 Loss:  0.026959287002682686
Epoch:  73 	 Loss:  0.02694890648126602
Epoch:  74 	 Loss:  0.027381012216210365
Epoch:  75 	 Loss:  0.027201013639569283
Epoch:  76 	 Loss:  0.026972567662596703
Epoch:  77 	 Loss:  0.026909934356808662
Epoch:  78 	 Loss:  0.02704271487891674
Epoch:  79 	 Loss:  0.02707473188638687
Epoch:  80 	 Loss:  0.027030523866415024
Epoch:  81 	 Loss:  0.026928311213850975
Epoch:  82 	 Loss:  0.027372106909751892
Epoch:  83 	 Loss:  0.02752574533224106
Epoch:  84 	 Loss:  0.027688516303896904
Epoch:  85 	 Loss:  0.027700794860720634
Epoch:  86 	 Loss:  0.027419118210673332
Epoch:  87 	 Loss:  0.026575079187750816
Epoch:  88 	 Loss:  0.027492737397551537
Epoch:  89 	 Loss:  0.026624375954270363
Epoch:  90 	 Loss:  0.027511853724718094
Epoch:  91 	 Loss:  0.027292044833302498
Epoch:  92 	 Loss:  0.027075735852122307
Epoch:  93 	 Loss:  0.027183320373296738
Epoch:  94 	 Loss:  0.027479009702801704
Epoch:  95 	 Loss:  0.02701614983379841
Epoch:  96 	 Loss:  0.027144093066453934
Epoch:  97 	 Loss:  0.027219831943511963
Epoch:  98 	 Loss:  0.027110103517770767
Epoch:  99 	 Loss:  0.027486130595207214
Epoch:  100 	 Loss:  0.02695458196103573
wandb: Waiting for W&B process to finish, PID 1770701
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152024-9ivdyk5s/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152024-9ivdyk5s/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02695
wandb:        _step 800000
wandb:     _runtime 80
wandb:   _timestamp 1612902105
wandb:         loss 0.02695
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–ƒâ–…â–†â–„â–†â–†â–†â–„â–‡â–ƒâ–…â–‡â–„â–†â–„â–…â–…â–„â–†â–…â–ˆâ–‡â–…â–†â–„â–„â–…â–ƒâ–†â–ƒâ–„â–†â–ˆâ–â–â–„â–‡â–…â–ƒ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced crisp-sweep-42: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/9ivdyk5s
wandb: Agent Starting Run: 8esxvnot with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:21:51.341971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:21:51.348650: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run faithful-sweep-43
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/8esxvnot
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_152149-8esxvnot
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028990168124437332
Epoch:  2 	 Loss:  0.028918104246258736
Epoch:  3 	 Loss:  0.028885629028081894
Epoch:  4 	 Loss:  0.02943551167845726
Epoch:  5 	 Loss:  0.02901221625506878
Epoch:  6 	 Loss:  0.02882041409611702
Epoch:  7 	 Loss:  0.029222125187516212
Epoch:  8 	 Loss:  0.02924141101539135
Epoch:  9 	 Loss:  0.028969094157218933
Epoch:  10 	 Loss:  0.02925865910947323
Epoch:  11 	 Loss:  0.02916756458580494
Epoch:  12 	 Loss:  0.028993600979447365
Epoch:  13 	 Loss:  0.028436196967959404
Epoch:  14 	 Loss:  0.028746163472533226
Epoch:  15 	 Loss:  0.028271839022636414
Epoch:  16 	 Loss:  0.028807125985622406
Epoch:  17 	 Loss:  0.028933996334671974
Epoch:  18 	 Loss:  0.0292136799544096
Epoch:  19 	 Loss:  0.02849290892481804
Epoch:  20 	 Loss:  0.028734413906931877
Epoch:  21 	 Loss:  0.028986820951104164
Epoch:  22 	 Loss:  0.0285553690046072
Epoch:  23 	 Loss:  0.02882949262857437
Epoch:  24 	 Loss:  0.028990821912884712
Epoch:  25 	 Loss:  0.029207449406385422
Epoch:  26 	 Loss:  0.029000813141465187
Epoch:  27 	 Loss:  0.028907446190714836
Epoch:  28 	 Loss:  0.029233671724796295
Epoch:  29 	 Loss:  0.028851615265011787
Epoch:  30 	 Loss:  0.02908329665660858
Epoch:  31 	 Loss:  0.028734467923641205
Epoch:  32 	 Loss:  0.028704119846224785
Epoch:  33 	 Loss:  0.02918391488492489
Epoch:  34 	 Loss:  0.028029341250658035
Epoch:  35 	 Loss:  0.028804805129766464
Epoch:  36 	 Loss:  0.029026703909039497
Epoch:  37 	 Loss:  0.029474295675754547
Epoch:  38 	 Loss:  0.028860853984951973
Epoch:  39 	 Loss:  0.02864023856818676
Epoch:  40 	 Loss:  0.02839096076786518
Epoch:  41 	 Loss:  0.02870062179863453
Epoch:  42 	 Loss:  0.02889464981853962
Epoch:  43 	 Loss:  0.02893330529332161
Epoch:  44 	 Loss:  0.028514191508293152
Epoch:  45 	 Loss:  0.02819291315972805
Epoch:  46 	 Loss:  0.028564797714352608
Epoch:  47 	 Loss:  0.028900068253278732
Epoch:  48 	 Loss:  0.028988048434257507
Epoch:  49 	 Loss:  0.028335126116871834
Epoch:  50 	 Loss:  0.028290212154388428
Epoch:  51 	 Loss:  0.02865290641784668
Epoch:  52 	 Loss:  0.029072191566228867
Epoch:  53 	 Loss:  0.02845398150384426
Epoch:  54 	 Loss:  0.028862809762358665
Epoch:  55 	 Loss:  0.02890002727508545
Epoch:  56 	 Loss:  0.029606850817799568
Epoch:  57 	 Loss:  0.028971459716558456
Epoch:  58 	 Loss:  0.029166629537940025
Epoch:  59 	 Loss:  0.02853723056614399
Epoch:  60 	 Loss:  0.02838508039712906
Epoch:  61 	 Loss:  0.028863219544291496
Epoch:  62 	 Loss:  0.028596987947821617
Epoch:  63 	 Loss:  0.028680233284831047
Epoch:  64 	 Loss:  0.02866293303668499
Epoch:  65 	 Loss:  0.02956417389214039
Epoch:  66 	 Loss:  0.028828617185354233
Epoch:  67 	 Loss:  0.028322426602244377
Epoch:  68 	 Loss:  0.02861298993229866
Epoch:  69 	 Loss:  0.028730718418955803
Epoch:  70 	 Loss:  0.028476815670728683
Epoch:  71 	 Loss:  0.02880137227475643
Epoch:  72 	 Loss:  0.02839193493127823
Epoch:  73 	 Loss:  0.02863043174147606
Epoch:  74 	 Loss:  0.028838878497481346
Epoch:  75 	 Loss:  0.029198037460446358
Epoch:  76 	 Loss:  0.029334401711821556
Epoch:  77 	 Loss:  0.028886253014206886
Epoch:  78 	 Loss:  0.02895824797451496
Epoch:  79 	 Loss:  0.02885669469833374
Epoch:  80 	 Loss:  0.02877567894756794
Epoch:  81 	 Loss:  0.02896101213991642
Epoch:  82 	 Loss:  0.028861723840236664
Epoch:  83 	 Loss:  0.0287178885191679
Epoch:  84 	 Loss:  0.029101578518748283
Epoch:  85 	 Loss:  0.028063969686627388
Epoch:  86 	 Loss:  0.028942961245775223
Epoch:  87 	 Loss:  0.02933572418987751
Epoch:  88 	 Loss:  0.029010750353336334
Epoch:  89 	 Loss:  0.029080849140882492
Epoch:  90 	 Loss:  0.028332553803920746
Epoch:  91 	 Loss:  0.029028860852122307
Epoch:  92 	 Loss:  0.029251588508486748
Epoch:  93 	 Loss:  0.029549244791269302
Epoch:  94 	 Loss:  0.028327252715826035
Epoch:  95 	 Loss:  0.028957774862647057
Epoch:  96 	 Loss:  0.028947917744517326
Epoch:  97 	 Loss:  0.029907364398241043
Epoch:  98 	 Loss:  0.029192177578806877
Epoch:  99 	 Loss:  0.02929868921637535
Epoch:  100 	 Loss:  0.028534211218357086
wandb: Waiting for W&B process to finish, PID 1774383
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152149-8esxvnot/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152149-8esxvnot/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02853
wandb:        _step 800000
wandb:     _runtime 80
wandb:   _timestamp 1612902189
wandb:         loss 0.02853
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–„â–„â–†â–…â–ƒâ–„â–…â–…â–„â–…â–…â–„â–â–…â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–„â–‡â–ƒâ–„â–ƒâ–‚â–„â–‚â–„â–„â–„â–„â–…â–†â–…â–†â–‚â–ˆâ–ƒ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced faithful-sweep-43: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/8esxvnot
wandb: Agent Starting Run: kbd6tnj7 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:23:15.293105: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:23:15.299007: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run proud-sweep-44
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/kbd6tnj7
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_152313-kbd6tnj7
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.029395543038845062
Epoch:  2 	 Loss:  0.028692321851849556
Epoch:  3 	 Loss:  0.02932717651128769
Epoch:  4 	 Loss:  0.028038376942276955
Epoch:  5 	 Loss:  0.02925296500325203
Epoch:  6 	 Loss:  0.02903408743441105
Epoch:  7 	 Loss:  0.02874577045440674
Epoch:  8 	 Loss:  0.029705286026000977
Epoch:  9 	 Loss:  0.028921928256750107
Epoch:  10 	 Loss:  0.029374053701758385
Epoch:  11 	 Loss:  0.028877872973680496
Epoch:  12 	 Loss:  0.028845304623246193
Epoch:  13 	 Loss:  0.02857031673192978
Epoch:  14 	 Loss:  0.029501227661967278
Epoch:  15 	 Loss:  0.0295261237770319
Epoch:  16 	 Loss:  0.02865108661353588
Epoch:  17 	 Loss:  0.028996100649237633
Epoch:  18 	 Loss:  0.029682837426662445
Epoch:  19 	 Loss:  0.028918342664837837
Epoch:  20 	 Loss:  0.02948474884033203
Epoch:  21 	 Loss:  0.029123112559318542
Epoch:  22 	 Loss:  0.029829492792487144
Epoch:  23 	 Loss:  0.028717420995235443
Epoch:  24 	 Loss:  0.028790323063731194
Epoch:  25 	 Loss:  0.029244428500533104
Epoch:  26 	 Loss:  0.02866225875914097
Epoch:  27 	 Loss:  0.029268627986311913
Epoch:  28 	 Loss:  0.029715055599808693
Epoch:  29 	 Loss:  0.02925749309360981
Epoch:  30 	 Loss:  0.029301004484295845
Epoch:  31 	 Loss:  0.02913646399974823
Epoch:  32 	 Loss:  0.029078418388962746
Epoch:  33 	 Loss:  0.029281528666615486
Epoch:  34 	 Loss:  0.02921789325773716
Epoch:  35 	 Loss:  0.029207568615674973
Epoch:  36 	 Loss:  0.028877638280391693
Epoch:  37 	 Loss:  0.029481856152415276
Epoch:  38 	 Loss:  0.02926163375377655
Epoch:  39 	 Loss:  0.029050564393401146
Epoch:  40 	 Loss:  0.029161471873521805
Epoch:  41 	 Loss:  0.029603388160467148
Epoch:  42 	 Loss:  0.029737409204244614
Epoch:  43 	 Loss:  0.02881668694317341
Epoch:  44 	 Loss:  0.029741212725639343
Epoch:  45 	 Loss:  0.02960813045501709
Epoch:  46 	 Loss:  0.029466189444065094
Epoch:  47 	 Loss:  0.02926306426525116
Epoch:  48 	 Loss:  0.029905682429671288
Epoch:  49 	 Loss:  0.02893715165555477
Epoch:  50 	 Loss:  0.02913915552198887
Epoch:  51 	 Loss:  0.02858005091547966
Epoch:  52 	 Loss:  0.028866248205304146
Epoch:  53 	 Loss:  0.02929569222033024
Epoch:  54 	 Loss:  0.02923675626516342
Epoch:  55 	 Loss:  0.02796334959566593
Epoch:  56 	 Loss:  0.029566099867224693
Epoch:  57 	 Loss:  0.029017653316259384
Epoch:  58 	 Loss:  0.029400957748293877
Epoch:  59 	 Loss:  0.029294397681951523
Epoch:  60 	 Loss:  0.02949206531047821
Epoch:  61 	 Loss:  0.029714185744524002
Epoch:  62 	 Loss:  0.029269438236951828
Epoch:  63 	 Loss:  0.02916393242776394
Epoch:  64 	 Loss:  0.02928115800023079
Epoch:  65 	 Loss:  0.02939971163868904
Epoch:  66 	 Loss:  0.028741294518113136
Epoch:  67 	 Loss:  0.02878906950354576
Epoch:  68 	 Loss:  0.029943471774458885
Epoch:  69 	 Loss:  0.028538038954138756
Epoch:  70 	 Loss:  0.02892211265861988
Epoch:  71 	 Loss:  0.029043154790997505
Epoch:  72 	 Loss:  0.029847387224435806
Epoch:  73 	 Loss:  0.02813798561692238
Epoch:  74 	 Loss:  0.02902611903846264
Epoch:  75 	 Loss:  0.029321350157260895
Epoch:  76 	 Loss:  0.029440462589263916
Epoch:  77 	 Loss:  0.028601031750440598
Epoch:  78 	 Loss:  0.03009718656539917
Epoch:  79 	 Loss:  0.028586795553565025
Epoch:  80 	 Loss:  0.02946401946246624
Epoch:  81 	 Loss:  0.029149124398827553
Epoch:  82 	 Loss:  0.029837878420948982
Epoch:  83 	 Loss:  0.029459388926625252
Epoch:  84 	 Loss:  0.0298536978662014
Epoch:  85 	 Loss:  0.02925027348101139
Epoch:  86 	 Loss:  0.028755225241184235
Epoch:  87 	 Loss:  0.028993628919124603
Epoch:  88 	 Loss:  0.028856057673692703
Epoch:  89 	 Loss:  0.02891528606414795
Epoch:  90 	 Loss:  0.02981891855597496
Epoch:  91 	 Loss:  0.02927526645362377
Epoch:  92 	 Loss:  0.02923070453107357
Epoch:  93 	 Loss:  0.029458798468112946
Epoch:  94 	 Loss:  0.029489723965525627
Epoch:  95 	 Loss:  0.028712229803204536
Epoch:  96 	 Loss:  0.029223648831248283
Epoch:  97 	 Loss:  0.029031511396169662
Epoch:  98 	 Loss:  0.029322680085897446
Epoch:  99 	 Loss:  0.02956286258995533
Epoch:  100 	 Loss:  0.028697893023490906
wandb: Waiting for W&B process to finish, PID 1778071
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152313-kbd6tnj7/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152313-kbd6tnj7/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0287
wandb:        _step 800000
wandb:     _runtime 73
wandb:   _timestamp 1612902266
wandb:         loss 0.0287
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–…â–„â–‡â–ƒâ–â–‚â–‡â–„â–‚â–‚â–‡â–„â–…â–ƒâ–„â–‡â–‡â–†â–ƒâ–â–…â–†â–…â–‡â–…â–‚â–â–ˆâ–„â–â–â–ˆâ–ˆâ–ƒâ–ƒâ–…â–†â–„â–‚
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced proud-sweep-44: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/kbd6tnj7
wandb: Agent Starting Run: n5javra8 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:24:32.466554: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:24:32.471834: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run misunderstood-sweep-45
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/n5javra8
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_152430-n5javra8
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.029072221368551254
Epoch:  2 	 Loss:  0.029021790251135826
Epoch:  3 	 Loss:  0.028792953118681908
Epoch:  4 	 Loss:  0.02867894433438778
Epoch:  5 	 Loss:  0.028476163744926453
Epoch:  6 	 Loss:  0.028211360797286034
Epoch:  7 	 Loss:  0.028904732316732407
Epoch:  8 	 Loss:  0.028973039239645004
Epoch:  9 	 Loss:  0.029610490426421165
Epoch:  10 	 Loss:  0.028615718707442284
Epoch:  11 	 Loss:  0.028654154390096664
Epoch:  12 	 Loss:  0.028838712722063065
Epoch:  13 	 Loss:  0.02974994294345379
Epoch:  14 	 Loss:  0.028927408158779144
Epoch:  15 	 Loss:  0.02838214859366417
Epoch:  16 	 Loss:  0.02949892170727253
Epoch:  17 	 Loss:  0.028902411460876465
Epoch:  18 	 Loss:  0.028533142060041428
Epoch:  19 	 Loss:  0.02851475216448307
Epoch:  20 	 Loss:  0.028620274737477303
Epoch:  21 	 Loss:  0.028689729049801826
Epoch:  22 	 Loss:  0.028649568557739258
Epoch:  23 	 Loss:  0.029240932315587997
Epoch:  24 	 Loss:  0.02832409366965294
Epoch:  25 	 Loss:  0.028898926451802254
Epoch:  26 	 Loss:  0.028179626911878586
Epoch:  27 	 Loss:  0.028917009010910988
Epoch:  28 	 Loss:  0.02892754040658474
Epoch:  29 	 Loss:  0.028374888002872467
Epoch:  30 	 Loss:  0.02805810235440731
Epoch:  31 	 Loss:  0.028746599331498146
Epoch:  32 	 Loss:  0.0288083516061306
Epoch:  33 	 Loss:  0.02859179675579071
Epoch:  34 	 Loss:  0.028410786762833595
Epoch:  35 	 Loss:  0.02888261340558529
Epoch:  36 	 Loss:  0.028930112719535828
Epoch:  37 	 Loss:  0.028774848207831383
Epoch:  38 	 Loss:  0.02876690775156021
Epoch:  39 	 Loss:  0.028094163164496422
Epoch:  40 	 Loss:  0.02850501239299774
Epoch:  41 	 Loss:  0.02842218242585659
Epoch:  42 	 Loss:  0.029073204845190048
Epoch:  43 	 Loss:  0.028370916843414307
Epoch:  44 	 Loss:  0.02940226159989834
Epoch:  45 	 Loss:  0.029112940654158592
Epoch:  46 	 Loss:  0.02819286473095417
Epoch:  47 	 Loss:  0.02866247110068798
Epoch:  48 	 Loss:  0.028885051608085632
Epoch:  49 	 Loss:  0.0294623002409935
Epoch:  50 	 Loss:  0.028729965910315514
Epoch:  51 	 Loss:  0.0285659022629261
Epoch:  52 	 Loss:  0.02829301729798317
Epoch:  53 	 Loss:  0.029292838647961617
Epoch:  54 	 Loss:  0.028723716735839844
Epoch:  55 	 Loss:  0.028829330578446388
Epoch:  56 	 Loss:  0.028992295265197754
Epoch:  57 	 Loss:  0.029138978570699692
Epoch:  58 	 Loss:  0.0286578256636858
Epoch:  59 	 Loss:  0.02886733040213585
Epoch:  60 	 Loss:  0.028651965782046318
Epoch:  61 	 Loss:  0.028322558850049973
Epoch:  62 	 Loss:  0.0281368400901556
Epoch:  63 	 Loss:  0.0290486179292202
Epoch:  64 	 Loss:  0.028372539207339287
Epoch:  65 	 Loss:  0.02814480848610401
Epoch:  66 	 Loss:  0.02921069972217083
Epoch:  67 	 Loss:  0.028365442529320717
Epoch:  68 	 Loss:  0.029484014958143234
Epoch:  69 	 Loss:  0.02864759787917137
Epoch:  70 	 Loss:  0.029206352308392525
Epoch:  71 	 Loss:  0.029245302081108093
Epoch:  72 	 Loss:  0.028851574286818504
Epoch:  73 	 Loss:  0.028913315385580063
Epoch:  74 	 Loss:  0.029548829421401024
Epoch:  75 	 Loss:  0.028653794899582863
Epoch:  76 	 Loss:  0.02871818281710148
Epoch:  77 	 Loss:  0.028800921514630318
Epoch:  78 	 Loss:  0.028987323865294456
Epoch:  79 	 Loss:  0.028525732457637787
Epoch:  80 	 Loss:  0.028113897889852524
Epoch:  81 	 Loss:  0.029007622972130775
Epoch:  82 	 Loss:  0.028800688683986664
Epoch:  83 	 Loss:  0.02932409942150116
Epoch:  84 	 Loss:  0.028579160571098328
Epoch:  85 	 Loss:  0.028871674090623856
Epoch:  86 	 Loss:  0.028792450204491615
Epoch:  87 	 Loss:  0.02895605005323887
Epoch:  88 	 Loss:  0.028797345235943794
Epoch:  89 	 Loss:  0.029193799942731857
Epoch:  90 	 Loss:  0.02847304567694664
Epoch:  91 	 Loss:  0.028861209750175476
Epoch:  92 	 Loss:  0.028684064745903015
Epoch:  93 	 Loss:  0.028942491859197617
Epoch:  94 	 Loss:  0.02893509343266487
Epoch:  95 	 Loss:  0.028869276866316795
Epoch:  96 	 Loss:  0.028787437826395035
Epoch:  97 	 Loss:  0.029433345422148705
Epoch:  98 	 Loss:  0.028276855126023293
Epoch:  99 	 Loss:  0.02911844477057457
Epoch:  100 	 Loss:  0.028781816363334656
wandb: Waiting for W&B process to finish, PID 1781770
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152430-n5javra8/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152430-n5javra8/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02878
wandb:        _step 800000
wandb:     _runtime 75
wandb:   _timestamp 1612902345
wandb:         loss 0.02878
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–„â–â–…â–ƒâ–ˆâ–‡â–ƒâ–„â–†â–â–…â–„â–‚â–…â–â–‚â–‡â–â–‡â–ƒâ–„â–…â–„â–‚â–‚â–‚â–ƒâ–„â–‡â–„â–ƒâ–„â–ƒâ–…â–†â–ƒâ–…â–‡â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced misunderstood-sweep-45: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/n5javra8
wandb: Agent Starting Run: lbr8oasi with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:25:51.982240: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:25:51.987925: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run curious-sweep-46
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/lbr8oasi
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_152550-lbr8oasi
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028204234316945076
Epoch:  2 	 Loss:  0.027581164613366127
Epoch:  3 	 Loss:  0.02814829908311367
Epoch:  4 	 Loss:  0.02755690924823284
Epoch:  5 	 Loss:  0.028582783415913582
Epoch:  6 	 Loss:  0.02870621159672737
Epoch:  7 	 Loss:  0.02848276123404503
Epoch:  8 	 Loss:  0.028060056269168854
Epoch:  9 	 Loss:  0.028684798628091812
Epoch:  10 	 Loss:  0.028642579913139343
Epoch:  11 	 Loss:  0.02834632620215416
Epoch:  12 	 Loss:  0.028245333582162857
Epoch:  13 	 Loss:  0.02848912589251995
Epoch:  14 	 Loss:  0.027742009609937668
Epoch:  15 	 Loss:  0.028298722580075264
Epoch:  16 	 Loss:  0.02834033966064453
Epoch:  17 	 Loss:  0.0286429300904274
Epoch:  18 	 Loss:  0.02837526798248291
Epoch:  19 	 Loss:  0.02798115834593773
Epoch:  20 	 Loss:  0.02806522324681282
Epoch:  21 	 Loss:  0.02881259098649025
Epoch:  22 	 Loss:  0.02894081547856331
Epoch:  23 	 Loss:  0.027900462970137596
Epoch:  24 	 Loss:  0.02832537516951561
Epoch:  25 	 Loss:  0.028402959927916527
Epoch:  26 	 Loss:  0.028549887239933014
Epoch:  27 	 Loss:  0.028487343341112137
Epoch:  28 	 Loss:  0.028756003826856613
Epoch:  29 	 Loss:  0.028689472004771233
Epoch:  30 	 Loss:  0.02886657603085041
Epoch:  31 	 Loss:  0.028340330347418785
Epoch:  32 	 Loss:  0.028403813019394875
Epoch:  33 	 Loss:  0.028248488903045654
Epoch:  34 	 Loss:  0.028691625222563744
Epoch:  35 	 Loss:  0.028410617262125015
Epoch:  36 	 Loss:  0.02846003882586956
Epoch:  37 	 Loss:  0.02856622263789177
Epoch:  38 	 Loss:  0.027818812057375908
Epoch:  39 	 Loss:  0.028693612664937973
Epoch:  40 	 Loss:  0.028646092861890793
Epoch:  41 	 Loss:  0.028665009886026382
Epoch:  42 	 Loss:  0.02867244742810726
Epoch:  43 	 Loss:  0.02804514579474926
Epoch:  44 	 Loss:  0.02816685289144516
Epoch:  45 	 Loss:  0.02835695631802082
Epoch:  46 	 Loss:  0.028267085552215576
Epoch:  47 	 Loss:  0.02826058305799961
Epoch:  48 	 Loss:  0.02870015986263752
Epoch:  49 	 Loss:  0.028719818219542503
Epoch:  50 	 Loss:  0.028182970359921455
Epoch:  51 	 Loss:  0.0283107440918684
Epoch:  52 	 Loss:  0.028195537626743317
Epoch:  53 	 Loss:  0.028386304154992104
Epoch:  54 	 Loss:  0.02876114845275879
Epoch:  55 	 Loss:  0.028430677950382233
Epoch:  56 	 Loss:  0.028718842193484306
Epoch:  57 	 Loss:  0.028164569288492203
Epoch:  58 	 Loss:  0.028546283021569252
Epoch:  59 	 Loss:  0.02821340039372444
Epoch:  60 	 Loss:  0.02879841811954975
Epoch:  61 	 Loss:  0.02860444411635399
Epoch:  62 	 Loss:  0.028618015348911285
Epoch:  63 	 Loss:  0.02873094193637371
Epoch:  64 	 Loss:  0.028232568874955177
Epoch:  65 	 Loss:  0.027966663241386414
Epoch:  66 	 Loss:  0.028032606467604637
Epoch:  67 	 Loss:  0.028448062017560005
Epoch:  68 	 Loss:  0.02830885350704193
Epoch:  69 	 Loss:  0.028438059613108635
Epoch:  70 	 Loss:  0.029197944328188896
Epoch:  71 	 Loss:  0.028246760368347168
Epoch:  72 	 Loss:  0.028538336977362633
Epoch:  73 	 Loss:  0.02861027419567108
Epoch:  74 	 Loss:  0.028460299596190453
Epoch:  75 	 Loss:  0.028485573828220367
Epoch:  76 	 Loss:  0.028531400486826897
Epoch:  77 	 Loss:  0.028731610625982285
Epoch:  78 	 Loss:  0.028349243104457855
Epoch:  79 	 Loss:  0.02864845283329487
Epoch:  80 	 Loss:  0.02840241976082325
Epoch:  81 	 Loss:  0.027861570939421654
Epoch:  82 	 Loss:  0.02849714457988739
Epoch:  83 	 Loss:  0.028439421206712723
Epoch:  84 	 Loss:  0.028371378779411316
Epoch:  85 	 Loss:  0.028175756335258484
Epoch:  86 	 Loss:  0.02881496213376522
Epoch:  87 	 Loss:  0.02899508737027645
Epoch:  88 	 Loss:  0.02887643687427044
Epoch:  89 	 Loss:  0.028771350160241127
Epoch:  90 	 Loss:  0.028429415076971054
Epoch:  91 	 Loss:  0.028345800936222076
Epoch:  92 	 Loss:  0.028358930721879005
Epoch:  93 	 Loss:  0.028522472828626633
Epoch:  94 	 Loss:  0.02851058356463909
Epoch:  95 	 Loss:  0.028808927163481712
Epoch:  96 	 Loss:  0.027966385707259178
Epoch:  97 	 Loss:  0.027718540281057358
Epoch:  98 	 Loss:  0.02898520603775978
Epoch:  99 	 Loss:  0.02811094932258129
Epoch:  100 	 Loss:  0.028835246339440346
wandb: Waiting for W&B process to finish, PID 1785451
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152550-lbr8oasi/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152550-lbr8oasi/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02884
wandb:        _step 800000
wandb:     _runtime 76
wandb:   _timestamp 1612902426
wandb:         loss 0.02884
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–ƒâ–†â–ƒâ–„â–…â–„â–…â–‡â–‚â–†â–‡â–„â–†â–…â–†â–†â–ƒâ–„â–†â–„â–‡â–†â–„â–†â–„â–…â–…â–…â–…â–‡â–†â–…â–…â–ˆâ–‡â–…â–…â–â–‡
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced curious-sweep-46: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/lbr8oasi
wandb: Agent Starting Run: g832lyw1 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:27:12.712760: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:27:12.718188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run dark-sweep-47
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/g832lyw1
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_152710-g832lyw1
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027224985882639885
Epoch:  2 	 Loss:  0.026998477056622505
Epoch:  3 	 Loss:  0.027411088347434998
Epoch:  4 	 Loss:  0.02662033773958683
Epoch:  5 	 Loss:  0.02712821401655674
Epoch:  6 	 Loss:  0.027317767962813377
Epoch:  7 	 Loss:  0.027274446561932564
Epoch:  8 	 Loss:  0.027130410075187683
Epoch:  9 	 Loss:  0.0271177738904953
Epoch:  10 	 Loss:  0.02690477855503559
Epoch:  11 	 Loss:  0.02699107863008976
Epoch:  12 	 Loss:  0.027227576822042465
Epoch:  13 	 Loss:  0.02723262459039688
Epoch:  14 	 Loss:  0.02719748020172119
Epoch:  15 	 Loss:  0.027299921959638596
Epoch:  16 	 Loss:  0.027057858183979988
Epoch:  17 	 Loss:  0.027142910286784172
Epoch:  18 	 Loss:  0.027016103267669678
Epoch:  19 	 Loss:  0.027134615927934647
Epoch:  20 	 Loss:  0.026607302948832512
Epoch:  21 	 Loss:  0.027165092527866364
Epoch:  22 	 Loss:  0.027251385152339935
Epoch:  23 	 Loss:  0.02701358124613762
Epoch:  24 	 Loss:  0.027515104040503502
Epoch:  25 	 Loss:  0.026876486837863922
Epoch:  26 	 Loss:  0.027378050610423088
Epoch:  27 	 Loss:  0.027296781539916992
Epoch:  28 	 Loss:  0.02738656848669052
Epoch:  29 	 Loss:  0.02734386920928955
Epoch:  30 	 Loss:  0.026914844289422035
Epoch:  31 	 Loss:  0.026900840923190117
Epoch:  32 	 Loss:  0.02688792534172535
Epoch:  33 	 Loss:  0.026849141344428062
Epoch:  34 	 Loss:  0.027287539094686508
Epoch:  35 	 Loss:  0.02688734419643879
Epoch:  36 	 Loss:  0.02714155800640583
Epoch:  37 	 Loss:  0.027000214904546738
Epoch:  38 	 Loss:  0.027325574308633804
Epoch:  39 	 Loss:  0.026934463530778885
Epoch:  40 	 Loss:  0.026617558673024178
Epoch:  41 	 Loss:  0.02670835517346859
Epoch:  42 	 Loss:  0.027373960241675377
Epoch:  43 	 Loss:  0.026728717610239983
Epoch:  44 	 Loss:  0.02655424363911152
Epoch:  45 	 Loss:  0.02671768143773079
Epoch:  46 	 Loss:  0.027234362438321114
Epoch:  47 	 Loss:  0.026738839223980904
Epoch:  48 	 Loss:  0.02688300609588623
Epoch:  49 	 Loss:  0.027002284303307533
Epoch:  50 	 Loss:  0.026706870645284653
Epoch:  51 	 Loss:  0.026749692857265472
Epoch:  52 	 Loss:  0.027137862518429756
Epoch:  53 	 Loss:  0.027449576184153557
Epoch:  54 	 Loss:  0.02705424651503563
Epoch:  55 	 Loss:  0.02722536213696003
Epoch:  56 	 Loss:  0.027352873235940933
Epoch:  57 	 Loss:  0.027403252199292183
Epoch:  58 	 Loss:  0.026844849810004234
Epoch:  59 	 Loss:  0.02703426405787468
Epoch:  60 	 Loss:  0.027145134285092354
Epoch:  61 	 Loss:  0.027219276875257492
Epoch:  62 	 Loss:  0.026440834626555443
Epoch:  63 	 Loss:  0.02707272209227085
Epoch:  64 	 Loss:  0.027267593890428543
Epoch:  65 	 Loss:  0.02712913602590561
Epoch:  66 	 Loss:  0.027264131233096123
Epoch:  67 	 Loss:  0.026863064616918564
Epoch:  68 	 Loss:  0.02754005417227745
Epoch:  69 	 Loss:  0.027334289625287056
Epoch:  70 	 Loss:  0.027017002925276756
Epoch:  71 	 Loss:  0.027413384988904
Epoch:  72 	 Loss:  0.02671116217970848
Epoch:  73 	 Loss:  0.027412330731749535
Epoch:  74 	 Loss:  0.02677781693637371
Epoch:  75 	 Loss:  0.0272632148116827
Epoch:  76 	 Loss:  0.02769996039569378
Epoch:  77 	 Loss:  0.026741040870547295
Epoch:  78 	 Loss:  0.02713688462972641
Epoch:  79 	 Loss:  0.02665305696427822
Epoch:  80 	 Loss:  0.026827815920114517
Epoch:  81 	 Loss:  0.027243375778198242
Epoch:  82 	 Loss:  0.02713402360677719
Epoch:  83 	 Loss:  0.02759058214724064
Epoch:  84 	 Loss:  0.026868971064686775
Epoch:  85 	 Loss:  0.02709660492837429
Epoch:  86 	 Loss:  0.02690349519252777
Epoch:  87 	 Loss:  0.027323290705680847
Epoch:  88 	 Loss:  0.026634972542524338
Epoch:  89 	 Loss:  0.02725556120276451
Epoch:  90 	 Loss:  0.0272660069167614
Epoch:  91 	 Loss:  0.02737346850335598
Epoch:  92 	 Loss:  0.026884932070970535
Epoch:  93 	 Loss:  0.027034863829612732
Epoch:  94 	 Loss:  0.026829810813069344
Epoch:  95 	 Loss:  0.0269523486495018
Epoch:  96 	 Loss:  0.02729579620063305
Epoch:  97 	 Loss:  0.027121521532535553
Epoch:  98 	 Loss:  0.026547055691480637
Epoch:  99 	 Loss:  0.027297822758555412
Epoch:  100 	 Loss:  0.027199584990739822
wandb: Waiting for W&B process to finish, PID 1789133
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152710-g832lyw1/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152710-g832lyw1/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0272
wandb:        _step 800000
wandb:     _runtime 77
wandb:   _timestamp 1612902507
wandb:         loss 0.0272
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–ˆâ–‡â–†â–…â–‡â–…â–…â–†â–…â–ˆâ–ˆâ–„â–‡â–†â–„â–‚â–â–‡â–…â–ƒâ–…â–ˆâ–…â–†â–‡â–„â–‡â–‚â–ƒâ–ƒâ–‚â–†â–„â–‡â–‡â–„â–ƒâ–†â–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced dark-sweep-47: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/g832lyw1
wandb: Agent Starting Run: 9r5egbc6 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:28:33.188535: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:28:33.195462: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run scarlet-sweep-48
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/9r5egbc6
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_152831-9r5egbc6
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.029464419931173325
Epoch:  2 	 Loss:  0.028754454106092453
Epoch:  3 	 Loss:  0.029501356184482574
Epoch:  4 	 Loss:  0.029441801831126213
Epoch:  5 	 Loss:  0.02929820492863655
Epoch:  6 	 Loss:  0.02981438860297203
Epoch:  7 	 Loss:  0.028920438140630722
Epoch:  8 	 Loss:  0.02821795642375946
Epoch:  9 	 Loss:  0.028679614886641502
Epoch:  10 	 Loss:  0.02853824943304062
Epoch:  11 	 Loss:  0.0294344425201416
Epoch:  12 	 Loss:  0.029267873615026474
Epoch:  13 	 Loss:  0.028825437650084496
Epoch:  14 	 Loss:  0.029054397717118263
Epoch:  15 	 Loss:  0.029216343536973
Epoch:  16 	 Loss:  0.029388707131147385
Epoch:  17 	 Loss:  0.02874194271862507
Epoch:  18 	 Loss:  0.029485877603292465
Epoch:  19 	 Loss:  0.029455121606588364
Epoch:  20 	 Loss:  0.029039103537797928
Epoch:  21 	 Loss:  0.029448196291923523
Epoch:  22 	 Loss:  0.028979448601603508
Epoch:  23 	 Loss:  0.029316594824194908
Epoch:  24 	 Loss:  0.029241066426038742
Epoch:  25 	 Loss:  0.028252551332116127
Epoch:  26 	 Loss:  0.02936197631061077
Epoch:  27 	 Loss:  0.02929757535457611
Epoch:  28 	 Loss:  0.029220841825008392
Epoch:  29 	 Loss:  0.028684312477707863
Epoch:  30 	 Loss:  0.029389049857854843
Epoch:  31 	 Loss:  0.028496425598859787
Epoch:  32 	 Loss:  0.028820903971791267
Epoch:  33 	 Loss:  0.028869962319731712
Epoch:  34 	 Loss:  0.02871767431497574
Epoch:  35 	 Loss:  0.029337456449866295
Epoch:  36 	 Loss:  0.029606299474835396
Epoch:  37 	 Loss:  0.029054421931505203
Epoch:  38 	 Loss:  0.029036063700914383
Epoch:  39 	 Loss:  0.02900833450257778
Epoch:  40 	 Loss:  0.029041726142168045
Epoch:  41 	 Loss:  0.02967941202223301
Epoch:  42 	 Loss:  0.029325002804398537
Epoch:  43 	 Loss:  0.02922821044921875
Epoch:  44 	 Loss:  0.02863006480038166
Epoch:  45 	 Loss:  0.029413606971502304
Epoch:  46 	 Loss:  0.029041359201073647
Epoch:  47 	 Loss:  0.029256250709295273
Epoch:  48 	 Loss:  0.028853764757514
Epoch:  49 	 Loss:  0.029796013608574867
Epoch:  50 	 Loss:  0.02875445783138275
Epoch:  51 	 Loss:  0.029468899592757225
Epoch:  52 	 Loss:  0.02897893451154232
Epoch:  53 	 Loss:  0.02957144007086754
Epoch:  54 	 Loss:  0.029469188302755356
Epoch:  55 	 Loss:  0.02933480218052864
Epoch:  56 	 Loss:  0.029274338856339455
Epoch:  57 	 Loss:  0.02901584282517433
Epoch:  58 	 Loss:  0.02924412488937378
Epoch:  59 	 Loss:  0.02946523018181324
Epoch:  60 	 Loss:  0.029412778094410896
Epoch:  61 	 Loss:  0.029019851237535477
Epoch:  62 	 Loss:  0.02898193709552288
Epoch:  63 	 Loss:  0.02872331626713276
Epoch:  64 	 Loss:  0.028840485960245132
Epoch:  65 	 Loss:  0.0292049553245306
Epoch:  66 	 Loss:  0.02902645245194435
Epoch:  67 	 Loss:  0.029013611376285553
Epoch:  68 	 Loss:  0.029819021001458168
Epoch:  69 	 Loss:  0.02851925417780876
Epoch:  70 	 Loss:  0.029316920787096024
Epoch:  71 	 Loss:  0.029032494872808456
Epoch:  72 	 Loss:  0.02849196456372738
Epoch:  73 	 Loss:  0.02881477400660515
Epoch:  74 	 Loss:  0.02925114333629608
Epoch:  75 	 Loss:  0.028739994391798973
Epoch:  76 	 Loss:  0.029093842953443527
Epoch:  77 	 Loss:  0.029538888484239578
Epoch:  78 	 Loss:  0.029140638187527657
Epoch:  79 	 Loss:  0.028380518779158592
Epoch:  80 	 Loss:  0.029523661360144615
Epoch:  81 	 Loss:  0.028993619605898857
Epoch:  82 	 Loss:  0.02933596633374691
Epoch:  83 	 Loss:  0.029559800401329994
Epoch:  84 	 Loss:  0.02879292704164982
Epoch:  85 	 Loss:  0.02895568311214447
Epoch:  86 	 Loss:  0.028779176995158195
Epoch:  87 	 Loss:  0.029095575213432312
Epoch:  88 	 Loss:  0.028882591053843498
Epoch:  89 	 Loss:  0.02911844477057457
Epoch:  90 	 Loss:  0.029107937589287758
Epoch:  91 	 Loss:  0.028963813558220863
Epoch:  92 	 Loss:  0.028918957337737083
Epoch:  93 	 Loss:  0.028965730220079422
Epoch:  94 	 Loss:  0.028763284906744957
Epoch:  95 	 Loss:  0.02953208051621914
Epoch:  96 	 Loss:  0.02930592931807041
Epoch:  97 	 Loss:  0.02861093170940876
Epoch:  98 	 Loss:  0.029527263715863228
Epoch:  99 	 Loss:  0.028775664046406746
Epoch:  100 	 Loss:  0.029540523886680603
wandb: Waiting for W&B process to finish, PID 1792822
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152831-9r5egbc6/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152831-9r5egbc6/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02954
wandb:        _step 800000
wandb:     _runtime 76
wandb:   _timestamp 1612902587
wandb:         loss 0.02954
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–‡â–ˆâ–â–†â–„â–†â–‡â–†â–†â–†â–…â–‚â–ƒâ–‡â–„â–‡â–ƒâ–…â–ˆâ–†â–†â–†â–†â–…â–„â–„â–‚â–‚â–†â–‡â–‚â–†â–„â–…â–…â–„â–ƒâ–ƒâ–‡
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced scarlet-sweep-48: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/9r5egbc6
wandb: Agent Starting Run: sy6w8161 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: adam
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:29:53.456010: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:29:53.461667: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run pious-sweep-49
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/sy6w8161
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_152951-sy6w8161
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.026867778971791267
Epoch:  2 	 Loss:  0.02705564722418785
Epoch:  3 	 Loss:  0.026463733986020088
Epoch:  4 	 Loss:  0.02708258107304573
Epoch:  5 	 Loss:  0.02681402862071991
Epoch:  6 	 Loss:  0.026554163545370102
Epoch:  7 	 Loss:  0.026808133348822594
Epoch:  8 	 Loss:  0.02648700587451458
Epoch:  9 	 Loss:  0.027204379439353943
Epoch:  10 	 Loss:  0.02663901261985302
Epoch:  11 	 Loss:  0.0266104806214571
Epoch:  12 	 Loss:  0.026798922568559647
Epoch:  13 	 Loss:  0.02707391418516636
Epoch:  14 	 Loss:  0.02707435004413128
Epoch:  15 	 Loss:  0.026485713198781013
Epoch:  16 	 Loss:  0.0268003698438406
Epoch:  17 	 Loss:  0.02648192085325718
Epoch:  18 	 Loss:  0.02679525502026081
Epoch:  19 	 Loss:  0.026901954784989357
Epoch:  20 	 Loss:  0.026769069954752922
Epoch:  21 	 Loss:  0.027046486735343933
Epoch:  22 	 Loss:  0.02642124332487583
Epoch:  23 	 Loss:  0.026877576485276222
Epoch:  24 	 Loss:  0.02686920575797558
Epoch:  25 	 Loss:  0.026825346052646637
Epoch:  26 	 Loss:  0.026796115562319756
Epoch:  27 	 Loss:  0.026780370622873306
Epoch:  28 	 Loss:  0.027018524706363678
Epoch:  29 	 Loss:  0.026407625526189804
Epoch:  30 	 Loss:  0.026495525613427162
Epoch:  31 	 Loss:  0.02693394012749195
Epoch:  32 	 Loss:  0.026499954983592033
Epoch:  33 	 Loss:  0.027006221935153008
Epoch:  34 	 Loss:  0.026404041796922684
Epoch:  35 	 Loss:  0.026852644979953766
Epoch:  36 	 Loss:  0.02636745758354664
Epoch:  37 	 Loss:  0.026718534529209137
Epoch:  38 	 Loss:  0.026923423632979393
Epoch:  39 	 Loss:  0.027136903256177902
Epoch:  40 	 Loss:  0.026656951755285263
Epoch:  41 	 Loss:  0.026673633605241776
Epoch:  42 	 Loss:  0.02703734114766121
Epoch:  43 	 Loss:  0.026507817208766937
Epoch:  44 	 Loss:  0.026786331087350845
Epoch:  45 	 Loss:  0.02636730670928955
Epoch:  46 	 Loss:  0.026655200868844986
Epoch:  47 	 Loss:  0.027064869180321693
Epoch:  48 	 Loss:  0.02725745178759098
Epoch:  49 	 Loss:  0.02673986740410328
Epoch:  50 	 Loss:  0.026694035157561302
Epoch:  51 	 Loss:  0.026767408475279808
Epoch:  52 	 Loss:  0.026804974302649498
Epoch:  53 	 Loss:  0.026722559705376625
Epoch:  54 	 Loss:  0.02672632783651352
Epoch:  55 	 Loss:  0.0269083920866251
Epoch:  56 	 Loss:  0.026859916746616364
Epoch:  57 	 Loss:  0.027071470394730568
Epoch:  58 	 Loss:  0.026454614475369453
Epoch:  59 	 Loss:  0.027022095397114754
Epoch:  60 	 Loss:  0.026473911479115486
Epoch:  61 	 Loss:  0.026935793459415436
Epoch:  62 	 Loss:  0.026653986424207687
Epoch:  63 	 Loss:  0.026300687342882156
Epoch:  64 	 Loss:  0.02675999514758587
Epoch:  65 	 Loss:  0.026462523266673088
Epoch:  66 	 Loss:  0.0267123244702816
Epoch:  67 	 Loss:  0.02650890313088894
Epoch:  68 	 Loss:  0.026747992262244225
Epoch:  69 	 Loss:  0.02656771056354046
Epoch:  70 	 Loss:  0.02662932500243187
Epoch:  71 	 Loss:  0.02694052830338478
Epoch:  72 	 Loss:  0.026806386187672615
Epoch:  73 	 Loss:  0.026474053040146828
Epoch:  74 	 Loss:  0.026396654546260834
Epoch:  75 	 Loss:  0.02600712515413761
Epoch:  76 	 Loss:  0.026949124410748482
Epoch:  77 	 Loss:  0.02692670188844204
Epoch:  78 	 Loss:  0.02659684047102928
Epoch:  79 	 Loss:  0.026990460231900215
Epoch:  80 	 Loss:  0.026819691061973572
Epoch:  81 	 Loss:  0.026566794142127037
Epoch:  82 	 Loss:  0.027072196826338768
Epoch:  83 	 Loss:  0.026598719879984856
Epoch:  84 	 Loss:  0.026949424296617508
Epoch:  85 	 Loss:  0.026832345873117447
Epoch:  86 	 Loss:  0.026462605223059654
Epoch:  87 	 Loss:  0.02683180198073387
Epoch:  88 	 Loss:  0.027111031115055084
Epoch:  89 	 Loss:  0.02649085223674774
Epoch:  90 	 Loss:  0.026808522641658783
Epoch:  91 	 Loss:  0.02683071419596672
Epoch:  92 	 Loss:  0.026399418711662292
Epoch:  93 	 Loss:  0.027201112359762192
Epoch:  94 	 Loss:  0.026690244674682617
Epoch:  95 	 Loss:  0.026590397581458092
Epoch:  96 	 Loss:  0.026586230844259262
Epoch:  97 	 Loss:  0.026954971253871918
Epoch:  98 	 Loss:  0.026729147881269455
Epoch:  99 	 Loss:  0.026901965960860252
Epoch:  100 	 Loss:  0.026717519387602806
wandb: Waiting for W&B process to finish, PID 1796503
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152951-sy6w8161/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_152951-sy6w8161/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02672
wandb:        _step 800000
wandb:     _runtime 77
wandb:   _timestamp 1612902668
wandb:         loss 0.02672
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–‚â–ƒâ–‚â–ƒâ–‡â–…â–…â–‡â–†â–…â–‡â–†â–â–â–ˆâ–„â–…â–„â–„â–…â–„â–…â–‡â–†â–…â–‚â–ƒâ–…â–â–†â–‡â–‡â–†â–…â–‚â–â–„â–†â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced pious-sweep-49: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/sy6w8161
wandb: Agent Starting Run: 9zbznvcw with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:31:14.307470: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:31:14.312763: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run swift-sweep-50
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/9zbznvcw
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_153112-9zbznvcw
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.026773888617753983
Epoch:  2 	 Loss:  0.026313304901123047
Epoch:  3 	 Loss:  0.026787757873535156
Epoch:  4 	 Loss:  0.02650400809943676
Epoch:  5 	 Loss:  0.027182547375559807
Epoch:  6 	 Loss:  0.026751970872282982
Epoch:  7 	 Loss:  0.026949839666485786
Epoch:  8 	 Loss:  0.02691704034805298
Epoch:  9 	 Loss:  0.026478078216314316
Epoch:  10 	 Loss:  0.026918932795524597
Epoch:  11 	 Loss:  0.026398176327347755
Epoch:  12 	 Loss:  0.026302432641386986
Epoch:  13 	 Loss:  0.02659575268626213
Epoch:  14 	 Loss:  0.026609255000948906
Epoch:  15 	 Loss:  0.026745837181806564
Epoch:  16 	 Loss:  0.02673330530524254
Epoch:  17 	 Loss:  0.02669735997915268
Epoch:  18 	 Loss:  0.02639232575893402
Epoch:  19 	 Loss:  0.026716621592640877
Epoch:  20 	 Loss:  0.026920713484287262
Epoch:  21 	 Loss:  0.026691148057579994
Epoch:  22 	 Loss:  0.026891065761446953
Epoch:  23 	 Loss:  0.026648947969079018
Epoch:  24 	 Loss:  0.026483379304409027
Epoch:  25 	 Loss:  0.02650078758597374
Epoch:  26 	 Loss:  0.026546066626906395
Epoch:  27 	 Loss:  0.026228537783026695
Epoch:  28 	 Loss:  0.026669850572943687
Epoch:  29 	 Loss:  0.026781227439641953
Epoch:  30 	 Loss:  0.026638401672244072
Epoch:  31 	 Loss:  0.026981329545378685
Epoch:  32 	 Loss:  0.02660689316689968
Epoch:  33 	 Loss:  0.026214079931378365
Epoch:  34 	 Loss:  0.02645084448158741
Epoch:  35 	 Loss:  0.026392942294478416
Epoch:  36 	 Loss:  0.02655671536922455
Epoch:  37 	 Loss:  0.026940282434225082
Epoch:  38 	 Loss:  0.026555974036455154
Epoch:  39 	 Loss:  0.026587888598442078
Epoch:  40 	 Loss:  0.02694961242377758
Epoch:  41 	 Loss:  0.027030782774090767
Epoch:  42 	 Loss:  0.02689577080309391
Epoch:  43 	 Loss:  0.026905875653028488
Epoch:  44 	 Loss:  0.026349809020757675
Epoch:  45 	 Loss:  0.026599539443850517
Epoch:  46 	 Loss:  0.026588866487145424
Epoch:  47 	 Loss:  0.02653810940682888
Epoch:  48 	 Loss:  0.026609787717461586
Epoch:  49 	 Loss:  0.026708360761404037
Epoch:  50 	 Loss:  0.026872290298342705
Epoch:  51 	 Loss:  0.026550155133008957
Epoch:  52 	 Loss:  0.026585638523101807
Epoch:  53 	 Loss:  0.026299191638827324
Epoch:  54 	 Loss:  0.026613162830471992
Epoch:  55 	 Loss:  0.026760637760162354
Epoch:  56 	 Loss:  0.02664523385465145
Epoch:  57 	 Loss:  0.026447296142578125
Epoch:  58 	 Loss:  0.026699062436819077
Epoch:  59 	 Loss:  0.026732228696346283
Epoch:  60 	 Loss:  0.02627766877412796
Epoch:  61 	 Loss:  0.02683941274881363
Epoch:  62 	 Loss:  0.026400422677397728
Epoch:  63 	 Loss:  0.026499412953853607
Epoch:  64 	 Loss:  0.02651752345263958
Epoch:  65 	 Loss:  0.026560580357909203
Epoch:  66 	 Loss:  0.026646407321095467
Epoch:  67 	 Loss:  0.026499276980757713
Epoch:  68 	 Loss:  0.026639694347977638
Epoch:  69 	 Loss:  0.02656688541173935
Epoch:  70 	 Loss:  0.026793545112013817
Epoch:  71 	 Loss:  0.027100112289190292
Epoch:  72 	 Loss:  0.026481546461582184
Epoch:  73 	 Loss:  0.026491917669773102
Epoch:  74 	 Loss:  0.02643803507089615
Epoch:  75 	 Loss:  0.026797300204634666
Epoch:  76 	 Loss:  0.0265245009213686
Epoch:  77 	 Loss:  0.026820635423064232
Epoch:  78 	 Loss:  0.026710575446486473
Epoch:  79 	 Loss:  0.026628689840435982
Epoch:  80 	 Loss:  0.026201987639069557
Epoch:  81 	 Loss:  0.026252467185258865
Epoch:  82 	 Loss:  0.026926394551992416
Epoch:  83 	 Loss:  0.026715341955423355
Epoch:  84 	 Loss:  0.026859654113650322
Epoch:  85 	 Loss:  0.02626843936741352
Epoch:  86 	 Loss:  0.026397626847028732
Epoch:  87 	 Loss:  0.026644514873623848
Epoch:  88 	 Loss:  0.026532236486673355
Epoch:  89 	 Loss:  0.026997094973921776
Epoch:  90 	 Loss:  0.0267782025039196
Epoch:  91 	 Loss:  0.026529714465141296
Epoch:  92 	 Loss:  0.026857413351535797
Epoch:  93 	 Loss:  0.02671404555439949
Epoch:  94 	 Loss:  0.02692222036421299
Epoch:  95 	 Loss:  0.026661472395062447
Epoch:  96 	 Loss:  0.026606803759932518
Epoch:  97 	 Loss:  0.02650354988873005
Epoch:  98 	 Loss:  0.02672935090959072
Epoch:  99 	 Loss:  0.026972251012921333
Epoch:  100 	 Loss:  0.026597289368510246
wandb: Waiting for W&B process to finish, PID 1800199
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153112-9zbznvcw/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153112-9zbznvcw/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0266
wandb:        _step 800000
wandb:     _runtime 85
wandb:   _timestamp 1612902757
wandb:         loss 0.0266
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–†â–…â–‡â–â–„â–…â–â–…â–„â–ƒâ–„â–‡â–‚â–ƒâ–ƒâ–ˆâ–â–ƒâ–…â–ƒâ–„â–„â–…â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–†â–„â–‡â–†â–„â–ˆâ–†â–‡â–ƒâ–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced swift-sweep-50: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/9zbznvcw
wandb: Agent Starting Run: ot1ttmxa with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:32:44.075717: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:32:44.083069: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run autumn-sweep-51
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/ot1ttmxa
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_153242-ot1ttmxa
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02625299245119095
Epoch:  2 	 Loss:  0.026182223111391068
Epoch:  3 	 Loss:  0.026174776256084442
Epoch:  4 	 Loss:  0.02655649185180664
Epoch:  5 	 Loss:  0.02617194503545761
Epoch:  6 	 Loss:  0.02657855488359928
Epoch:  7 	 Loss:  0.02629980258643627
Epoch:  8 	 Loss:  0.026032617315649986
Epoch:  9 	 Loss:  0.02668127603828907
Epoch:  10 	 Loss:  0.026566894724965096
Epoch:  11 	 Loss:  0.026755359023809433
Epoch:  12 	 Loss:  0.02678575925529003
Epoch:  13 	 Loss:  0.026222294196486473
Epoch:  14 	 Loss:  0.026407521218061447
Epoch:  15 	 Loss:  0.02676783688366413
Epoch:  16 	 Loss:  0.026229673996567726
Epoch:  17 	 Loss:  0.026642121374607086
Epoch:  18 	 Loss:  0.026362909004092216
Epoch:  19 	 Loss:  0.02669951505959034
Epoch:  20 	 Loss:  0.026197977364063263
Epoch:  21 	 Loss:  0.026030873879790306
Epoch:  22 	 Loss:  0.02625909447669983
Epoch:  23 	 Loss:  0.026345152407884598
Epoch:  24 	 Loss:  0.02680189721286297
Epoch:  25 	 Loss:  0.02674694173038006
Epoch:  26 	 Loss:  0.02663641795516014
Epoch:  27 	 Loss:  0.026295248419046402
Epoch:  28 	 Loss:  0.026247892528772354
Epoch:  29 	 Loss:  0.02660531736910343
Epoch:  30 	 Loss:  0.026624592021107674
Epoch:  31 	 Loss:  0.026320727542042732
Epoch:  32 	 Loss:  0.026555772870779037
Epoch:  33 	 Loss:  0.026667620986700058
Epoch:  34 	 Loss:  0.026482384651899338
Epoch:  35 	 Loss:  0.02634519711136818
Epoch:  36 	 Loss:  0.026522265747189522
Epoch:  37 	 Loss:  0.026548316702246666
Epoch:  38 	 Loss:  0.0263214148581028
Epoch:  39 	 Loss:  0.026758607476949692
Epoch:  40 	 Loss:  0.02660280466079712
Epoch:  41 	 Loss:  0.02669169381260872
Epoch:  42 	 Loss:  0.025997590273618698
Epoch:  43 	 Loss:  0.026371551677584648
Epoch:  44 	 Loss:  0.02619554102420807
Epoch:  45 	 Loss:  0.02648637816309929
Epoch:  46 	 Loss:  0.02632775716483593
Epoch:  47 	 Loss:  0.026167279109358788
Epoch:  48 	 Loss:  0.026470234617590904
Epoch:  49 	 Loss:  0.02623625658452511
Epoch:  50 	 Loss:  0.026387076824903488
Epoch:  51 	 Loss:  0.026886874809861183
Epoch:  52 	 Loss:  0.026503540575504303
Epoch:  53 	 Loss:  0.026697993278503418
Epoch:  54 	 Loss:  0.026380492374300957
Epoch:  55 	 Loss:  0.026531366631388664
Epoch:  56 	 Loss:  0.026621606200933456
Epoch:  57 	 Loss:  0.026110611855983734
Epoch:  58 	 Loss:  0.026321599259972572
Epoch:  59 	 Loss:  0.026765841990709305
Epoch:  60 	 Loss:  0.026490362361073494
Epoch:  61 	 Loss:  0.026769403368234634
Epoch:  62 	 Loss:  0.026384422555565834
Epoch:  63 	 Loss:  0.026500200852751732
Epoch:  64 	 Loss:  0.02658558450639248
Epoch:  65 	 Loss:  0.02659011445939541
Epoch:  66 	 Loss:  0.026459455490112305
Epoch:  67 	 Loss:  0.02653823420405388
Epoch:  68 	 Loss:  0.026427695527672768
Epoch:  69 	 Loss:  0.026730788871645927
Epoch:  70 	 Loss:  0.02663671411573887
Epoch:  71 	 Loss:  0.026418035849928856
Epoch:  72 	 Loss:  0.02629512920975685
Epoch:  73 	 Loss:  0.02632426843047142
Epoch:  74 	 Loss:  0.026360731571912766
Epoch:  75 	 Loss:  0.026744117960333824
Epoch:  76 	 Loss:  0.026355523616075516
Epoch:  77 	 Loss:  0.02667825110256672
Epoch:  78 	 Loss:  0.026329131796956062
Epoch:  79 	 Loss:  0.026286518201231956
Epoch:  80 	 Loss:  0.025981159880757332
Epoch:  81 	 Loss:  0.02654108591377735
Epoch:  82 	 Loss:  0.026596710085868835
Epoch:  83 	 Loss:  0.026434745639562607
Epoch:  84 	 Loss:  0.02636968530714512
Epoch:  85 	 Loss:  0.026421230286359787
Epoch:  86 	 Loss:  0.026306243613362312
Epoch:  87 	 Loss:  0.02649465762078762
Epoch:  88 	 Loss:  0.02628161571919918
Epoch:  89 	 Loss:  0.026477552950382233
Epoch:  90 	 Loss:  0.026237323880195618
Epoch:  91 	 Loss:  0.026652712374925613
Epoch:  92 	 Loss:  0.026903316378593445
Epoch:  93 	 Loss:  0.02633938379585743
Epoch:  94 	 Loss:  0.02646048180758953
Epoch:  95 	 Loss:  0.026559704914689064
Epoch:  96 	 Loss:  0.026357168331742287
Epoch:  97 	 Loss:  0.02646520920097828
Epoch:  98 	 Loss:  0.026512155309319496
Epoch:  99 	 Loss:  0.02647980861365795
Epoch:  100 	 Loss:  0.02662322111427784
wandb: Waiting for W&B process to finish, PID 1803914
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153242-ot1ttmxa/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153242-ot1ttmxa/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02662
wandb:        _step 800000
wandb:     _runtime 77
wandb:   _timestamp 1612902839
wandb:         loss 0.02662
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–ƒâ–‚â–…â–â–‡â–ƒâ–ƒâ–„â–â–„â–†â–ƒâ–ƒâ–…â–…â–‡â–†â–‚â–ƒâ–ƒâ–ˆâ–„â–†â–‡â–‡â–…â–…â–‡â–ƒâ–„â–†â–ƒâ–†â–„â–…â–…â–ˆâ–„â–„â–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced autumn-sweep-51: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/ot1ttmxa
wandb: Agent Starting Run: abqp8ch3 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:34:06.000950: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:34:06.006437: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run polar-sweep-52
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/abqp8ch3
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_153404-abqp8ch3
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02722722664475441
Epoch:  2 	 Loss:  0.026468710973858833
Epoch:  3 	 Loss:  0.026834405958652496
Epoch:  4 	 Loss:  0.026697473600506783
Epoch:  5 	 Loss:  0.026594651862978935
Epoch:  6 	 Loss:  0.026567954570055008
Epoch:  7 	 Loss:  0.026813620701432228
Epoch:  8 	 Loss:  0.02684573456645012
Epoch:  9 	 Loss:  0.027099164202809334
Epoch:  10 	 Loss:  0.027034414932131767
Epoch:  11 	 Loss:  0.027050651609897614
Epoch:  12 	 Loss:  0.02709999494254589
Epoch:  13 	 Loss:  0.027033040300011635
Epoch:  14 	 Loss:  0.027166975662112236
Epoch:  15 	 Loss:  0.02675999142229557
Epoch:  16 	 Loss:  0.026954902336001396
Epoch:  17 	 Loss:  0.026677267625927925
Epoch:  18 	 Loss:  0.02626563049852848
Epoch:  19 	 Loss:  0.026984242722392082
Epoch:  20 	 Loss:  0.02726266346871853
Epoch:  21 	 Loss:  0.026712719351053238
Epoch:  22 	 Loss:  0.027028542011976242
Epoch:  23 	 Loss:  0.02667599357664585
Epoch:  24 	 Loss:  0.02706899866461754
Epoch:  25 	 Loss:  0.02683650143444538
Epoch:  26 	 Loss:  0.027021577581763268
Epoch:  27 	 Loss:  0.02712135575711727
Epoch:  28 	 Loss:  0.027198970317840576
Epoch:  29 	 Loss:  0.026726653799414635
Epoch:  30 	 Loss:  0.02692132256925106
Epoch:  31 	 Loss:  0.026780830696225166
Epoch:  32 	 Loss:  0.026966791599988937
Epoch:  33 	 Loss:  0.026744475588202477
Epoch:  34 	 Loss:  0.02679128758609295
Epoch:  35 	 Loss:  0.02709321863949299
Epoch:  36 	 Loss:  0.026786433532834053
Epoch:  37 	 Loss:  0.02684895694255829
Epoch:  38 	 Loss:  0.026733605191111565
Epoch:  39 	 Loss:  0.026882478967308998
Epoch:  40 	 Loss:  0.027206476777791977
Epoch:  41 	 Loss:  0.02705291658639908
Epoch:  42 	 Loss:  0.026576625183224678
Epoch:  43 	 Loss:  0.026955673471093178
Epoch:  44 	 Loss:  0.026589220389723778
Epoch:  45 	 Loss:  0.02699948661029339
Epoch:  46 	 Loss:  0.026715831831097603
Epoch:  47 	 Loss:  0.026592932641506195
Epoch:  48 	 Loss:  0.02681231126189232
Epoch:  49 	 Loss:  0.027391424402594566
Epoch:  50 	 Loss:  0.027283966541290283
Epoch:  51 	 Loss:  0.026832567527890205
Epoch:  52 	 Loss:  0.027071520686149597
Epoch:  53 	 Loss:  0.026802292093634605
Epoch:  54 	 Loss:  0.02733800560235977
Epoch:  55 	 Loss:  0.027073588222265244
Epoch:  56 	 Loss:  0.027222169563174248
Epoch:  57 	 Loss:  0.02709929645061493
Epoch:  58 	 Loss:  0.0271445382386446
Epoch:  59 	 Loss:  0.026404889300465584
Epoch:  60 	 Loss:  0.026563098654150963
Epoch:  61 	 Loss:  0.026779701933264732
Epoch:  62 	 Loss:  0.026984581723809242
Epoch:  63 	 Loss:  0.02686307020485401
Epoch:  64 	 Loss:  0.027005579322576523
Epoch:  65 	 Loss:  0.027653373777866364
Epoch:  66 	 Loss:  0.02695707231760025
Epoch:  67 	 Loss:  0.026835765689611435
Epoch:  68 	 Loss:  0.026839883998036385
Epoch:  69 	 Loss:  0.027374666184186935
Epoch:  70 	 Loss:  0.026983138173818588
Epoch:  71 	 Loss:  0.027151621878147125
Epoch:  72 	 Loss:  0.027158554643392563
Epoch:  73 	 Loss:  0.02685954049229622
Epoch:  74 	 Loss:  0.02662673220038414
Epoch:  75 	 Loss:  0.026842130348086357
Epoch:  76 	 Loss:  0.02704519033432007
Epoch:  77 	 Loss:  0.027165193110704422
Epoch:  78 	 Loss:  0.027460431680083275
Epoch:  79 	 Loss:  0.027154188603162766
Epoch:  80 	 Loss:  0.027056630700826645
Epoch:  81 	 Loss:  0.026872798800468445
Epoch:  82 	 Loss:  0.02693263813853264
Epoch:  83 	 Loss:  0.026604074984788895
Epoch:  84 	 Loss:  0.027141759172081947
Epoch:  85 	 Loss:  0.026774682104587555
Epoch:  86 	 Loss:  0.026449844241142273
Epoch:  87 	 Loss:  0.02717556431889534
Epoch:  88 	 Loss:  0.027169736102223396
Epoch:  89 	 Loss:  0.02661188691854477
Epoch:  90 	 Loss:  0.027327165007591248
Epoch:  91 	 Loss:  0.02720666490495205
Epoch:  92 	 Loss:  0.026755044236779213
Epoch:  93 	 Loss:  0.02688923105597496
Epoch:  94 	 Loss:  0.027057912200689316
Epoch:  95 	 Loss:  0.0271324273198843
Epoch:  96 	 Loss:  0.02695322409272194
Epoch:  97 	 Loss:  0.02677125111222267
Epoch:  98 	 Loss:  0.02698158659040928
Epoch:  99 	 Loss:  0.02663472853600979
Epoch:  100 	 Loss:  0.027402890846133232
wandb: Waiting for W&B process to finish, PID 1807602
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153404-abqp8ch3/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153404-abqp8ch3/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0274
wandb:        _step 800000
wandb:     _runtime 76
wandb:   _timestamp 1612902920
wandb:         loss 0.0274
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–‡â–…â–ƒâ–…â–†â–†â–…â–â–„â–„â–†â–‡â–„â–„â–„â–…â–†â–ƒâ–„â–ˆâ–„â–ˆâ–‡â–‚â–„â–†â–…â–ˆâ–†â–ƒâ–‡â–†â–…â–†â–‡â–ƒâ–„â–†â–„â–ˆ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced polar-sweep-52: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/abqp8ch3
wandb: Agent Starting Run: d0qtz8nj with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:35:26.860279: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:35:26.867394: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run soft-sweep-53
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/d0qtz8nj
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_153524-d0qtz8nj
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02685205638408661
Epoch:  2 	 Loss:  0.027043525129556656
Epoch:  3 	 Loss:  0.026612062007188797
Epoch:  4 	 Loss:  0.026774337515234947
Epoch:  5 	 Loss:  0.0269259475171566
Epoch:  6 	 Loss:  0.026662593707442284
Epoch:  7 	 Loss:  0.027006901800632477
Epoch:  8 	 Loss:  0.026714947074651718
Epoch:  9 	 Loss:  0.0265643373131752
Epoch:  10 	 Loss:  0.026848511770367622
Epoch:  11 	 Loss:  0.02626759186387062
Epoch:  12 	 Loss:  0.026767922565340996
Epoch:  13 	 Loss:  0.02640044130384922
Epoch:  14 	 Loss:  0.026713553816080093
Epoch:  15 	 Loss:  0.026406247168779373
Epoch:  16 	 Loss:  0.026725273579359055
Epoch:  17 	 Loss:  0.026636119931936264
Epoch:  18 	 Loss:  0.026778340339660645
Epoch:  19 	 Loss:  0.026502195745706558
Epoch:  20 	 Loss:  0.02695327438414097
Epoch:  21 	 Loss:  0.026680538430809975
Epoch:  22 	 Loss:  0.026925669983029366
Epoch:  23 	 Loss:  0.026739582419395447
Epoch:  24 	 Loss:  0.026914265006780624
Epoch:  25 	 Loss:  0.02684466540813446
Epoch:  26 	 Loss:  0.026692824438214302
Epoch:  27 	 Loss:  0.026994429528713226
Epoch:  28 	 Loss:  0.02659437619149685
Epoch:  29 	 Loss:  0.02658565156161785
Epoch:  30 	 Loss:  0.026884155347943306
Epoch:  31 	 Loss:  0.02669569104909897
Epoch:  32 	 Loss:  0.02679254300892353
Epoch:  33 	 Loss:  0.0261598601937294
Epoch:  34 	 Loss:  0.026673229411244392
Epoch:  35 	 Loss:  0.026682747527956963
Epoch:  36 	 Loss:  0.02657870016992092
Epoch:  37 	 Loss:  0.026499008759856224
Epoch:  38 	 Loss:  0.026763424277305603
Epoch:  39 	 Loss:  0.02662639319896698
Epoch:  40 	 Loss:  0.02656479924917221
Epoch:  41 	 Loss:  0.02700044959783554
Epoch:  42 	 Loss:  0.026934808120131493
Epoch:  43 	 Loss:  0.02676429972052574
Epoch:  44 	 Loss:  0.026824085041880608
Epoch:  45 	 Loss:  0.026539331302046776
Epoch:  46 	 Loss:  0.026975231245160103
Epoch:  47 	 Loss:  0.027044590562582016
Epoch:  48 	 Loss:  0.026550110429525375
Epoch:  49 	 Loss:  0.02655760943889618
Epoch:  50 	 Loss:  0.02647601068019867
Epoch:  51 	 Loss:  0.02705446444451809
Epoch:  52 	 Loss:  0.026799486950039864
Epoch:  53 	 Loss:  0.02658364735543728
Epoch:  54 	 Loss:  0.026453325524926186
Epoch:  55 	 Loss:  0.026921909302473068
Epoch:  56 	 Loss:  0.02663886547088623
Epoch:  57 	 Loss:  0.02668715827167034
Epoch:  58 	 Loss:  0.026817595586180687
Epoch:  59 	 Loss:  0.026400621980428696
Epoch:  60 	 Loss:  0.026501361280679703
Epoch:  61 	 Loss:  0.026813432574272156
Epoch:  62 	 Loss:  0.026670951396226883
Epoch:  63 	 Loss:  0.026728469878435135
Epoch:  64 	 Loss:  0.026469064876437187
Epoch:  65 	 Loss:  0.026921266689896584
Epoch:  66 	 Loss:  0.026666127145290375
Epoch:  67 	 Loss:  0.02688458189368248
Epoch:  68 	 Loss:  0.02659333311021328
Epoch:  69 	 Loss:  0.026893697679042816
Epoch:  70 	 Loss:  0.026775317266583443
Epoch:  71 	 Loss:  0.026660041883587837
Epoch:  72 	 Loss:  0.026946989819407463
Epoch:  73 	 Loss:  0.026664040982723236
Epoch:  74 	 Loss:  0.02671164833009243
Epoch:  75 	 Loss:  0.026769375428557396
Epoch:  76 	 Loss:  0.02662806212902069
Epoch:  77 	 Loss:  0.027075208723545074
Epoch:  78 	 Loss:  0.026867564767599106
Epoch:  79 	 Loss:  0.026731720194220543
Epoch:  80 	 Loss:  0.02675117924809456
Epoch:  81 	 Loss:  0.02689480222761631
Epoch:  82 	 Loss:  0.026698915287852287
Epoch:  83 	 Loss:  0.026186760514974594
Epoch:  84 	 Loss:  0.026617756113409996
Epoch:  85 	 Loss:  0.026860130950808525
Epoch:  86 	 Loss:  0.02687946707010269
Epoch:  87 	 Loss:  0.02665875479578972
Epoch:  88 	 Loss:  0.02662876807153225
Epoch:  89 	 Loss:  0.0270409993827343
Epoch:  90 	 Loss:  0.026485776528716087
Epoch:  91 	 Loss:  0.02693849429488182
Epoch:  92 	 Loss:  0.027053259313106537
Epoch:  93 	 Loss:  0.02659180946648121
Epoch:  94 	 Loss:  0.026803499087691307
Epoch:  95 	 Loss:  0.026660826057195663
Epoch:  96 	 Loss:  0.026921579614281654
Epoch:  97 	 Loss:  0.02684697136282921
Epoch:  98 	 Loss:  0.026831695809960365
Epoch:  99 	 Loss:  0.02676299400627613
Epoch:  100 	 Loss:  0.02665826492011547
wandb: Waiting for W&B process to finish, PID 1811287
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153524-d0qtz8nj/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153524-d0qtz8nj/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02666
wandb:        _step 800000
wandb:     _runtime 73
wandb:   _timestamp 1612902997
wandb:         loss 0.02666
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–„â–„â–…â–â–‚â–…â–…â–…â–…â–…â–„â–…â–…â–„â–„â–‡â–†â–‡â–„â–ˆâ–ƒâ–„â–‚â–†â–ƒâ–†â–†â–‡â–…â–ˆâ–…â–…â–„â–„â–ˆâ–ˆâ–†â–†â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced soft-sweep-53: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/d0qtz8nj
wandb: Agent Starting Run: 3kcjli64 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:36:43.923329: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:36:43.928549: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run dauntless-sweep-54
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/3kcjli64
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_153641-3kcjli64
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02714039571583271
Epoch:  2 	 Loss:  0.02685501053929329
Epoch:  3 	 Loss:  0.027130616828799248
Epoch:  4 	 Loss:  0.027185866609215736
Epoch:  5 	 Loss:  0.027310114353895187
Epoch:  6 	 Loss:  0.02732575498521328
Epoch:  7 	 Loss:  0.0271438118070364
Epoch:  8 	 Loss:  0.027646316215395927
Epoch:  9 	 Loss:  0.027011774480342865
Epoch:  10 	 Loss:  0.02720988169312477
Epoch:  11 	 Loss:  0.02714424952864647
Epoch:  12 	 Loss:  0.026946838945150375
Epoch:  13 	 Loss:  0.027452029287815094
Epoch:  14 	 Loss:  0.027309546247124672
Epoch:  15 	 Loss:  0.026952354237437248
Epoch:  16 	 Loss:  0.02723861299455166
Epoch:  17 	 Loss:  0.02718173898756504
Epoch:  18 	 Loss:  0.027614835649728775
Epoch:  19 	 Loss:  0.027556130662560463
Epoch:  20 	 Loss:  0.027433430776000023
Epoch:  21 	 Loss:  0.027065763249993324
Epoch:  22 	 Loss:  0.027187447994947433
Epoch:  23 	 Loss:  0.027257107198238373
Epoch:  24 	 Loss:  0.027586940675973892
Epoch:  25 	 Loss:  0.027459396049380302
Epoch:  26 	 Loss:  0.027372727170586586
Epoch:  27 	 Loss:  0.02694772370159626
Epoch:  28 	 Loss:  0.0274230744689703
Epoch:  29 	 Loss:  0.027719052508473396
Epoch:  30 	 Loss:  0.02768556773662567
Epoch:  31 	 Loss:  0.027156315743923187
Epoch:  32 	 Loss:  0.02732180617749691
Epoch:  33 	 Loss:  0.02729170024394989
Epoch:  34 	 Loss:  0.026962898671627045
Epoch:  35 	 Loss:  0.02725221961736679
Epoch:  36 	 Loss:  0.02710319496691227
Epoch:  37 	 Loss:  0.02704598195850849
Epoch:  38 	 Loss:  0.02692941576242447
Epoch:  39 	 Loss:  0.02715892158448696
Epoch:  40 	 Loss:  0.027235357090830803
Epoch:  41 	 Loss:  0.02716643549501896
Epoch:  42 	 Loss:  0.026757901534438133
Epoch:  43 	 Loss:  0.027643905952572823
Epoch:  44 	 Loss:  0.027298660948872566
Epoch:  45 	 Loss:  0.02702176570892334
Epoch:  46 	 Loss:  0.02714688517153263
Epoch:  47 	 Loss:  0.02731478586792946
Epoch:  48 	 Loss:  0.027138710021972656
Epoch:  49 	 Loss:  0.026776231825351715
Epoch:  50 	 Loss:  0.02668924815952778
Epoch:  51 	 Loss:  0.027383051812648773
Epoch:  52 	 Loss:  0.027038712054491043
Epoch:  53 	 Loss:  0.027030494064092636
Epoch:  54 	 Loss:  0.026866313070058823
Epoch:  55 	 Loss:  0.02688351832330227
Epoch:  56 	 Loss:  0.027176059782505035
Epoch:  57 	 Loss:  0.026768863201141357
Epoch:  58 	 Loss:  0.026319297030568123
Epoch:  59 	 Loss:  0.02720099315047264
Epoch:  60 	 Loss:  0.026930414140224457
Epoch:  61 	 Loss:  0.02707665227353573
Epoch:  62 	 Loss:  0.02699892409145832
Epoch:  63 	 Loss:  0.026781879365444183
Epoch:  64 	 Loss:  0.02748137339949608
Epoch:  65 	 Loss:  0.02719847857952118
Epoch:  66 	 Loss:  0.027079783380031586
Epoch:  67 	 Loss:  0.02693457342684269
Epoch:  68 	 Loss:  0.027138786390423775
Epoch:  69 	 Loss:  0.027300959452986717
Epoch:  70 	 Loss:  0.02722753956913948
Epoch:  71 	 Loss:  0.02713550440967083
Epoch:  72 	 Loss:  0.027178531512618065
Epoch:  73 	 Loss:  0.02697048708796501
Epoch:  74 	 Loss:  0.02747996896505356
Epoch:  75 	 Loss:  0.026552878320217133
Epoch:  76 	 Loss:  0.027010058984160423
Epoch:  77 	 Loss:  0.02718208357691765
Epoch:  78 	 Loss:  0.027306880801916122
Epoch:  79 	 Loss:  0.0271884985268116
Epoch:  80 	 Loss:  0.027667196467518806
Epoch:  81 	 Loss:  0.027406003326177597
Epoch:  82 	 Loss:  0.02701900526881218
Epoch:  83 	 Loss:  0.027033772319555283
Epoch:  84 	 Loss:  0.026962794363498688
Epoch:  85 	 Loss:  0.02737586200237274
Epoch:  86 	 Loss:  0.026780705899000168
Epoch:  87 	 Loss:  0.02734517492353916
Epoch:  88 	 Loss:  0.0272689089179039
Epoch:  89 	 Loss:  0.027418242767453194
Epoch:  90 	 Loss:  0.027320701628923416
Epoch:  91 	 Loss:  0.027402209118008614
Epoch:  92 	 Loss:  0.02694336324930191
Epoch:  93 	 Loss:  0.02754293568432331
Epoch:  94 	 Loss:  0.026896879076957703
Epoch:  95 	 Loss:  0.027756713330745697
Epoch:  96 	 Loss:  0.02773316577076912
Epoch:  97 	 Loss:  0.026840589940547943
Epoch:  98 	 Loss:  0.027174321934580803
Epoch:  99 	 Loss:  0.026535799726843834
Epoch:  100 	 Loss:  0.026945339515805244
wandb: Waiting for W&B process to finish, PID 1814963
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153641-3kcjli64/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153641-3kcjli64/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02695
wandb:        _step 800000
wandb:     _runtime 71
wandb:   _timestamp 1612903073
wandb:         loss 0.02695
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–„â–…â–ˆâ–„â–†â–…â–ˆâ–ƒâ–…â–†â–†â–„â–ƒâ–„â–„â–„â–…â–„â–â–†â–‚â–„â–„â–ƒâ–‡â–‚â–…â–„â–‡â–„â–„â–ƒâ–ƒâ–†â–†â–‚â–‚â–‚â–‚
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced dauntless-sweep-54: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/3kcjli64
wandb: Agent Starting Run: 9vxslkc4 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:38:01.854417: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:38:01.859767: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run kind-sweep-55
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/9vxslkc4
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_153759-9vxslkc4
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02709825150668621
Epoch:  2 	 Loss:  0.026911811903119087
Epoch:  3 	 Loss:  0.02676747366786003
Epoch:  4 	 Loss:  0.026805156841874123
Epoch:  5 	 Loss:  0.026743458583950996
Epoch:  6 	 Loss:  0.027242660522460938
Epoch:  7 	 Loss:  0.026643119752407074
Epoch:  8 	 Loss:  0.026775768026709557
Epoch:  9 	 Loss:  0.026809636503458023
Epoch:  10 	 Loss:  0.027218731120228767
Epoch:  11 	 Loss:  0.026929795742034912
Epoch:  12 	 Loss:  0.026624999940395355
Epoch:  13 	 Loss:  0.02700188383460045
Epoch:  14 	 Loss:  0.026986397802829742
Epoch:  15 	 Loss:  0.027054404839873314
Epoch:  16 	 Loss:  0.027001041918992996
Epoch:  17 	 Loss:  0.02701227366924286
Epoch:  18 	 Loss:  0.027092911303043365
Epoch:  19 	 Loss:  0.027237290516495705
Epoch:  20 	 Loss:  0.02655554749071598
Epoch:  21 	 Loss:  0.026709850877523422
Epoch:  22 	 Loss:  0.026971038430929184
Epoch:  23 	 Loss:  0.026969529688358307
Epoch:  24 	 Loss:  0.027126256376504898
Epoch:  25 	 Loss:  0.027452033013105392
Epoch:  26 	 Loss:  0.026796476915478706
Epoch:  27 	 Loss:  0.026627447456121445
Epoch:  28 	 Loss:  0.027068302035331726
Epoch:  29 	 Loss:  0.02714821882545948
Epoch:  30 	 Loss:  0.026364540681242943
Epoch:  31 	 Loss:  0.02720007114112377
Epoch:  32 	 Loss:  0.026928650215268135
Epoch:  33 	 Loss:  0.027088617905974388
Epoch:  34 	 Loss:  0.02718331292271614
Epoch:  35 	 Loss:  0.026855193078517914
Epoch:  36 	 Loss:  0.027072956785559654
Epoch:  37 	 Loss:  0.027516469359397888
Epoch:  38 	 Loss:  0.027009064331650734
Epoch:  39 	 Loss:  0.026823652908205986
Epoch:  40 	 Loss:  0.02691028080880642
Epoch:  41 	 Loss:  0.02655242569744587
Epoch:  42 	 Loss:  0.026778381317853928
Epoch:  43 	 Loss:  0.02707696333527565
Epoch:  44 	 Loss:  0.027321288362145424
Epoch:  45 	 Loss:  0.027030330151319504
Epoch:  46 	 Loss:  0.027185609564185143
Epoch:  47 	 Loss:  0.026952620595693588
Epoch:  48 	 Loss:  0.026976749300956726
Epoch:  49 	 Loss:  0.027187209576368332
Epoch:  50 	 Loss:  0.026897426694631577
Epoch:  51 	 Loss:  0.02671809308230877
Epoch:  52 	 Loss:  0.026838604360818863
Epoch:  53 	 Loss:  0.027284104377031326
Epoch:  54 	 Loss:  0.027095220983028412
Epoch:  55 	 Loss:  0.027144620195031166
Epoch:  56 	 Loss:  0.02655342034995556
Epoch:  57 	 Loss:  0.026982979848980904
Epoch:  58 	 Loss:  0.027165180072188377
Epoch:  59 	 Loss:  0.027259651571512222
Epoch:  60 	 Loss:  0.02707437239587307
Epoch:  61 	 Loss:  0.026887197047472
Epoch:  62 	 Loss:  0.027445664629340172
Epoch:  63 	 Loss:  0.026865188032388687
Epoch:  64 	 Loss:  0.027068691328167915
Epoch:  65 	 Loss:  0.0268614012748003
Epoch:  66 	 Loss:  0.027144676074385643
Epoch:  67 	 Loss:  0.026975149288773537
Epoch:  68 	 Loss:  0.026871006935834885
Epoch:  69 	 Loss:  0.027209872379899025
Epoch:  70 	 Loss:  0.02703757770359516
Epoch:  71 	 Loss:  0.02708851359784603
Epoch:  72 	 Loss:  0.02713214047253132
Epoch:  73 	 Loss:  0.027087757363915443
Epoch:  74 	 Loss:  0.026979899033904076
Epoch:  75 	 Loss:  0.027177779003977776
Epoch:  76 	 Loss:  0.026994507759809494
Epoch:  77 	 Loss:  0.026438003405928612
Epoch:  78 	 Loss:  0.02701803296804428
Epoch:  79 	 Loss:  0.02663039229810238
Epoch:  80 	 Loss:  0.02719944156706333
Epoch:  81 	 Loss:  0.026893923059105873
Epoch:  82 	 Loss:  0.0266040600836277
Epoch:  83 	 Loss:  0.027169691398739815
Epoch:  84 	 Loss:  0.027045583352446556
Epoch:  85 	 Loss:  0.02665388584136963
Epoch:  86 	 Loss:  0.02706155739724636
Epoch:  87 	 Loss:  0.0271366685628891
Epoch:  88 	 Loss:  0.027237987145781517
Epoch:  89 	 Loss:  0.026545854285359383
Epoch:  90 	 Loss:  0.026651235297322273
Epoch:  91 	 Loss:  0.02686024270951748
Epoch:  92 	 Loss:  0.027326835319399834
Epoch:  93 	 Loss:  0.02700359933078289
Epoch:  94 	 Loss:  0.027201790362596512
Epoch:  95 	 Loss:  0.027453115209937096
Epoch:  96 	 Loss:  0.02673432044684887
Epoch:  97 	 Loss:  0.027312617748975754
Epoch:  98 	 Loss:  0.02640128694474697
Epoch:  99 	 Loss:  0.027143854647874832
Epoch:  100 	 Loss:  0.027301784604787827
wandb: Waiting for W&B process to finish, PID 1818658
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153759-9vxslkc4/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153759-9vxslkc4/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0273
wandb:        _step 800000
wandb:     _runtime 71
wandb:   _timestamp 1612903151
wandb:         loss 0.0273
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–„â–‡â–„â–…â–…â–…â–†â–ƒâ–…â–„â–†â–‡â–‡â–†â–„â–‚â–ˆâ–‡â–‡â–ƒâ–†â–‚â–‡â–…â–†â–…â–‡â–†â–…â–â–ƒâ–‚â–†â–‡â–‚â–ˆâ–‡â–ˆâ–ˆ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced kind-sweep-55: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/9vxslkc4
wandb: Agent Starting Run: p9pyho9v with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:39:17.253216: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:39:17.258661: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run comfy-sweep-56
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/p9pyho9v
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_153915-p9pyho9v
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.028004376217722893
Epoch:  2 	 Loss:  0.027834845706820488
Epoch:  3 	 Loss:  0.027992907911539078
Epoch:  4 	 Loss:  0.027534710243344307
Epoch:  5 	 Loss:  0.027754275128245354
Epoch:  6 	 Loss:  0.02721216529607773
Epoch:  7 	 Loss:  0.02813066728413105
Epoch:  8 	 Loss:  0.02768072299659252
Epoch:  9 	 Loss:  0.02788718231022358
Epoch:  10 	 Loss:  0.027998868376016617
Epoch:  11 	 Loss:  0.028292452916502953
Epoch:  12 	 Loss:  0.02817726507782936
Epoch:  13 	 Loss:  0.028160741552710533
Epoch:  14 	 Loss:  0.027435118332505226
Epoch:  15 	 Loss:  0.027768727391958237
Epoch:  16 	 Loss:  0.028267836198210716
Epoch:  17 	 Loss:  0.027281541377305984
Epoch:  18 	 Loss:  0.027603965252637863
Epoch:  19 	 Loss:  0.028495866805315018
Epoch:  20 	 Loss:  0.027840210124850273
Epoch:  21 	 Loss:  0.02809290587902069
Epoch:  22 	 Loss:  0.02805403061211109
Epoch:  23 	 Loss:  0.02812940813601017
Epoch:  24 	 Loss:  0.027671368792653084
Epoch:  25 	 Loss:  0.027473757043480873
Epoch:  26 	 Loss:  0.028350627049803734
Epoch:  27 	 Loss:  0.028187494724988937
Epoch:  28 	 Loss:  0.028326699510216713
Epoch:  29 	 Loss:  0.028185831382870674
Epoch:  30 	 Loss:  0.027808042243123055
Epoch:  31 	 Loss:  0.027891485020518303
Epoch:  32 	 Loss:  0.02793974056839943
Epoch:  33 	 Loss:  0.0278591550886631
Epoch:  34 	 Loss:  0.02806013450026512
Epoch:  35 	 Loss:  0.027467643842101097
Epoch:  36 	 Loss:  0.02795550785958767
Epoch:  37 	 Loss:  0.02812471240758896
Epoch:  38 	 Loss:  0.027946127578616142
Epoch:  39 	 Loss:  0.0276934877038002
Epoch:  40 	 Loss:  0.027479955926537514
Epoch:  41 	 Loss:  0.028033869341015816
Epoch:  42 	 Loss:  0.027325768023729324
Epoch:  43 	 Loss:  0.028121741488575935
Epoch:  44 	 Loss:  0.027928030118346214
Epoch:  45 	 Loss:  0.02801142819225788
Epoch:  46 	 Loss:  0.027984289452433586
Epoch:  47 	 Loss:  0.027999397367239
Epoch:  48 	 Loss:  0.028273632749915123
Epoch:  49 	 Loss:  0.02812204323709011
Epoch:  50 	 Loss:  0.028406884521245956
Epoch:  51 	 Loss:  0.027411524206399918
Epoch:  52 	 Loss:  0.028450332581996918
Epoch:  53 	 Loss:  0.028133561834692955
Epoch:  54 	 Loss:  0.02812253125011921
Epoch:  55 	 Loss:  0.028140462934970856
Epoch:  56 	 Loss:  0.02776079811155796
Epoch:  57 	 Loss:  0.027931498363614082
Epoch:  58 	 Loss:  0.02745133265852928
Epoch:  59 	 Loss:  0.02760140225291252
Epoch:  60 	 Loss:  0.02839653752744198
Epoch:  61 	 Loss:  0.02811342477798462
Epoch:  62 	 Loss:  0.027890073135495186
Epoch:  63 	 Loss:  0.027662035077810287
Epoch:  64 	 Loss:  0.027998244389891624
Epoch:  65 	 Loss:  0.027498658746480942
Epoch:  66 	 Loss:  0.027440093457698822
Epoch:  67 	 Loss:  0.02782008796930313
Epoch:  68 	 Loss:  0.027395743876695633
Epoch:  69 	 Loss:  0.02810707874596119
Epoch:  70 	 Loss:  0.028399592265486717
Epoch:  71 	 Loss:  0.027832990512251854
Epoch:  72 	 Loss:  0.02777804620563984
Epoch:  73 	 Loss:  0.02817344106733799
Epoch:  74 	 Loss:  0.0277072973549366
Epoch:  75 	 Loss:  0.02822878211736679
Epoch:  76 	 Loss:  0.028290459886193275
Epoch:  77 	 Loss:  0.02775644324719906
Epoch:  78 	 Loss:  0.027879036962985992
Epoch:  79 	 Loss:  0.028107821941375732
Epoch:  80 	 Loss:  0.027982234954833984
Epoch:  81 	 Loss:  0.028245316818356514
Epoch:  82 	 Loss:  0.027840232476592064
Epoch:  83 	 Loss:  0.027630306780338287
Epoch:  84 	 Loss:  0.02752838283777237
Epoch:  85 	 Loss:  0.027468662708997726
Epoch:  86 	 Loss:  0.027082668617367744
Epoch:  87 	 Loss:  0.027735477313399315
Epoch:  88 	 Loss:  0.027729405090212822
Epoch:  89 	 Loss:  0.028308304026722908
Epoch:  90 	 Loss:  0.028146032243967056
Epoch:  91 	 Loss:  0.02786690928041935
Epoch:  92 	 Loss:  0.027282699942588806
Epoch:  93 	 Loss:  0.027372121810913086
Epoch:  94 	 Loss:  0.027395358309149742
Epoch:  95 	 Loss:  0.027646472677588463
Epoch:  96 	 Loss:  0.027953173965215683
Epoch:  97 	 Loss:  0.02766798622906208
Epoch:  98 	 Loss:  0.02792172133922577
Epoch:  99 	 Loss:  0.027817239984869957
Epoch:  100 	 Loss:  0.027736080810427666
wandb: Waiting for W&B process to finish, PID 1822339
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153915-p9pyho9v/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_153915-p9pyho9v/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02774
wandb:        _step 800000
wandb:     _runtime 71
wandb:   _timestamp 1612903226
wandb:         loss 0.02774
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–†â–â–„â–ˆâ–‡â–‡â–ƒâ–†â–‡â–ˆâ–ˆâ–…â–†â–†â–„â–†â–…â–†â–‡â–‚â–‡â–„â–ƒâ–‡â–†â–…â–‡â–„â–„â–„â–‡â–…â–ƒâ–„â–ˆâ–â–‚â–„â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced comfy-sweep-56: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/p9pyho9v
wandb: Agent Starting Run: cw3uffd3 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:40:32.889724: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:40:32.895320: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run zany-sweep-57
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/cw3uffd3
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_154030-cw3uffd3
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027697104960680008
Epoch:  2 	 Loss:  0.028020350262522697
Epoch:  3 	 Loss:  0.02814430370926857
Epoch:  4 	 Loss:  0.027983972802758217
Epoch:  5 	 Loss:  0.028416013345122337
Epoch:  6 	 Loss:  0.027800995856523514
Epoch:  7 	 Loss:  0.027697090059518814
Epoch:  8 	 Loss:  0.027707144618034363
Epoch:  9 	 Loss:  0.028219744563102722
Epoch:  10 	 Loss:  0.027685074135661125
Epoch:  11 	 Loss:  0.028155578300356865
Epoch:  12 	 Loss:  0.027543971315026283
Epoch:  13 	 Loss:  0.02780420519411564
Epoch:  14 	 Loss:  0.027898715808987617
Epoch:  15 	 Loss:  0.02781808376312256
Epoch:  16 	 Loss:  0.027247516438364983
Epoch:  17 	 Loss:  0.02711530774831772
Epoch:  18 	 Loss:  0.027600828558206558
Epoch:  19 	 Loss:  0.02760976180434227
Epoch:  20 	 Loss:  0.027601132169365883
Epoch:  21 	 Loss:  0.027981610968708992
Epoch:  22 	 Loss:  0.02806823141872883
Epoch:  23 	 Loss:  0.028225095942616463
Epoch:  24 	 Loss:  0.027838123962283134
Epoch:  25 	 Loss:  0.028418870642781258
Epoch:  26 	 Loss:  0.027973510324954987
Epoch:  27 	 Loss:  0.027924763038754463
Epoch:  28 	 Loss:  0.027578061446547508
Epoch:  29 	 Loss:  0.027659522369503975
Epoch:  30 	 Loss:  0.02772800251841545
Epoch:  31 	 Loss:  0.027850044891238213
Epoch:  32 	 Loss:  0.027745729312300682
Epoch:  33 	 Loss:  0.027606194838881493
Epoch:  34 	 Loss:  0.027943050488829613
Epoch:  35 	 Loss:  0.027965856716036797
Epoch:  36 	 Loss:  0.028075074777007103
Epoch:  37 	 Loss:  0.027723347768187523
Epoch:  38 	 Loss:  0.027887871488928795
Epoch:  39 	 Loss:  0.027812035754323006
Epoch:  40 	 Loss:  0.027764951810240746
Epoch:  41 	 Loss:  0.027719730511307716
Epoch:  42 	 Loss:  0.02741355635225773
Epoch:  43 	 Loss:  0.0278935469686985
Epoch:  44 	 Loss:  0.027721542865037918
Epoch:  45 	 Loss:  0.027245668694376945
Epoch:  46 	 Loss:  0.028106890618801117
Epoch:  47 	 Loss:  0.027968939393758774
Epoch:  48 	 Loss:  0.027567412704229355
Epoch:  49 	 Loss:  0.0279296413064003
Epoch:  50 	 Loss:  0.02706310711801052
Epoch:  51 	 Loss:  0.02826923131942749
Epoch:  52 	 Loss:  0.027897778898477554
Epoch:  53 	 Loss:  0.027561401948332787
Epoch:  54 	 Loss:  0.02728656679391861
Epoch:  55 	 Loss:  0.028172718361020088
Epoch:  56 	 Loss:  0.027677403762936592
Epoch:  57 	 Loss:  0.027535846456885338
Epoch:  58 	 Loss:  0.027579674497246742
Epoch:  59 	 Loss:  0.027911502867937088
Epoch:  60 	 Loss:  0.027986915782094002
Epoch:  61 	 Loss:  0.02793443202972412
Epoch:  62 	 Loss:  0.02795347385108471
Epoch:  63 	 Loss:  0.027725256979465485
Epoch:  64 	 Loss:  0.027301587164402008
Epoch:  65 	 Loss:  0.02794785611331463
Epoch:  66 	 Loss:  0.027801193296909332
Epoch:  67 	 Loss:  0.02765762247145176
Epoch:  68 	 Loss:  0.02760712057352066
Epoch:  69 	 Loss:  0.02760506235063076
Epoch:  70 	 Loss:  0.027438467368483543
Epoch:  71 	 Loss:  0.027705229818820953
Epoch:  72 	 Loss:  0.0272221602499485
Epoch:  73 	 Loss:  0.027655545622110367
Epoch:  74 	 Loss:  0.028079679235816002
Epoch:  75 	 Loss:  0.027504896745085716
Epoch:  76 	 Loss:  0.027811037376523018
Epoch:  77 	 Loss:  0.027906017377972603
Epoch:  78 	 Loss:  0.02793845348060131
Epoch:  79 	 Loss:  0.02776029333472252
Epoch:  80 	 Loss:  0.028132690116763115
Epoch:  81 	 Loss:  0.027751024812459946
Epoch:  82 	 Loss:  0.027618013322353363
Epoch:  83 	 Loss:  0.027474958449602127
Epoch:  84 	 Loss:  0.028279153630137444
Epoch:  85 	 Loss:  0.027354924008250237
Epoch:  86 	 Loss:  0.027772903442382812
Epoch:  87 	 Loss:  0.027717245742678642
Epoch:  88 	 Loss:  0.028374116867780685
Epoch:  89 	 Loss:  0.027406984940171242
Epoch:  90 	 Loss:  0.02777213230729103
Epoch:  91 	 Loss:  0.02798699401319027
Epoch:  92 	 Loss:  0.027263067662715912
Epoch:  93 	 Loss:  0.0275773536413908
Epoch:  94 	 Loss:  0.027460236102342606
Epoch:  95 	 Loss:  0.028004325926303864
Epoch:  96 	 Loss:  0.0276811346411705
Epoch:  97 	 Loss:  0.028012312948703766
Epoch:  98 	 Loss:  0.02731725387275219
Epoch:  99 	 Loss:  0.027786359190940857
Epoch:  100 	 Loss:  0.02795359306037426
wandb: Waiting for W&B process to finish, PID 1826036
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154030-cw3uffd3/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154030-cw3uffd3/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02795
wandb:        _step 800000
wandb:     _runtime 76
wandb:   _timestamp 1612903306
wandb:         loss 0.02795
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–‡â–…â–„â–‡â–…â–â–„â–†â–ˆâ–†â–ƒâ–…â–†â–‡â–…â–„â–„â–‡â–†â–ˆâ–â–„â–†â–†â–‚â–„â–„â–â–‡â–†â–…â–„â–ˆâ–„â–‚â–â–ƒâ–†â–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced zany-sweep-57: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/cw3uffd3
wandb: Agent Starting Run: 2hwstur4 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:41:52.234766: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:41:52.240517: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run apricot-sweep-58
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/2hwstur4
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_154150-2hwstur4
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.026764819398522377
Epoch:  2 	 Loss:  0.026508638635277748
Epoch:  3 	 Loss:  0.02679169736802578
Epoch:  4 	 Loss:  0.026530761271715164
Epoch:  5 	 Loss:  0.02661648578941822
Epoch:  6 	 Loss:  0.026624243706464767
Epoch:  7 	 Loss:  0.02613174170255661
Epoch:  8 	 Loss:  0.02663678489625454
Epoch:  9 	 Loss:  0.026440074667334557
Epoch:  10 	 Loss:  0.026636682450771332
Epoch:  11 	 Loss:  0.02652297541499138
Epoch:  12 	 Loss:  0.026882516220211983
Epoch:  13 	 Loss:  0.02660706825554371
Epoch:  14 	 Loss:  0.026361552998423576
Epoch:  15 	 Loss:  0.02644040808081627
Epoch:  16 	 Loss:  0.026381555944681168
Epoch:  17 	 Loss:  0.026716314256191254
Epoch:  18 	 Loss:  0.0266884695738554
Epoch:  19 	 Loss:  0.026427680626511574
Epoch:  20 	 Loss:  0.02624051831662655
Epoch:  21 	 Loss:  0.026526369154453278
Epoch:  22 	 Loss:  0.02674964629113674
Epoch:  23 	 Loss:  0.026474695652723312
Epoch:  24 	 Loss:  0.026324953883886337
Epoch:  25 	 Loss:  0.026633000001311302
Epoch:  26 	 Loss:  0.026637347415089607
Epoch:  27 	 Loss:  0.026414083316922188
Epoch:  28 	 Loss:  0.02597179263830185
Epoch:  29 	 Loss:  0.026596063748002052
Epoch:  30 	 Loss:  0.026692472398281097
Epoch:  31 	 Loss:  0.026694612577557564
Epoch:  32 	 Loss:  0.02647397480905056
Epoch:  33 	 Loss:  0.026659350842237473
Epoch:  34 	 Loss:  0.02677094377577305
Epoch:  35 	 Loss:  0.026654940098524094
Epoch:  36 	 Loss:  0.026672376319766045
Epoch:  37 	 Loss:  0.026571815833449364
Epoch:  38 	 Loss:  0.026758037507534027
Epoch:  39 	 Loss:  0.02661290019750595
Epoch:  40 	 Loss:  0.0264871995896101
Epoch:  41 	 Loss:  0.0267624631524086
Epoch:  42 	 Loss:  0.02662494033575058
Epoch:  43 	 Loss:  0.026399750262498856
Epoch:  44 	 Loss:  0.026219142600893974
Epoch:  45 	 Loss:  0.026494698598980904
Epoch:  46 	 Loss:  0.026704465970396996
Epoch:  47 	 Loss:  0.026477409526705742
Epoch:  48 	 Loss:  0.02654246985912323
Epoch:  49 	 Loss:  0.026435337960720062
Epoch:  50 	 Loss:  0.02675122395157814
Epoch:  51 	 Loss:  0.026338577270507812
Epoch:  52 	 Loss:  0.026568220928311348
Epoch:  53 	 Loss:  0.02695097215473652
Epoch:  54 	 Loss:  0.026518238708376884
Epoch:  55 	 Loss:  0.02666824497282505
Epoch:  56 	 Loss:  0.02670956216752529
Epoch:  57 	 Loss:  0.02625630423426628
Epoch:  58 	 Loss:  0.02643858827650547
Epoch:  59 	 Loss:  0.026187224313616753
Epoch:  60 	 Loss:  0.02638232335448265
Epoch:  61 	 Loss:  0.026486318558454514
Epoch:  62 	 Loss:  0.026563601568341255
Epoch:  63 	 Loss:  0.026539739221334457
Epoch:  64 	 Loss:  0.026345638558268547
Epoch:  65 	 Loss:  0.026719894260168076
Epoch:  66 	 Loss:  0.026257824152708054
Epoch:  67 	 Loss:  0.026687508448958397
Epoch:  68 	 Loss:  0.026509899646043777
Epoch:  69 	 Loss:  0.026473045349121094
Epoch:  70 	 Loss:  0.02655927836894989
Epoch:  71 	 Loss:  0.02675929293036461
Epoch:  72 	 Loss:  0.026322832331061363
Epoch:  73 	 Loss:  0.02650112472474575
Epoch:  74 	 Loss:  0.026691317558288574
Epoch:  75 	 Loss:  0.026646796613931656
Epoch:  76 	 Loss:  0.02629818767309189
Epoch:  77 	 Loss:  0.02701021544635296
Epoch:  78 	 Loss:  0.026634877547621727
Epoch:  79 	 Loss:  0.026617003604769707
Epoch:  80 	 Loss:  0.02651931159198284
Epoch:  81 	 Loss:  0.02638206072151661
Epoch:  82 	 Loss:  0.026500949636101723
Epoch:  83 	 Loss:  0.026705151423811913
Epoch:  84 	 Loss:  0.026619503274559975
Epoch:  85 	 Loss:  0.02654542215168476
Epoch:  86 	 Loss:  0.026593433693051338
Epoch:  87 	 Loss:  0.026677552610635757
Epoch:  88 	 Loss:  0.026503747329115868
Epoch:  89 	 Loss:  0.026753155514597893
Epoch:  90 	 Loss:  0.026617195457220078
Epoch:  91 	 Loss:  0.026788102462887764
Epoch:  92 	 Loss:  0.02628597430884838
Epoch:  93 	 Loss:  0.026523690670728683
Epoch:  94 	 Loss:  0.0265581663697958
Epoch:  95 	 Loss:  0.026762114837765694
Epoch:  96 	 Loss:  0.02654385007917881
Epoch:  97 	 Loss:  0.026185529306530952
Epoch:  98 	 Loss:  0.026716411113739014
Epoch:  99 	 Loss:  0.026497235521674156
Epoch:  100 	 Loss:  0.02628992684185505
wandb: Waiting for W&B process to finish, PID 1829733
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154150-2hwstur4/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154150-2hwstur4/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02629
wandb:        _step 800000
wandb:     _runtime 75
wandb:   _timestamp 1612903385
wandb:         loss 0.02629
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–‡â–…â–…â–…â–…â–„â–†â–…â–„â–…â–â–†â–†â–†â–…â–†â–ƒâ–†â–„â–ƒâ–…â–†â–‚â–„â–„â–†â–„â–ƒâ–†â–ˆâ–…â–…â–…â–†â–†â–ƒâ–…â–‚â–ƒ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced apricot-sweep-58: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/2hwstur4
wandb: Agent Starting Run: akw4cj56 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:43:11.034303: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:43:11.039505: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run stellar-sweep-59
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/akw4cj56
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_154309-akw4cj56
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02647114358842373
Epoch:  2 	 Loss:  0.026729458943009377
Epoch:  3 	 Loss:  0.02679935283958912
Epoch:  4 	 Loss:  0.026317326352000237
Epoch:  5 	 Loss:  0.027385050430893898
Epoch:  6 	 Loss:  0.0266669150441885
Epoch:  7 	 Loss:  0.026555588468909264
Epoch:  8 	 Loss:  0.026479197666049004
Epoch:  9 	 Loss:  0.027005907148122787
Epoch:  10 	 Loss:  0.02681160531938076
Epoch:  11 	 Loss:  0.02678014151751995
Epoch:  12 	 Loss:  0.026973064988851547
Epoch:  13 	 Loss:  0.02657383680343628
Epoch:  14 	 Loss:  0.026947373524308205
Epoch:  15 	 Loss:  0.027268853038549423
Epoch:  16 	 Loss:  0.02658315747976303
Epoch:  17 	 Loss:  0.026831144466996193
Epoch:  18 	 Loss:  0.026868104934692383
Epoch:  19 	 Loss:  0.026469068601727486
Epoch:  20 	 Loss:  0.026813840493559837
Epoch:  21 	 Loss:  0.027080614119768143
Epoch:  22 	 Loss:  0.02678998000919819
Epoch:  23 	 Loss:  0.026989702135324478
Epoch:  24 	 Loss:  0.027145041152834892
Epoch:  25 	 Loss:  0.026858197525143623
Epoch:  26 	 Loss:  0.026440294459462166
Epoch:  27 	 Loss:  0.02691039629280567
Epoch:  28 	 Loss:  0.026718202978372574
Epoch:  29 	 Loss:  0.0269569531083107
Epoch:  30 	 Loss:  0.026861561462283134
Epoch:  31 	 Loss:  0.026862163096666336
Epoch:  32 	 Loss:  0.026767399162054062
Epoch:  33 	 Loss:  0.02680296078324318
Epoch:  34 	 Loss:  0.026884298771619797
Epoch:  35 	 Loss:  0.026743067428469658
Epoch:  36 	 Loss:  0.026320215314626694
Epoch:  37 	 Loss:  0.026713917031884193
Epoch:  38 	 Loss:  0.026193687692284584
Epoch:  39 	 Loss:  0.02662585861980915
Epoch:  40 	 Loss:  0.026638025417923927
Epoch:  41 	 Loss:  0.026880135759711266
Epoch:  42 	 Loss:  0.026510044932365417
Epoch:  43 	 Loss:  0.026869256049394608
Epoch:  44 	 Loss:  0.027059655636548996
Epoch:  45 	 Loss:  0.026723118498921394
Epoch:  46 	 Loss:  0.026911873370409012
Epoch:  47 	 Loss:  0.026874536648392677
Epoch:  48 	 Loss:  0.02701753005385399
Epoch:  49 	 Loss:  0.026818202808499336
Epoch:  50 	 Loss:  0.02663273550570011
Epoch:  51 	 Loss:  0.027033723890781403
Epoch:  52 	 Loss:  0.02701694704592228
Epoch:  53 	 Loss:  0.025974906980991364
Epoch:  54 	 Loss:  0.027077926322817802
Epoch:  55 	 Loss:  0.026762595400214195
Epoch:  56 	 Loss:  0.027012856677174568
Epoch:  57 	 Loss:  0.026669640094041824
Epoch:  58 	 Loss:  0.026823263615369797
Epoch:  59 	 Loss:  0.02689221315085888
Epoch:  60 	 Loss:  0.02667025849223137
Epoch:  61 	 Loss:  0.027066024020314217
Epoch:  62 	 Loss:  0.027115238830447197
Epoch:  63 	 Loss:  0.027082117274403572
Epoch:  64 	 Loss:  0.026493048295378685
Epoch:  65 	 Loss:  0.026747634634375572
Epoch:  66 	 Loss:  0.026607871055603027
Epoch:  67 	 Loss:  0.026858191937208176
Epoch:  68 	 Loss:  0.02652350254356861
Epoch:  69 	 Loss:  0.026780536398291588
Epoch:  70 	 Loss:  0.02700946293771267
Epoch:  71 	 Loss:  0.026360131800174713
Epoch:  72 	 Loss:  0.026853345334529877
Epoch:  73 	 Loss:  0.026792626827955246
Epoch:  74 	 Loss:  0.027001477777957916
Epoch:  75 	 Loss:  0.02674511820077896
Epoch:  76 	 Loss:  0.026847466826438904
Epoch:  77 	 Loss:  0.027141213417053223
Epoch:  78 	 Loss:  0.0270290058106184
Epoch:  79 	 Loss:  0.026417072862386703
Epoch:  80 	 Loss:  0.026650140061974525
Epoch:  81 	 Loss:  0.026926295831799507
Epoch:  82 	 Loss:  0.026400553062558174
Epoch:  83 	 Loss:  0.026505226269364357
Epoch:  84 	 Loss:  0.026550007984042168
Epoch:  85 	 Loss:  0.026818018406629562
Epoch:  86 	 Loss:  0.02658642642199993
Epoch:  87 	 Loss:  0.026907896623015404
Epoch:  88 	 Loss:  0.026545261964201927
Epoch:  89 	 Loss:  0.02671990543603897
Epoch:  90 	 Loss:  0.026823759078979492
Epoch:  91 	 Loss:  0.026693353429436684
Epoch:  92 	 Loss:  0.026667343452572823
Epoch:  93 	 Loss:  0.026776574552059174
Epoch:  94 	 Loss:  0.02729463018476963
Epoch:  95 	 Loss:  0.026980863884091377
Epoch:  96 	 Loss:  0.02649812400341034
Epoch:  97 	 Loss:  0.02661634422838688
Epoch:  98 	 Loss:  0.026864638552069664
Epoch:  99 	 Loss:  0.026847701519727707
Epoch:  100 	 Loss:  0.02664804458618164
wandb: Waiting for W&B process to finish, PID 1833498
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154309-akw4cj56/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154309-akw4cj56/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02665
wandb:        _step 800000
wandb:     _runtime 75
wandb:   _timestamp 1612903464
wandb:         loss 0.02665
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–‚â–„â–ƒâ–‚â–„â–ƒâ–ƒâ–…â–†â–†â–‚â–„â–…â–…â–â–ƒâ–…â–†â–…â–…â–†â–†â–†â–…â–†â–‚â–…â–„â–…â–†â–‡â–‚â–‚â–ƒâ–…â–„â–ƒâ–ˆâ–ƒâ–ƒ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced stellar-sweep-59: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/akw4cj56
wandb: Agent Starting Run: 65xb99h1 with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 64
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:44:30.384052: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:44:30.389930: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run peach-sweep-60
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/65xb99h1
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_154428-65xb99h1
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02687746286392212
Epoch:  2 	 Loss:  0.02680085599422455
Epoch:  3 	 Loss:  0.026613749563694
Epoch:  4 	 Loss:  0.026704220101237297
Epoch:  5 	 Loss:  0.02686910517513752
Epoch:  6 	 Loss:  0.026755832135677338
Epoch:  7 	 Loss:  0.0269771758466959
Epoch:  8 	 Loss:  0.026743587106466293
Epoch:  9 	 Loss:  0.026922909542918205
Epoch:  10 	 Loss:  0.026605825871229172
Epoch:  11 	 Loss:  0.026660367846488953
Epoch:  12 	 Loss:  0.026516523212194443
Epoch:  13 	 Loss:  0.026745038107037544
Epoch:  14 	 Loss:  0.02672990970313549
Epoch:  15 	 Loss:  0.02659224160015583
Epoch:  16 	 Loss:  0.026513224467635155
Epoch:  17 	 Loss:  0.02613680623471737
Epoch:  18 	 Loss:  0.027056541293859482
Epoch:  19 	 Loss:  0.02668151445686817
Epoch:  20 	 Loss:  0.026871437206864357
Epoch:  21 	 Loss:  0.02659180946648121
Epoch:  22 	 Loss:  0.02695557102560997
Epoch:  23 	 Loss:  0.026826871559023857
Epoch:  24 	 Loss:  0.026678452268242836
Epoch:  25 	 Loss:  0.026716526597738266
Epoch:  26 	 Loss:  0.026852507144212723
Epoch:  27 	 Loss:  0.026835190132260323
Epoch:  28 	 Loss:  0.02681601420044899
Epoch:  29 	 Loss:  0.026920510455965996
Epoch:  30 	 Loss:  0.026961620897054672
Epoch:  31 	 Loss:  0.026774033904075623
Epoch:  32 	 Loss:  0.026889314875006676
Epoch:  33 	 Loss:  0.026265153661370277
Epoch:  34 	 Loss:  0.0263498704880476
Epoch:  35 	 Loss:  0.026624178513884544
Epoch:  36 	 Loss:  0.02656714804470539
Epoch:  37 	 Loss:  0.02670089155435562
Epoch:  38 	 Loss:  0.02640089951455593
Epoch:  39 	 Loss:  0.026728784665465355
Epoch:  40 	 Loss:  0.026679104194045067
Epoch:  41 	 Loss:  0.026817278936505318
Epoch:  42 	 Loss:  0.026539413258433342
Epoch:  43 	 Loss:  0.026561420410871506
Epoch:  44 	 Loss:  0.026597892865538597
Epoch:  45 	 Loss:  0.02682875283062458
Epoch:  46 	 Loss:  0.026864077895879745
Epoch:  47 	 Loss:  0.026987044140696526
Epoch:  48 	 Loss:  0.026483433321118355
Epoch:  49 	 Loss:  0.02651062235236168
Epoch:  50 	 Loss:  0.026795074343681335
Epoch:  51 	 Loss:  0.02657579444348812
Epoch:  52 	 Loss:  0.026780473068356514
Epoch:  53 	 Loss:  0.02676435559988022
Epoch:  54 	 Loss:  0.026276985183358192
Epoch:  55 	 Loss:  0.026632245630025864
Epoch:  56 	 Loss:  0.026970671489834785
Epoch:  57 	 Loss:  0.02637827955186367
Epoch:  58 	 Loss:  0.026703791692852974
Epoch:  59 	 Loss:  0.027111655101180077
Epoch:  60 	 Loss:  0.02642822265625
Epoch:  61 	 Loss:  0.02658098377287388
Epoch:  62 	 Loss:  0.02659551054239273
Epoch:  63 	 Loss:  0.02693488821387291
Epoch:  64 	 Loss:  0.026694485917687416
Epoch:  65 	 Loss:  0.026573793962597847
Epoch:  66 	 Loss:  0.026665911078453064
Epoch:  67 	 Loss:  0.02651626244187355
Epoch:  68 	 Loss:  0.026745952665805817
Epoch:  69 	 Loss:  0.02639743499457836
Epoch:  70 	 Loss:  0.02698480524122715
Epoch:  71 	 Loss:  0.026874862611293793
Epoch:  72 	 Loss:  0.026378493756055832
Epoch:  73 	 Loss:  0.026418209075927734
Epoch:  74 	 Loss:  0.026757385581731796
Epoch:  75 	 Loss:  0.026633739471435547
Epoch:  76 	 Loss:  0.026459528133273125
Epoch:  77 	 Loss:  0.026244565844535828
Epoch:  78 	 Loss:  0.026595192030072212
Epoch:  79 	 Loss:  0.02652481012046337
Epoch:  80 	 Loss:  0.02674928866326809
Epoch:  81 	 Loss:  0.02680441550910473
Epoch:  82 	 Loss:  0.026645774021744728
Epoch:  83 	 Loss:  0.026631245389580727
Epoch:  84 	 Loss:  0.026757245883345604
Epoch:  85 	 Loss:  0.02630237303674221
Epoch:  86 	 Loss:  0.02662373147904873
Epoch:  87 	 Loss:  0.026556147262454033
Epoch:  88 	 Loss:  0.026470724493265152
Epoch:  89 	 Loss:  0.026346787810325623
Epoch:  90 	 Loss:  0.026537923142313957
Epoch:  91 	 Loss:  0.026845110580325127
Epoch:  92 	 Loss:  0.02672632783651352
Epoch:  93 	 Loss:  0.026517754420638084
Epoch:  94 	 Loss:  0.026832645758986473
Epoch:  95 	 Loss:  0.026195265352725983
Epoch:  96 	 Loss:  0.02673005498945713
Epoch:  97 	 Loss:  0.026774415746331215
Epoch:  98 	 Loss:  0.026577875018119812
Epoch:  99 	 Loss:  0.02668391726911068
Epoch:  100 	 Loss:  0.0267010685056448
wandb: Waiting for W&B process to finish, PID 1837191
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154428-65xb99h1/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154428-65xb99h1/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0267
wandb:        _step 800000
wandb:     _runtime 76
wandb:   _timestamp 1612903544
wandb:         loss 0.0267
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–„â–…â–…â–„â–…â–ƒâ–ˆâ–„â–†â–†â–†â–…â–‚â–„â–…â–†â–„â–†â–ƒâ–„â–â–‡â–ˆâ–„â–…â–ƒâ–‚â–‚â–…â–â–ƒâ–„â–…â–„â–‚â–…â–†â–…â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced peach-sweep-60: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/65xb99h1
wandb: Agent Starting Run: 3j4vxqja with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: adam
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:45:50.446178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:45:50.451459: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run frosty-sweep-61
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/3j4vxqja
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_154548-3j4vxqja
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02675487846136093
Epoch:  2 	 Loss:  0.026729043573141098
Epoch:  3 	 Loss:  0.02661377191543579
Epoch:  4 	 Loss:  0.026508262380957603
Epoch:  5 	 Loss:  0.026671892032027245
Epoch:  6 	 Loss:  0.026688506826758385
Epoch:  7 	 Loss:  0.02700134739279747
Epoch:  8 	 Loss:  0.026762830093503
Epoch:  9 	 Loss:  0.026681197807192802
Epoch:  10 	 Loss:  0.027003463357686996
Epoch:  11 	 Loss:  0.02652222290635109
Epoch:  12 	 Loss:  0.02642585150897503
Epoch:  13 	 Loss:  0.026734456419944763
Epoch:  14 	 Loss:  0.02681887149810791
Epoch:  15 	 Loss:  0.026704417541623116
Epoch:  16 	 Loss:  0.026799581944942474
Epoch:  17 	 Loss:  0.02657315693795681
Epoch:  18 	 Loss:  0.026634423062205315
Epoch:  19 	 Loss:  0.026271458715200424
Epoch:  20 	 Loss:  0.0261013712733984
Epoch:  21 	 Loss:  0.0267809946089983
Epoch:  22 	 Loss:  0.026630891487002373
Epoch:  23 	 Loss:  0.026824386790394783
Epoch:  24 	 Loss:  0.02687542326748371
Epoch:  25 	 Loss:  0.02670644223690033
Epoch:  26 	 Loss:  0.02629506215453148
Epoch:  27 	 Loss:  0.02641650289297104
Epoch:  28 	 Loss:  0.02703080140054226
Epoch:  29 	 Loss:  0.026578545570373535
Epoch:  30 	 Loss:  0.02670395001769066
Epoch:  31 	 Loss:  0.02664431743323803
Epoch:  32 	 Loss:  0.026482468470931053
Epoch:  33 	 Loss:  0.026636013761162758
Epoch:  34 	 Loss:  0.026636755093932152
Epoch:  35 	 Loss:  0.02670036442577839
Epoch:  36 	 Loss:  0.02643461711704731
Epoch:  37 	 Loss:  0.026272227987647057
Epoch:  38 	 Loss:  0.026684409007430077
Epoch:  39 	 Loss:  0.02657967433333397
Epoch:  40 	 Loss:  0.02644316479563713
Epoch:  41 	 Loss:  0.02645951136946678
Epoch:  42 	 Loss:  0.02635014057159424
Epoch:  43 	 Loss:  0.026292750611901283
Epoch:  44 	 Loss:  0.026852186769247055
Epoch:  45 	 Loss:  0.026504354551434517
Epoch:  46 	 Loss:  0.026625454425811768
Epoch:  47 	 Loss:  0.0265047587454319
Epoch:  48 	 Loss:  0.027011990547180176
Epoch:  49 	 Loss:  0.026643913239240646
Epoch:  50 	 Loss:  0.02710079774260521
Epoch:  51 	 Loss:  0.026976799592375755
Epoch:  52 	 Loss:  0.026577338576316833
Epoch:  53 	 Loss:  0.026907814666628838
Epoch:  54 	 Loss:  0.026668479666113853
Epoch:  55 	 Loss:  0.02640029601752758
Epoch:  56 	 Loss:  0.026690592989325523
Epoch:  57 	 Loss:  0.02640063501894474
Epoch:  58 	 Loss:  0.026680968701839447
Epoch:  59 	 Loss:  0.026635315269231796
Epoch:  60 	 Loss:  0.026607908308506012
Epoch:  61 	 Loss:  0.026672687381505966
Epoch:  62 	 Loss:  0.026698753237724304
Epoch:  63 	 Loss:  0.026372071355581284
Epoch:  64 	 Loss:  0.026689065620303154
Epoch:  65 	 Loss:  0.02641497179865837
Epoch:  66 	 Loss:  0.02681925520300865
Epoch:  67 	 Loss:  0.026354720816016197
Epoch:  68 	 Loss:  0.026457199826836586
Epoch:  69 	 Loss:  0.026792705059051514
Epoch:  70 	 Loss:  0.026126328855752945
Epoch:  71 	 Loss:  0.026775235310196877
Epoch:  72 	 Loss:  0.02679389715194702
Epoch:  73 	 Loss:  0.026372041553258896
Epoch:  74 	 Loss:  0.026300277560949326
Epoch:  75 	 Loss:  0.026568738743662834
Epoch:  76 	 Loss:  0.02640489488840103
Epoch:  77 	 Loss:  0.02607046440243721
Epoch:  78 	 Loss:  0.026814529672265053
Epoch:  79 	 Loss:  0.026646343991160393
Epoch:  80 	 Loss:  0.026627635583281517
Epoch:  81 	 Loss:  0.026916461065411568
Epoch:  82 	 Loss:  0.0267759058624506
Epoch:  83 	 Loss:  0.0268730279058218
Epoch:  84 	 Loss:  0.02706798166036606
Epoch:  85 	 Loss:  0.02694547548890114
Epoch:  86 	 Loss:  0.026739146560430527
Epoch:  87 	 Loss:  0.026687797158956528
Epoch:  88 	 Loss:  0.02633737586438656
Epoch:  89 	 Loss:  0.026265770196914673
Epoch:  90 	 Loss:  0.026717528700828552
Epoch:  91 	 Loss:  0.026465868577361107
Epoch:  92 	 Loss:  0.0265871100127697
Epoch:  93 	 Loss:  0.026740776374936104
Epoch:  94 	 Loss:  0.02632441744208336
Epoch:  95 	 Loss:  0.026246732100844383
Epoch:  96 	 Loss:  0.027029037475585938
Epoch:  97 	 Loss:  0.02654300630092621
Epoch:  98 	 Loss:  0.02651122957468033
Epoch:  99 	 Loss:  0.026650648564100266
Epoch:  100 	 Loss:  0.026892956346273422
wandb: Waiting for W&B process to finish, PID 1840867
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154548-3j4vxqja/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154548-3j4vxqja/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02689
wandb:        _step 800000
wandb:     _runtime 77
wandb:   _timestamp 1612903625
wandb:         loss 0.02689
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–…â–…â–†â–„â–†â–†â–…â–†â–†â–ƒâ–ˆâ–…â–…â–„â–…â–„â–†â–…â–…â–‡â–…â–…â–…â–…â–…â–ƒâ–†â–†â–ƒâ–â–…â–†â–ˆâ–…â–‚â–…â–ƒâ–„â–‡
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced frosty-sweep-61: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/3j4vxqja
wandb: Agent Starting Run: 40dkojsk with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:47:11.709691: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:47:11.717000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run youthful-sweep-62
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/40dkojsk
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_154709-40dkojsk
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02685745805501938
Epoch:  2 	 Loss:  0.02695830538868904
Epoch:  3 	 Loss:  0.02670903690159321
Epoch:  4 	 Loss:  0.02649916149675846
Epoch:  5 	 Loss:  0.026848847046494484
Epoch:  6 	 Loss:  0.026662327349185944
Epoch:  7 	 Loss:  0.027433771640062332
Epoch:  8 	 Loss:  0.026697250083088875
Epoch:  9 	 Loss:  0.02698938548564911
Epoch:  10 	 Loss:  0.026868067681789398
Epoch:  11 	 Loss:  0.0269157737493515
Epoch:  12 	 Loss:  0.026956329122185707
Epoch:  13 	 Loss:  0.027329470962285995
Epoch:  14 	 Loss:  0.027032488957047462
Epoch:  15 	 Loss:  0.026789313182234764
Epoch:  16 	 Loss:  0.0271142590790987
Epoch:  17 	 Loss:  0.027468612417578697
Epoch:  18 	 Loss:  0.02724372036755085
Epoch:  19 	 Loss:  0.027158154174685478
Epoch:  20 	 Loss:  0.026994379237294197
Epoch:  21 	 Loss:  0.02698374167084694
Epoch:  22 	 Loss:  0.027091430500149727
Epoch:  23 	 Loss:  0.026984214782714844
Epoch:  24 	 Loss:  0.02667851559817791
Epoch:  25 	 Loss:  0.02713356539607048
Epoch:  26 	 Loss:  0.027165142819285393
Epoch:  27 	 Loss:  0.02676202729344368
Epoch:  28 	 Loss:  0.027163079008460045
Epoch:  29 	 Loss:  0.026462115347385406
Epoch:  30 	 Loss:  0.027236705645918846
Epoch:  31 	 Loss:  0.026914609596133232
Epoch:  32 	 Loss:  0.027000289410352707
Epoch:  33 	 Loss:  0.027100032195448875
Epoch:  34 	 Loss:  0.027124587446451187
Epoch:  35 	 Loss:  0.027277996763586998
Epoch:  36 	 Loss:  0.026875711977481842
Epoch:  37 	 Loss:  0.02706979401409626
Epoch:  38 	 Loss:  0.027163034304976463
Epoch:  39 	 Loss:  0.02764740213751793
Epoch:  40 	 Loss:  0.02640419639647007
Epoch:  41 	 Loss:  0.02663985639810562
Epoch:  42 	 Loss:  0.026787664741277695
Epoch:  43 	 Loss:  0.027181224897503853
Epoch:  44 	 Loss:  0.02684660628437996
Epoch:  45 	 Loss:  0.02727871760725975
Epoch:  46 	 Loss:  0.027704261243343353
Epoch:  47 	 Loss:  0.027243664488196373
Epoch:  48 	 Loss:  0.026954736560583115
Epoch:  49 	 Loss:  0.0274359080940485
Epoch:  50 	 Loss:  0.027218813076615334
Epoch:  51 	 Loss:  0.026751097291707993
Epoch:  52 	 Loss:  0.026559654623270035
Epoch:  53 	 Loss:  0.027034327387809753
Epoch:  54 	 Loss:  0.026570644229650497
Epoch:  55 	 Loss:  0.027279822155833244
Epoch:  56 	 Loss:  0.026806185021996498
Epoch:  57 	 Loss:  0.02716381661593914
Epoch:  58 	 Loss:  0.02706828899681568
Epoch:  59 	 Loss:  0.027233559638261795
Epoch:  60 	 Loss:  0.02699105441570282
Epoch:  61 	 Loss:  0.027534937486052513
Epoch:  62 	 Loss:  0.026992760598659515
Epoch:  63 	 Loss:  0.0270960982888937
Epoch:  64 	 Loss:  0.027079422026872635
Epoch:  65 	 Loss:  0.026597995311021805
Epoch:  66 	 Loss:  0.02698967047035694
Epoch:  67 	 Loss:  0.02712968736886978
Epoch:  68 	 Loss:  0.02729094587266445
Epoch:  69 	 Loss:  0.02657235972583294
Epoch:  70 	 Loss:  0.02712702751159668
Epoch:  71 	 Loss:  0.026995288208127022
Epoch:  72 	 Loss:  0.02691558189690113
Epoch:  73 	 Loss:  0.02670893259346485
Epoch:  74 	 Loss:  0.026500049978494644
Epoch:  75 	 Loss:  0.02752714976668358
Epoch:  76 	 Loss:  0.026919126510620117
Epoch:  77 	 Loss:  0.027393441647291183
Epoch:  78 	 Loss:  0.027185169979929924
Epoch:  79 	 Loss:  0.027406981214880943
Epoch:  80 	 Loss:  0.027091264724731445
Epoch:  81 	 Loss:  0.026832466945052147
Epoch:  82 	 Loss:  0.027315249666571617
Epoch:  83 	 Loss:  0.02715638466179371
Epoch:  84 	 Loss:  0.027033375576138496
Epoch:  85 	 Loss:  0.026653610169887543
Epoch:  86 	 Loss:  0.026970377191901207
Epoch:  87 	 Loss:  0.02715975046157837
Epoch:  88 	 Loss:  0.027058687061071396
Epoch:  89 	 Loss:  0.027147669345140457
Epoch:  90 	 Loss:  0.02664770931005478
Epoch:  91 	 Loss:  0.027128802612423897
Epoch:  92 	 Loss:  0.027008257806301117
Epoch:  93 	 Loss:  0.027282586321234703
Epoch:  94 	 Loss:  0.027442248538136482
Epoch:  95 	 Loss:  0.027528977021574974
Epoch:  96 	 Loss:  0.02698170579969883
Epoch:  97 	 Loss:  0.02728240191936493
Epoch:  98 	 Loss:  0.026368414983153343
Epoch:  99 	 Loss:  0.027259068563580513
Epoch:  100 	 Loss:  0.027064690366387367
wandb: Waiting for W&B process to finish, PID 1844547
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154709-40dkojsk/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154709-40dkojsk/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02706
wandb:        _step 800000
wandb:     _runtime 77
wandb:   _timestamp 1612903706
wandb:         loss 0.02706
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–ƒâ–‚â–‚â–‚â–ƒâ–†â–…â–…â–„â–„â–…â–…â–ƒâ–…â–ƒâ–ˆâ–‚â–ƒâ–ˆâ–†â–‚â–â–ƒâ–…â–‡â–„â–…â–â–ƒâ–â–†â–†â–†â–„â–…â–…â–„â–†â–†â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced youthful-sweep-62: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/40dkojsk
wandb: Agent Starting Run: rd0oe56n with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:48:32.657176: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:48:32.663172: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run giddy-sweep-63
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/rd0oe56n
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_154830-rd0oe56n
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02665543556213379
Epoch:  2 	 Loss:  0.0268859826028347
Epoch:  3 	 Loss:  0.027097877115011215
Epoch:  4 	 Loss:  0.02676047571003437
Epoch:  5 	 Loss:  0.026529820635914803
Epoch:  6 	 Loss:  0.026939179748296738
Epoch:  7 	 Loss:  0.02717314288020134
Epoch:  8 	 Loss:  0.026838388293981552
Epoch:  9 	 Loss:  0.026583796367049217
Epoch:  10 	 Loss:  0.027112070471048355
Epoch:  11 	 Loss:  0.026810897514224052
Epoch:  12 	 Loss:  0.027227049693465233
Epoch:  13 	 Loss:  0.02634979970753193
Epoch:  14 	 Loss:  0.026747386902570724
Epoch:  15 	 Loss:  0.026412412524223328
Epoch:  16 	 Loss:  0.02724906988441944
Epoch:  17 	 Loss:  0.026738038286566734
Epoch:  18 	 Loss:  0.026833442971110344
Epoch:  19 	 Loss:  0.026767542585730553
Epoch:  20 	 Loss:  0.02707865461707115
Epoch:  21 	 Loss:  0.026910817250609398
Epoch:  22 	 Loss:  0.026776419952511787
Epoch:  23 	 Loss:  0.02657819353044033
Epoch:  24 	 Loss:  0.02673172950744629
Epoch:  25 	 Loss:  0.027367549017071724
Epoch:  26 	 Loss:  0.026882817968726158
Epoch:  27 	 Loss:  0.02689841017127037
Epoch:  28 	 Loss:  0.02699296735227108
Epoch:  29 	 Loss:  0.027032598853111267
Epoch:  30 	 Loss:  0.02690708078444004
Epoch:  31 	 Loss:  0.026478616520762444
Epoch:  32 	 Loss:  0.027043847367167473
Epoch:  33 	 Loss:  0.026722637936472893
Epoch:  34 	 Loss:  0.026896988973021507
Epoch:  35 	 Loss:  0.026724621653556824
Epoch:  36 	 Loss:  0.026822084560990334
Epoch:  37 	 Loss:  0.027123084291815758
Epoch:  38 	 Loss:  0.027062511071562767
Epoch:  39 	 Loss:  0.027071787044405937
Epoch:  40 	 Loss:  0.02717970311641693
Epoch:  41 	 Loss:  0.027283819392323494
Epoch:  42 	 Loss:  0.027140799909830093
Epoch:  43 	 Loss:  0.026743251830339432
Epoch:  44 	 Loss:  0.027387427166104317
Epoch:  45 	 Loss:  0.026842741295695305
Epoch:  46 	 Loss:  0.02698504365980625
Epoch:  47 	 Loss:  0.026778675615787506
Epoch:  48 	 Loss:  0.02703794650733471
Epoch:  49 	 Loss:  0.026972560212016106
Epoch:  50 	 Loss:  0.026889948174357414
Epoch:  51 	 Loss:  0.02694389782845974
Epoch:  52 	 Loss:  0.026798224076628685
Epoch:  53 	 Loss:  0.026267103850841522
Epoch:  54 	 Loss:  0.027062246575951576
Epoch:  55 	 Loss:  0.02682539075613022
Epoch:  56 	 Loss:  0.026757782325148582
Epoch:  57 	 Loss:  0.026623763144016266
Epoch:  58 	 Loss:  0.02710937149822712
Epoch:  59 	 Loss:  0.027131956070661545
Epoch:  60 	 Loss:  0.026950161904096603
Epoch:  61 	 Loss:  0.026618536561727524
Epoch:  62 	 Loss:  0.026938432827591896
Epoch:  63 	 Loss:  0.026942212134599686
Epoch:  64 	 Loss:  0.02678856812417507
Epoch:  65 	 Loss:  0.026812728494405746
Epoch:  66 	 Loss:  0.02697615884244442
Epoch:  67 	 Loss:  0.02661389485001564
Epoch:  68 	 Loss:  0.02681792341172695
Epoch:  69 	 Loss:  0.026659762486815453
Epoch:  70 	 Loss:  0.026420217007398605
Epoch:  71 	 Loss:  0.02728363499045372
Epoch:  72 	 Loss:  0.027277840301394463
Epoch:  73 	 Loss:  0.027116524055600166
Epoch:  74 	 Loss:  0.027069875970482826
Epoch:  75 	 Loss:  0.026923606172204018
Epoch:  76 	 Loss:  0.027060922235250473
Epoch:  77 	 Loss:  0.027037501335144043
Epoch:  78 	 Loss:  0.02718646638095379
Epoch:  79 	 Loss:  0.02681298740208149
Epoch:  80 	 Loss:  0.02695380710065365
Epoch:  81 	 Loss:  0.026540981605648994
Epoch:  82 	 Loss:  0.026518192142248154
Epoch:  83 	 Loss:  0.02713262103497982
Epoch:  84 	 Loss:  0.027037758380174637
Epoch:  85 	 Loss:  0.02701231651008129
Epoch:  86 	 Loss:  0.027025185525417328
Epoch:  87 	 Loss:  0.026803668588399887
Epoch:  88 	 Loss:  0.026411835104227066
Epoch:  89 	 Loss:  0.027164453640580177
Epoch:  90 	 Loss:  0.027105767279863358
Epoch:  91 	 Loss:  0.026952508836984634
Epoch:  92 	 Loss:  0.0271966140717268
Epoch:  93 	 Loss:  0.027068501338362694
Epoch:  94 	 Loss:  0.02711051143705845
Epoch:  95 	 Loss:  0.02703619934618473
Epoch:  96 	 Loss:  0.026861250400543213
Epoch:  97 	 Loss:  0.02711404673755169
Epoch:  98 	 Loss:  0.027045991271734238
Epoch:  99 	 Loss:  0.02687801793217659
Epoch:  100 	 Loss:  0.02678508125245571
wandb: Waiting for W&B process to finish, PID 1848226
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154830-rd0oe56n/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154830-rd0oe56n/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02679
wandb:        _step 800000
wandb:     _runtime 77
wandb:   _timestamp 1612903787
wandb:         loss 0.02679
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–ƒâ–†â–…â–„â–„â–â–‡â–„â–…â–ƒâ–…â–…â–‚â–…â–„â–†â–‡â–ˆâ–…â–…â–…â–†â–„â–†â–ƒâ–„â–ƒâ–ƒâ–‡â–†â–†â–„â–‚â–†â–„â–†â–‡â–†â–†â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced giddy-sweep-63: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/rd0oe56n
wandb: Agent Starting Run: xnykqztn with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: adam
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:49:54.011982: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:49:54.017728: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run fanciful-sweep-64
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/xnykqztn
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_154952-xnykqztn
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02741047367453575
Epoch:  2 	 Loss:  0.02785472571849823
Epoch:  3 	 Loss:  0.027351006865501404
Epoch:  4 	 Loss:  0.02693879045546055
Epoch:  5 	 Loss:  0.027403295040130615
Epoch:  6 	 Loss:  0.027214188128709793
Epoch:  7 	 Loss:  0.02711336314678192
Epoch:  8 	 Loss:  0.027309443801641464
Epoch:  9 	 Loss:  0.027176493778824806
Epoch:  10 	 Loss:  0.02712266705930233
Epoch:  11 	 Loss:  0.02772418223321438
Epoch:  12 	 Loss:  0.027585120871663094
Epoch:  13 	 Loss:  0.027189597487449646
Epoch:  14 	 Loss:  0.027029162272810936
Epoch:  15 	 Loss:  0.02734086662530899
Epoch:  16 	 Loss:  0.027465321123600006
Epoch:  17 	 Loss:  0.02702176570892334
Epoch:  18 	 Loss:  0.02754310518503189
Epoch:  19 	 Loss:  0.027452334761619568
Epoch:  20 	 Loss:  0.027486443519592285
Epoch:  21 	 Loss:  0.02705668844282627
Epoch:  22 	 Loss:  0.027591420337557793
Epoch:  23 	 Loss:  0.027128329500555992
Epoch:  24 	 Loss:  0.02699131704866886
Epoch:  25 	 Loss:  0.02753090299665928
Epoch:  26 	 Loss:  0.027865473181009293
Epoch:  27 	 Loss:  0.02717687003314495
Epoch:  28 	 Loss:  0.02787637710571289
Epoch:  29 	 Loss:  0.027072874829173088
Epoch:  30 	 Loss:  0.02709769643843174
Epoch:  31 	 Loss:  0.02737005241215229
Epoch:  32 	 Loss:  0.02743428573012352
Epoch:  33 	 Loss:  0.027711039409041405
Epoch:  34 	 Loss:  0.02732817456126213
Epoch:  35 	 Loss:  0.027443723753094673
Epoch:  36 	 Loss:  0.027631795033812523
Epoch:  37 	 Loss:  0.027181725949048996
Epoch:  38 	 Loss:  0.02722064219415188
Epoch:  39 	 Loss:  0.02725095860660076
Epoch:  40 	 Loss:  0.027290940284729004
Epoch:  41 	 Loss:  0.027566658332943916
Epoch:  42 	 Loss:  0.027548102661967278
Epoch:  43 	 Loss:  0.027393698692321777
Epoch:  44 	 Loss:  0.027421098202466965
Epoch:  45 	 Loss:  0.02732902765274048
Epoch:  46 	 Loss:  0.027466008439660072
Epoch:  47 	 Loss:  0.027282949537038803
Epoch:  48 	 Loss:  0.02739850804209709
Epoch:  49 	 Loss:  0.02694074809551239
Epoch:  50 	 Loss:  0.027640169486403465
Epoch:  51 	 Loss:  0.02757221832871437
Epoch:  52 	 Loss:  0.0274440236389637
Epoch:  53 	 Loss:  0.027297433465719223
Epoch:  54 	 Loss:  0.027526535093784332
Epoch:  55 	 Loss:  0.027176737785339355
Epoch:  56 	 Loss:  0.027186738327145576
Epoch:  57 	 Loss:  0.02738158032298088
Epoch:  58 	 Loss:  0.02735905908048153
Epoch:  59 	 Loss:  0.027494248002767563
Epoch:  60 	 Loss:  0.027672545984387398
Epoch:  61 	 Loss:  0.02727751061320305
Epoch:  62 	 Loss:  0.027448002249002457
Epoch:  63 	 Loss:  0.027471812441945076
Epoch:  64 	 Loss:  0.027446705847978592
Epoch:  65 	 Loss:  0.02722316049039364
Epoch:  66 	 Loss:  0.027077486738562584
Epoch:  67 	 Loss:  0.027221189811825752
Epoch:  68 	 Loss:  0.027313025668263435
Epoch:  69 	 Loss:  0.027387553825974464
Epoch:  70 	 Loss:  0.02701609395444393
Epoch:  71 	 Loss:  0.027380365878343582
Epoch:  72 	 Loss:  0.0280007291585207
Epoch:  73 	 Loss:  0.027302002534270287
Epoch:  74 	 Loss:  0.026921818032860756
Epoch:  75 	 Loss:  0.02760160341858864
Epoch:  76 	 Loss:  0.027362575754523277
Epoch:  77 	 Loss:  0.027253948152065277
Epoch:  78 	 Loss:  0.02684824727475643
Epoch:  79 	 Loss:  0.027170203626155853
Epoch:  80 	 Loss:  0.02735498733818531
Epoch:  81 	 Loss:  0.027772847563028336
Epoch:  82 	 Loss:  0.027142392471432686
Epoch:  83 	 Loss:  0.027182510122656822
Epoch:  84 	 Loss:  0.027314359322190285
Epoch:  85 	 Loss:  0.027025992050766945
Epoch:  86 	 Loss:  0.02749009244143963
Epoch:  87 	 Loss:  0.027499990537762642
Epoch:  88 	 Loss:  0.02748793736100197
Epoch:  89 	 Loss:  0.027420856058597565
Epoch:  90 	 Loss:  0.02768649347126484
Epoch:  91 	 Loss:  0.027098678052425385
Epoch:  92 	 Loss:  0.027791399508714676
Epoch:  93 	 Loss:  0.027215801179409027
Epoch:  94 	 Loss:  0.02719014324247837
Epoch:  95 	 Loss:  0.027052108198404312
Epoch:  96 	 Loss:  0.02735326811671257
Epoch:  97 	 Loss:  0.027187058702111244
Epoch:  98 	 Loss:  0.02729695662856102
Epoch:  99 	 Loss:  0.027355941012501717
Epoch:  100 	 Loss:  0.027522463351488113
wandb: Waiting for W&B process to finish, PID 1851915
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154952-xnykqztn/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_154952-xnykqztn/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02752
wandb:        _step 800000
wandb:     _runtime 76
wandb:   _timestamp 1612903868
wandb:         loss 0.02752
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–„â–ƒâ–„â–†â–ƒâ–…â–…â–‚â–‚â–‡â–‡â–„â–„â–†â–ƒâ–…â–„â–…â–â–…â–…â–ƒâ–…â–ƒâ–„â–ƒâ–„â–ˆâ–â–ƒâ–ƒâ–‚â–„â–…â–„â–‡â–ƒâ–ƒâ–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced fanciful-sweep-64: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/xnykqztn
wandb: Agent Starting Run: m601owya with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:51:14.409321: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:51:14.414835: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run summer-sweep-65
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/m601owya
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_155112-m601owya
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.026870884001255035
Epoch:  2 	 Loss:  0.02727675810456276
Epoch:  3 	 Loss:  0.02671189047396183
Epoch:  4 	 Loss:  0.026723628863692284
Epoch:  5 	 Loss:  0.026919296011328697
Epoch:  6 	 Loss:  0.026595454663038254
Epoch:  7 	 Loss:  0.02688482590019703
Epoch:  8 	 Loss:  0.026588307693600655
Epoch:  9 	 Loss:  0.026767322793602943
Epoch:  10 	 Loss:  0.027019504457712173
Epoch:  11 	 Loss:  0.026928607374429703
Epoch:  12 	 Loss:  0.026791149750351906
Epoch:  13 	 Loss:  0.026661919429898262
Epoch:  14 	 Loss:  0.026814259588718414
Epoch:  15 	 Loss:  0.025959908962249756
Epoch:  16 	 Loss:  0.026572423055768013
Epoch:  17 	 Loss:  0.02667362429201603
Epoch:  18 	 Loss:  0.02678307704627514
Epoch:  19 	 Loss:  0.026691794395446777
Epoch:  20 	 Loss:  0.026608498767018318
Epoch:  21 	 Loss:  0.026757752522826195
Epoch:  22 	 Loss:  0.02674458920955658
Epoch:  23 	 Loss:  0.026959296315908432
Epoch:  24 	 Loss:  0.026751982048153877
Epoch:  25 	 Loss:  0.026861323043704033
Epoch:  26 	 Loss:  0.026633312925696373
Epoch:  27 	 Loss:  0.02685181051492691
Epoch:  28 	 Loss:  0.02682528644800186
Epoch:  29 	 Loss:  0.026771968230605125
Epoch:  30 	 Loss:  0.02692599780857563
Epoch:  31 	 Loss:  0.02682764269411564
Epoch:  32 	 Loss:  0.02660743147134781
Epoch:  33 	 Loss:  0.02635437622666359
Epoch:  34 	 Loss:  0.026576455682516098
Epoch:  35 	 Loss:  0.02703850530087948
Epoch:  36 	 Loss:  0.026986585929989815
Epoch:  37 	 Loss:  0.026875881478190422
Epoch:  38 	 Loss:  0.02665407769382
Epoch:  39 	 Loss:  0.026321690529584885
Epoch:  40 	 Loss:  0.02658694051206112
Epoch:  41 	 Loss:  0.026295218616724014
Epoch:  42 	 Loss:  0.02740437723696232
Epoch:  43 	 Loss:  0.026704508811235428
Epoch:  44 	 Loss:  0.026899613440036774
Epoch:  45 	 Loss:  0.026843171566724777
Epoch:  46 	 Loss:  0.02699911594390869
Epoch:  47 	 Loss:  0.0271032452583313
Epoch:  48 	 Loss:  0.027161207050085068
Epoch:  49 	 Loss:  0.02710041031241417
Epoch:  50 	 Loss:  0.026890691369771957
Epoch:  51 	 Loss:  0.02675676718354225
Epoch:  52 	 Loss:  0.02686849981546402
Epoch:  53 	 Loss:  0.02682456001639366
Epoch:  54 	 Loss:  0.026928769424557686
Epoch:  55 	 Loss:  0.02708371728658676
Epoch:  56 	 Loss:  0.02665320597589016
Epoch:  57 	 Loss:  0.02655649743974209
Epoch:  58 	 Loss:  0.026893334463238716
Epoch:  59 	 Loss:  0.02702997624874115
Epoch:  60 	 Loss:  0.027066286653280258
Epoch:  61 	 Loss:  0.026958221569657326
Epoch:  62 	 Loss:  0.02664950303733349
Epoch:  63 	 Loss:  0.027383584529161453
Epoch:  64 	 Loss:  0.026577509939670563
Epoch:  65 	 Loss:  0.026843275874853134
Epoch:  66 	 Loss:  0.026795318350195885
Epoch:  67 	 Loss:  0.027162181213498116
Epoch:  68 	 Loss:  0.02667567878961563
Epoch:  69 	 Loss:  0.026737386360764503
Epoch:  70 	 Loss:  0.02639050967991352
Epoch:  71 	 Loss:  0.026867540553212166
Epoch:  72 	 Loss:  0.02674008719623089
Epoch:  73 	 Loss:  0.0272222887724638
Epoch:  74 	 Loss:  0.026741595938801765
Epoch:  75 	 Loss:  0.026954974979162216
Epoch:  76 	 Loss:  0.026644665747880936
Epoch:  77 	 Loss:  0.026667989790439606
Epoch:  78 	 Loss:  0.026619328185915947
Epoch:  79 	 Loss:  0.026782210916280746
Epoch:  80 	 Loss:  0.02676953747868538
Epoch:  81 	 Loss:  0.026991935446858406
Epoch:  82 	 Loss:  0.02728123404085636
Epoch:  83 	 Loss:  0.026837559416890144
Epoch:  84 	 Loss:  0.026835152879357338
Epoch:  85 	 Loss:  0.026845073327422142
Epoch:  86 	 Loss:  0.026873139664530754
Epoch:  87 	 Loss:  0.027059072628617287
Epoch:  88 	 Loss:  0.027115684002637863
Epoch:  89 	 Loss:  0.026468968018889427
Epoch:  90 	 Loss:  0.02702799066901207
Epoch:  91 	 Loss:  0.027266841381788254
Epoch:  92 	 Loss:  0.026935316622257233
Epoch:  93 	 Loss:  0.026837412267923355
Epoch:  94 	 Loss:  0.02674773521721363
Epoch:  95 	 Loss:  0.026415223255753517
Epoch:  96 	 Loss:  0.026783412322402
Epoch:  97 	 Loss:  0.026809748262166977
Epoch:  98 	 Loss:  0.02655263990163803
Epoch:  99 	 Loss:  0.026853322982788086
Epoch:  100 	 Loss:  0.0268723051995039
wandb: Waiting for W&B process to finish, PID 1855594
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155112-m601owya/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155112-m601owya/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02687
wandb:        _step 800000
wandb:     _runtime 72
wandb:   _timestamp 1612903944
wandb:         loss 0.02687
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–„â–ƒâ–ƒâ–…â–„â–ƒâ–„â–„â–†â–ƒâ–…â–…â–ƒâ–†â–â–â–…â–†â–‡â–„â–…â–„â–†â–†â–ƒâ–‡â–„â–„â–„â–„â–„â–ˆâ–…â–†â–‚â–†â–„â–…â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced summer-sweep-65: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/m601owya
wandb: Agent Starting Run: uk1gf3ij with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:52:30.338614: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:52:30.344426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run lilac-sweep-66
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/uk1gf3ij
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_155228-uk1gf3ij
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02716587670147419
Epoch:  2 	 Loss:  0.026973329484462738
Epoch:  3 	 Loss:  0.026942899450659752
Epoch:  4 	 Loss:  0.026933785527944565
Epoch:  5 	 Loss:  0.026935815811157227
Epoch:  6 	 Loss:  0.026896513998508453
Epoch:  7 	 Loss:  0.026821039617061615
Epoch:  8 	 Loss:  0.02655826508998871
Epoch:  9 	 Loss:  0.02678460255265236
Epoch:  10 	 Loss:  0.02666049264371395
Epoch:  11 	 Loss:  0.02694900520145893
Epoch:  12 	 Loss:  0.026810433715581894
Epoch:  13 	 Loss:  0.027050279080867767
Epoch:  14 	 Loss:  0.02725638821721077
Epoch:  15 	 Loss:  0.026834644377231598
Epoch:  16 	 Loss:  0.02689085528254509
Epoch:  17 	 Loss:  0.027269156649708748
Epoch:  18 	 Loss:  0.027172472327947617
Epoch:  19 	 Loss:  0.027181211858987808
Epoch:  20 	 Loss:  0.026998134329915047
Epoch:  21 	 Loss:  0.02685515768826008
Epoch:  22 	 Loss:  0.02673327922821045
Epoch:  23 	 Loss:  0.027458420023322105
Epoch:  24 	 Loss:  0.027373740449547768
Epoch:  25 	 Loss:  0.02697252668440342
Epoch:  26 	 Loss:  0.02691100724041462
Epoch:  27 	 Loss:  0.026858223602175713
Epoch:  28 	 Loss:  0.02673017419874668
Epoch:  29 	 Loss:  0.026979446411132812
Epoch:  30 	 Loss:  0.027543773874640465
Epoch:  31 	 Loss:  0.026795662939548492
Epoch:  32 	 Loss:  0.026969928294420242
Epoch:  33 	 Loss:  0.027295148000121117
Epoch:  34 	 Loss:  0.02687222696840763
Epoch:  35 	 Loss:  0.027361836284399033
Epoch:  36 	 Loss:  0.027156498283147812
Epoch:  37 	 Loss:  0.02715614065527916
Epoch:  38 	 Loss:  0.02733202464878559
Epoch:  39 	 Loss:  0.026803895831108093
Epoch:  40 	 Loss:  0.02702208235859871
Epoch:  41 	 Loss:  0.027223976328969002
Epoch:  42 	 Loss:  0.02675769478082657
Epoch:  43 	 Loss:  0.02715003862977028
Epoch:  44 	 Loss:  0.027223652228713036
Epoch:  45 	 Loss:  0.026567138731479645
Epoch:  46 	 Loss:  0.026907600462436676
Epoch:  47 	 Loss:  0.027235426008701324
Epoch:  48 	 Loss:  0.027304556220769882
Epoch:  49 	 Loss:  0.027017807587981224
Epoch:  50 	 Loss:  0.02695336565375328
Epoch:  51 	 Loss:  0.026806451380252838
Epoch:  52 	 Loss:  0.027017423883080482
Epoch:  53 	 Loss:  0.026923682540655136
Epoch:  54 	 Loss:  0.027055615559220314
Epoch:  55 	 Loss:  0.027067936956882477
Epoch:  56 	 Loss:  0.02686820924282074
Epoch:  57 	 Loss:  0.026775158941745758
Epoch:  58 	 Loss:  0.027190353721380234
Epoch:  59 	 Loss:  0.026771431788802147
Epoch:  60 	 Loss:  0.02708817832171917
Epoch:  61 	 Loss:  0.02696448564529419
Epoch:  62 	 Loss:  0.02685590833425522
Epoch:  63 	 Loss:  0.02706313505768776
Epoch:  64 	 Loss:  0.027215296402573586
Epoch:  65 	 Loss:  0.02703939564526081
Epoch:  66 	 Loss:  0.02695716731250286
Epoch:  67 	 Loss:  0.02698315493762493
Epoch:  68 	 Loss:  0.027182849124073982
Epoch:  69 	 Loss:  0.027088802307844162
Epoch:  70 	 Loss:  0.02707570418715477
Epoch:  71 	 Loss:  0.027003595605492592
Epoch:  72 	 Loss:  0.026819154620170593
Epoch:  73 	 Loss:  0.027002686634659767
Epoch:  74 	 Loss:  0.026803886517882347
Epoch:  75 	 Loss:  0.026938864961266518
Epoch:  76 	 Loss:  0.027310026809573174
Epoch:  77 	 Loss:  0.027262408286333084
Epoch:  78 	 Loss:  0.026848582550883293
Epoch:  79 	 Loss:  0.026872236281633377
Epoch:  80 	 Loss:  0.02730659395456314
Epoch:  81 	 Loss:  0.02679034322500229
Epoch:  82 	 Loss:  0.02683556079864502
Epoch:  83 	 Loss:  0.026958024129271507
Epoch:  84 	 Loss:  0.02720499038696289
Epoch:  85 	 Loss:  0.02657819725573063
Epoch:  86 	 Loss:  0.02689356543123722
Epoch:  87 	 Loss:  0.02699066512286663
Epoch:  88 	 Loss:  0.027089376002550125
Epoch:  89 	 Loss:  0.02679244428873062
Epoch:  90 	 Loss:  0.026564570143818855
Epoch:  91 	 Loss:  0.02701672725379467
Epoch:  92 	 Loss:  0.027173930779099464
Epoch:  93 	 Loss:  0.027538444846868515
Epoch:  94 	 Loss:  0.0274343341588974
Epoch:  95 	 Loss:  0.027101406827569008
Epoch:  96 	 Loss:  0.026948625221848488
Epoch:  97 	 Loss:  0.0273915845900774
Epoch:  98 	 Loss:  0.02660898119211197
Epoch:  99 	 Loss:  0.026712380349636078
Epoch:  100 	 Loss:  0.02699594758450985
wandb: Waiting for W&B process to finish, PID 1859264
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155228-uk1gf3ij/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155228-uk1gf3ij/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.027
wandb:        _step 800000
wandb:     _runtime 72
wandb:   _timestamp 1612904020
wandb:         loss 0.027
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–„â–„â–â–„â–…â–„â–†â–ƒâ–ˆâ–„â–‚â–ƒâ–ƒâ–†â–ƒâ–†â–†â–„â–…â–ƒâ–…â–ƒâ–ƒâ–„â–†â–„â–…â–ƒâ–ƒâ–†â–ƒâ–ƒâ–†â–„â–ƒâ–†â–ˆâ–‡â–„
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced lilac-sweep-66: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/uk1gf3ij
wandb: Agent Starting Run: yzsqt8bf with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:53:46.366589: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:53:46.372272: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run sage-sweep-67
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/yzsqt8bf
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_155344-yzsqt8bf
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02697969786822796
Epoch:  2 	 Loss:  0.026918914169073105
Epoch:  3 	 Loss:  0.026517460122704506
Epoch:  4 	 Loss:  0.027319766581058502
Epoch:  5 	 Loss:  0.027260445058345795
Epoch:  6 	 Loss:  0.027318807318806648
Epoch:  7 	 Loss:  0.02702765166759491
Epoch:  8 	 Loss:  0.02747095562517643
Epoch:  9 	 Loss:  0.02708478458225727
Epoch:  10 	 Loss:  0.026920463889837265
Epoch:  11 	 Loss:  0.02765922248363495
Epoch:  12 	 Loss:  0.027319233864545822
Epoch:  13 	 Loss:  0.027282480150461197
Epoch:  14 	 Loss:  0.02698609046638012
Epoch:  15 	 Loss:  0.02722659334540367
Epoch:  16 	 Loss:  0.027373872697353363
Epoch:  17 	 Loss:  0.02713877335190773
Epoch:  18 	 Loss:  0.02715258300304413
Epoch:  19 	 Loss:  0.027437414973974228
Epoch:  20 	 Loss:  0.027273839339613914
Epoch:  21 	 Loss:  0.02678796462714672
Epoch:  22 	 Loss:  0.027379201725125313
Epoch:  23 	 Loss:  0.0271854717284441
Epoch:  24 	 Loss:  0.027322806417942047
Epoch:  25 	 Loss:  0.027658196166157722
Epoch:  26 	 Loss:  0.02709854207932949
Epoch:  27 	 Loss:  0.02711060829460621
Epoch:  28 	 Loss:  0.02743416652083397
Epoch:  29 	 Loss:  0.027100663632154465
Epoch:  30 	 Loss:  0.02720615826547146
Epoch:  31 	 Loss:  0.027079802006483078
Epoch:  32 	 Loss:  0.027811294421553612
Epoch:  33 	 Loss:  0.027580279856920242
Epoch:  34 	 Loss:  0.027653926983475685
Epoch:  35 	 Loss:  0.02735835872590542
Epoch:  36 	 Loss:  0.02706412971019745
Epoch:  37 	 Loss:  0.027244489639997482
Epoch:  38 	 Loss:  0.027233688160777092
Epoch:  39 	 Loss:  0.027628477662801743
Epoch:  40 	 Loss:  0.027281127870082855
Epoch:  41 	 Loss:  0.027111433446407318
Epoch:  42 	 Loss:  0.027344144880771637
Epoch:  43 	 Loss:  0.02701290138065815
Epoch:  44 	 Loss:  0.027170974761247635
Epoch:  45 	 Loss:  0.02713203988969326
Epoch:  46 	 Loss:  0.02727341093122959
Epoch:  47 	 Loss:  0.026404650881886482
Epoch:  48 	 Loss:  0.026644064113497734
Epoch:  49 	 Loss:  0.027296768501400948
Epoch:  50 	 Loss:  0.027161763980984688
Epoch:  51 	 Loss:  0.02666907012462616
Epoch:  52 	 Loss:  0.027479492127895355
Epoch:  53 	 Loss:  0.02757679484784603
Epoch:  54 	 Loss:  0.02768162079155445
Epoch:  55 	 Loss:  0.027291426435112953
Epoch:  56 	 Loss:  0.027629582211375237
Epoch:  57 	 Loss:  0.027527987957000732
Epoch:  58 	 Loss:  0.02693270705640316
Epoch:  59 	 Loss:  0.027069205418229103
Epoch:  60 	 Loss:  0.027608884498476982
Epoch:  61 	 Loss:  0.02710598334670067
Epoch:  62 	 Loss:  0.027231009677052498
Epoch:  63 	 Loss:  0.02724633738398552
Epoch:  64 	 Loss:  0.02757466398179531
Epoch:  65 	 Loss:  0.027082499116659164
Epoch:  66 	 Loss:  0.026925303041934967
Epoch:  67 	 Loss:  0.02718687616288662
Epoch:  68 	 Loss:  0.027477428317070007
Epoch:  69 	 Loss:  0.02769060805439949
Epoch:  70 	 Loss:  0.02680126391351223
Epoch:  71 	 Loss:  0.027480734512209892
Epoch:  72 	 Loss:  0.02729063853621483
Epoch:  73 	 Loss:  0.02752618119120598
Epoch:  74 	 Loss:  0.027247153222560883
Epoch:  75 	 Loss:  0.027352891862392426
Epoch:  76 	 Loss:  0.026969807222485542
Epoch:  77 	 Loss:  0.026926176622509956
Epoch:  78 	 Loss:  0.027254149317741394
Epoch:  79 	 Loss:  0.027128206565976143
Epoch:  80 	 Loss:  0.027345754206180573
Epoch:  81 	 Loss:  0.02722897008061409
Epoch:  82 	 Loss:  0.02775571495294571
Epoch:  83 	 Loss:  0.026819581165909767
Epoch:  84 	 Loss:  0.026899006217718124
Epoch:  85 	 Loss:  0.026980029419064522
Epoch:  86 	 Loss:  0.0271268580108881
Epoch:  87 	 Loss:  0.027334950864315033
Epoch:  88 	 Loss:  0.027381282299757004
Epoch:  89 	 Loss:  0.02732524834573269
Epoch:  90 	 Loss:  0.02759929746389389
Epoch:  91 	 Loss:  0.02736148051917553
Epoch:  92 	 Loss:  0.02712979167699814
Epoch:  93 	 Loss:  0.027358369901776314
Epoch:  94 	 Loss:  0.027225984260439873
Epoch:  95 	 Loss:  0.027036506682634354
Epoch:  96 	 Loss:  0.027377674356102943
Epoch:  97 	 Loss:  0.02684256248176098
Epoch:  98 	 Loss:  0.02730928361415863
Epoch:  99 	 Loss:  0.027403276413679123
Epoch:  100 	 Loss:  0.0277449581772089
wandb: Waiting for W&B process to finish, PID 1862939
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155344-yzsqt8bf/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155344-yzsqt8bf/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02774
wandb:        _step 800000
wandb:     _runtime 72
wandb:   _timestamp 1612904096
wandb:         loss 0.02774
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–„â–â–†â–†â–‡â–…â–†â–…â–ƒâ–…â–„â–†â–„â–‡â–„â–‡â–„â–…â–…â–…â–‚â–ˆâ–‡â–„â–„â–‡â–…â–ˆâ–…â–…â–ƒâ–„â–ˆâ–ƒâ–†â–†â–„â–…â–ƒâ–ˆ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced sage-sweep-67: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/yzsqt8bf
wandb: Agent Starting Run: drdxfy7o with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: sgd
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:55:02.248495: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:55:02.253652: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run likely-sweep-68
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/drdxfy7o
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_155500-drdxfy7o
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.026994027197360992
Epoch:  2 	 Loss:  0.026956891641020775
Epoch:  3 	 Loss:  0.026561593636870384
Epoch:  4 	 Loss:  0.026595229282975197
Epoch:  5 	 Loss:  0.0263704601675272
Epoch:  6 	 Loss:  0.026440726593136787
Epoch:  7 	 Loss:  0.026608046144247055
Epoch:  8 	 Loss:  0.02660536766052246
Epoch:  9 	 Loss:  0.02661980502307415
Epoch:  10 	 Loss:  0.026888852939009666
Epoch:  11 	 Loss:  0.026423849165439606
Epoch:  12 	 Loss:  0.026720091700553894
Epoch:  13 	 Loss:  0.026551567018032074
Epoch:  14 	 Loss:  0.02624722197651863
Epoch:  15 	 Loss:  0.02657048963010311
Epoch:  16 	 Loss:  0.026728134602308273
Epoch:  17 	 Loss:  0.027133803814649582
Epoch:  18 	 Loss:  0.026870684698224068
Epoch:  19 	 Loss:  0.027107715606689453
Epoch:  20 	 Loss:  0.026584094390273094
Epoch:  21 	 Loss:  0.026540379971265793
Epoch:  22 	 Loss:  0.026762250810861588
Epoch:  23 	 Loss:  0.02666502632200718
Epoch:  24 	 Loss:  0.026730066165328026
Epoch:  25 	 Loss:  0.026859857141971588
Epoch:  26 	 Loss:  0.02666294574737549
Epoch:  27 	 Loss:  0.02698015235364437
Epoch:  28 	 Loss:  0.026640474796295166
Epoch:  29 	 Loss:  0.026750601828098297
Epoch:  30 	 Loss:  0.026867007836699486
Epoch:  31 	 Loss:  0.02677215449512005
Epoch:  32 	 Loss:  0.02637637034058571
Epoch:  33 	 Loss:  0.026254788041114807
Epoch:  34 	 Loss:  0.026588622480630875
Epoch:  35 	 Loss:  0.026784611865878105
Epoch:  36 	 Loss:  0.02660098858177662
Epoch:  37 	 Loss:  0.02682618796825409
Epoch:  38 	 Loss:  0.027152566239237785
Epoch:  39 	 Loss:  0.026663808152079582
Epoch:  40 	 Loss:  0.02687859535217285
Epoch:  41 	 Loss:  0.026455411687493324
Epoch:  42 	 Loss:  0.026885321363806725
Epoch:  43 	 Loss:  0.02655293419957161
Epoch:  44 	 Loss:  0.026733046397566795
Epoch:  45 	 Loss:  0.026742810383439064
Epoch:  46 	 Loss:  0.02673819288611412
Epoch:  47 	 Loss:  0.02685842476785183
Epoch:  48 	 Loss:  0.02717643603682518
Epoch:  49 	 Loss:  0.026768093928694725
Epoch:  50 	 Loss:  0.02682175114750862
Epoch:  51 	 Loss:  0.026680588722229004
Epoch:  52 	 Loss:  0.026773549616336823
Epoch:  53 	 Loss:  0.027170157060027122
Epoch:  54 	 Loss:  0.026640716940164566
Epoch:  55 	 Loss:  0.026831526309251785
Epoch:  56 	 Loss:  0.02659989707171917
Epoch:  57 	 Loss:  0.02682465687394142
Epoch:  58 	 Loss:  0.02673659287393093
Epoch:  59 	 Loss:  0.026725919917225838
Epoch:  60 	 Loss:  0.026831595227122307
Epoch:  61 	 Loss:  0.02722136303782463
Epoch:  62 	 Loss:  0.026690589264035225
Epoch:  63 	 Loss:  0.026267101988196373
Epoch:  64 	 Loss:  0.026752114295959473
Epoch:  65 	 Loss:  0.026717936620116234
Epoch:  66 	 Loss:  0.02640083245933056
Epoch:  67 	 Loss:  0.026511218398809433
Epoch:  68 	 Loss:  0.02680133283138275
Epoch:  69 	 Loss:  0.026780875399708748
Epoch:  70 	 Loss:  0.02680552564561367
Epoch:  71 	 Loss:  0.026478830724954605
Epoch:  72 	 Loss:  0.026773866266012192
Epoch:  73 	 Loss:  0.026904746890068054
Epoch:  74 	 Loss:  0.027057478204369545
Epoch:  75 	 Loss:  0.02679399400949478
Epoch:  76 	 Loss:  0.026622414588928223
Epoch:  77 	 Loss:  0.026877528056502342
Epoch:  78 	 Loss:  0.026682164520025253
Epoch:  79 	 Loss:  0.027072662487626076
Epoch:  80 	 Loss:  0.027074461802840233
Epoch:  81 	 Loss:  0.026476098224520683
Epoch:  82 	 Loss:  0.026730820536613464
Epoch:  83 	 Loss:  0.026514405384659767
Epoch:  84 	 Loss:  0.026390204206109047
Epoch:  85 	 Loss:  0.0271683931350708
Epoch:  86 	 Loss:  0.026787055656313896
Epoch:  87 	 Loss:  0.027157828211784363
Epoch:  88 	 Loss:  0.026854533702135086
Epoch:  89 	 Loss:  0.026807637885212898
Epoch:  90 	 Loss:  0.026795443147420883
Epoch:  91 	 Loss:  0.026720350608229637
Epoch:  92 	 Loss:  0.02678445540368557
Epoch:  93 	 Loss:  0.026752660050988197
Epoch:  94 	 Loss:  0.026723599061369896
Epoch:  95 	 Loss:  0.026474837213754654
Epoch:  96 	 Loss:  0.026882147416472435
Epoch:  97 	 Loss:  0.026736905798316002
Epoch:  98 	 Loss:  0.026755984872579575
Epoch:  99 	 Loss:  0.026985827833414078
Epoch:  100 	 Loss:  0.026554768905043602
wandb: Waiting for W&B process to finish, PID 1866609
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155500-drdxfy7o/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155500-drdxfy7o/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02655
wandb:        _step 800000
wandb:     _runtime 71
wandb:   _timestamp 1612904171
wandb:         loss 0.02655
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–‚â–â–ƒâ–â–‚â–„â–…â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ˆâ–„â–‚â–„â–„â–‡â–…â–‡â–„â–â–‡â–…â–„â–„â–„â–‚
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced likely-sweep-68: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/drdxfy7o
wandb: Agent Starting Run: alsdstpl with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:56:17.770529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:56:17.775962: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run usual-sweep-69
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/alsdstpl
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_155615-alsdstpl
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02719510905444622
Epoch:  2 	 Loss:  0.026840118691325188
Epoch:  3 	 Loss:  0.0269098412245512
Epoch:  4 	 Loss:  0.027170564979314804
Epoch:  5 	 Loss:  0.02716241404414177
Epoch:  6 	 Loss:  0.027162885293364525
Epoch:  7 	 Loss:  0.027117040008306503
Epoch:  8 	 Loss:  0.02703145332634449
Epoch:  9 	 Loss:  0.026784267276525497
Epoch:  10 	 Loss:  0.027208836749196053
Epoch:  11 	 Loss:  0.027076946571469307
Epoch:  12 	 Loss:  0.026753174141049385
Epoch:  13 	 Loss:  0.026981404051184654
Epoch:  14 	 Loss:  0.026754602789878845
Epoch:  15 	 Loss:  0.027182623744010925
Epoch:  16 	 Loss:  0.02724067121744156
Epoch:  17 	 Loss:  0.026933519169688225
Epoch:  18 	 Loss:  0.02702430635690689
Epoch:  19 	 Loss:  0.02681182324886322
Epoch:  20 	 Loss:  0.02693791128695011
Epoch:  21 	 Loss:  0.02713065966963768
Epoch:  22 	 Loss:  0.026916811242699623
Epoch:  23 	 Loss:  0.026610879227519035
Epoch:  24 	 Loss:  0.02652665786445141
Epoch:  25 	 Loss:  0.026908228173851967
Epoch:  26 	 Loss:  0.027259143069386482
Epoch:  27 	 Loss:  0.02691866271197796
Epoch:  28 	 Loss:  0.027059849351644516
Epoch:  29 	 Loss:  0.027100618928670883
Epoch:  30 	 Loss:  0.0269736610352993
Epoch:  31 	 Loss:  0.027180882170796394
Epoch:  32 	 Loss:  0.026926714926958084
Epoch:  33 	 Loss:  0.026793960481882095
Epoch:  34 	 Loss:  0.027237052097916603
Epoch:  35 	 Loss:  0.027296176180243492
Epoch:  36 	 Loss:  0.027308711782097816
Epoch:  37 	 Loss:  0.026820100843906403
Epoch:  38 	 Loss:  0.027081506326794624
Epoch:  39 	 Loss:  0.02732529118657112
Epoch:  40 	 Loss:  0.02648303098976612
Epoch:  41 	 Loss:  0.02697327546775341
Epoch:  42 	 Loss:  0.02660188265144825
Epoch:  43 	 Loss:  0.027334759011864662
Epoch:  44 	 Loss:  0.026658227667212486
Epoch:  45 	 Loss:  0.027386317029595375
Epoch:  46 	 Loss:  0.02702370099723339
Epoch:  47 	 Loss:  0.026950497180223465
Epoch:  48 	 Loss:  0.02702883444726467
Epoch:  49 	 Loss:  0.027059737592935562
Epoch:  50 	 Loss:  0.027206888422369957
Epoch:  51 	 Loss:  0.027194278314709663
Epoch:  52 	 Loss:  0.026660731062293053
Epoch:  53 	 Loss:  0.027027731761336327
Epoch:  54 	 Loss:  0.02691434510052204
Epoch:  55 	 Loss:  0.026802727952599525
Epoch:  56 	 Loss:  0.027075061574578285
Epoch:  57 	 Loss:  0.027064461261034012
Epoch:  58 	 Loss:  0.02719498611986637
Epoch:  59 	 Loss:  0.02716541662812233
Epoch:  60 	 Loss:  0.026747120544314384
Epoch:  61 	 Loss:  0.027157219126820564
Epoch:  62 	 Loss:  0.02720530517399311
Epoch:  63 	 Loss:  0.027011828497052193
Epoch:  64 	 Loss:  0.027225591242313385
Epoch:  65 	 Loss:  0.02725672721862793
Epoch:  66 	 Loss:  0.02722974866628647
Epoch:  67 	 Loss:  0.02704765275120735
Epoch:  68 	 Loss:  0.026530398055911064
Epoch:  69 	 Loss:  0.026534469798207283
Epoch:  70 	 Loss:  0.02679235115647316
Epoch:  71 	 Loss:  0.02670861966907978
Epoch:  72 	 Loss:  0.026923377066850662
Epoch:  73 	 Loss:  0.02715918980538845
Epoch:  74 	 Loss:  0.026781320571899414
Epoch:  75 	 Loss:  0.026822781190276146
Epoch:  76 	 Loss:  0.026974596083164215
Epoch:  77 	 Loss:  0.02735525742173195
Epoch:  78 	 Loss:  0.027088366448879242
Epoch:  79 	 Loss:  0.026766981929540634
Epoch:  80 	 Loss:  0.02686324343085289
Epoch:  81 	 Loss:  0.026812678202986717
Epoch:  82 	 Loss:  0.02666686661541462
Epoch:  83 	 Loss:  0.02710968628525734
Epoch:  84 	 Loss:  0.02716805227100849
Epoch:  85 	 Loss:  0.02693665586411953
Epoch:  86 	 Loss:  0.027005570009350777
Epoch:  87 	 Loss:  0.02718537114560604
Epoch:  88 	 Loss:  0.026922738179564476
Epoch:  89 	 Loss:  0.0271440502256155
Epoch:  90 	 Loss:  0.026893634349107742
Epoch:  91 	 Loss:  0.02662927843630314
Epoch:  92 	 Loss:  0.026482563465833664
Epoch:  93 	 Loss:  0.026803698390722275
Epoch:  94 	 Loss:  0.026907246559858322
Epoch:  95 	 Loss:  0.026913022622466087
Epoch:  96 	 Loss:  0.02690986357629299
Epoch:  97 	 Loss:  0.02706981636583805
Epoch:  98 	 Loss:  0.02697231061756611
Epoch:  99 	 Loss:  0.027374468743801117
Epoch:  100 	 Loss:  0.027426399290561676
wandb: Waiting for W&B process to finish, PID 1870280
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155615-alsdstpl/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155615-alsdstpl/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02743
wandb:        _step 800000
wandb:     _runtime 74
wandb:   _timestamp 1612904250
wandb:         loss 0.02743
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–„â–†â–…â–…â–…â–‡â–…â–†â–‚â–‡â–…â–†â–‡â–‡â–‡â–…â–‚â–…â–…â–†â–„â–…â–†â–†â–‡â–…â–â–„â–ƒâ–‡â–ƒâ–‚â–†â–†â–†â–â–„â–…â–ˆ
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced usual-sweep-69: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/alsdstpl
wandb: Agent Starting Run: cqanrqie with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.0005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:57:36.812926: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:57:36.818521: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run fast-sweep-70
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/cqanrqie
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_155735-cqanrqie
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.02647658996284008
Epoch:  2 	 Loss:  0.026286736130714417
Epoch:  3 	 Loss:  0.026036664843559265
Epoch:  4 	 Loss:  0.026121528819203377
Epoch:  5 	 Loss:  0.026770705357193947
Epoch:  6 	 Loss:  0.026415815576910973
Epoch:  7 	 Loss:  0.02664848417043686
Epoch:  8 	 Loss:  0.026344092562794685
Epoch:  9 	 Loss:  0.026468414813280106
Epoch:  10 	 Loss:  0.02629467099905014
Epoch:  11 	 Loss:  0.02600759081542492
Epoch:  12 	 Loss:  0.026624556630849838
Epoch:  13 	 Loss:  0.026739459484815598
Epoch:  14 	 Loss:  0.026390327140688896
Epoch:  15 	 Loss:  0.026503926143050194
Epoch:  16 	 Loss:  0.026297027245163918
Epoch:  17 	 Loss:  0.026224752888083458
Epoch:  18 	 Loss:  0.026268064975738525
Epoch:  19 	 Loss:  0.026430394500494003
Epoch:  20 	 Loss:  0.026401488110423088
Epoch:  21 	 Loss:  0.026552392169833183
Epoch:  22 	 Loss:  0.02592197246849537
Epoch:  23 	 Loss:  0.026418955996632576
Epoch:  24 	 Loss:  0.02618463523685932
Epoch:  25 	 Loss:  0.026254868134856224
Epoch:  26 	 Loss:  0.02639414742588997
Epoch:  27 	 Loss:  0.026335863396525383
Epoch:  28 	 Loss:  0.02645088918507099
Epoch:  29 	 Loss:  0.02679489180445671
Epoch:  30 	 Loss:  0.02634785696864128
Epoch:  31 	 Loss:  0.026489948853850365
Epoch:  32 	 Loss:  0.02651154436171055
Epoch:  33 	 Loss:  0.026140818372368813
Epoch:  34 	 Loss:  0.02621571160852909
Epoch:  35 	 Loss:  0.026363395154476166
Epoch:  36 	 Loss:  0.02670258656144142
Epoch:  37 	 Loss:  0.02620875835418701
Epoch:  38 	 Loss:  0.026298558339476585
Epoch:  39 	 Loss:  0.026573369279503822
Epoch:  40 	 Loss:  0.02610268071293831
Epoch:  41 	 Loss:  0.026359256356954575
Epoch:  42 	 Loss:  0.026819348335266113
Epoch:  43 	 Loss:  0.02645215019583702
Epoch:  44 	 Loss:  0.026297949254512787
Epoch:  45 	 Loss:  0.02646913006901741
Epoch:  46 	 Loss:  0.02623947151005268
Epoch:  47 	 Loss:  0.026555733755230904
Epoch:  48 	 Loss:  0.026500096544623375
Epoch:  49 	 Loss:  0.02651980146765709
Epoch:  50 	 Loss:  0.026583360508084297
Epoch:  51 	 Loss:  0.02644108235836029
Epoch:  52 	 Loss:  0.026206033304333687
Epoch:  53 	 Loss:  0.02663540281355381
Epoch:  54 	 Loss:  0.026218226179480553
Epoch:  55 	 Loss:  0.026240268722176552
Epoch:  56 	 Loss:  0.026459025219082832
Epoch:  57 	 Loss:  0.026512347161769867
Epoch:  58 	 Loss:  0.026242082938551903
Epoch:  59 	 Loss:  0.026645775884389877
Epoch:  60 	 Loss:  0.02636835165321827
Epoch:  61 	 Loss:  0.026666443794965744
Epoch:  62 	 Loss:  0.026345014572143555
Epoch:  63 	 Loss:  0.02620426006615162
Epoch:  64 	 Loss:  0.026093726977705956
Epoch:  65 	 Loss:  0.0264926515519619
Epoch:  66 	 Loss:  0.026570558547973633
Epoch:  67 	 Loss:  0.026120562106370926
Epoch:  68 	 Loss:  0.025986092165112495
Epoch:  69 	 Loss:  0.02648223377764225
Epoch:  70 	 Loss:  0.026637163013219833
Epoch:  71 	 Loss:  0.02645784243941307
Epoch:  72 	 Loss:  0.026448896154761314
Epoch:  73 	 Loss:  0.026252640411257744
Epoch:  74 	 Loss:  0.026654010638594627
Epoch:  75 	 Loss:  0.026708172634243965
Epoch:  76 	 Loss:  0.026665590703487396
Epoch:  77 	 Loss:  0.026192205026745796
Epoch:  78 	 Loss:  0.026233237236738205
Epoch:  79 	 Loss:  0.02633453719317913
Epoch:  80 	 Loss:  0.026464741677045822
Epoch:  81 	 Loss:  0.026285888627171516
Epoch:  82 	 Loss:  0.02643922157585621
Epoch:  83 	 Loss:  0.026550889015197754
Epoch:  84 	 Loss:  0.026483705267310143
Epoch:  85 	 Loss:  0.026463979855179787
Epoch:  86 	 Loss:  0.026187758892774582
Epoch:  87 	 Loss:  0.026381351053714752
Epoch:  88 	 Loss:  0.02638373337686062
Epoch:  89 	 Loss:  0.026154858991503716
Epoch:  90 	 Loss:  0.026630191132426262
Epoch:  91 	 Loss:  0.025859855115413666
Epoch:  92 	 Loss:  0.026647087186574936
Epoch:  93 	 Loss:  0.026691308245062828
Epoch:  94 	 Loss:  0.02626657299697399
Epoch:  95 	 Loss:  0.02645028941333294
Epoch:  96 	 Loss:  0.026583131402730942
Epoch:  97 	 Loss:  0.026551686227321625
Epoch:  98 	 Loss:  0.02628522552549839
Epoch:  99 	 Loss:  0.026376646012067795
Epoch:  100 	 Loss:  0.026399699971079826
wandb: Waiting for W&B process to finish, PID 1873956
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155735-cqanrqie/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155735-cqanrqie/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.0264
wandb:        _step 800000
wandb:     _runtime 76
wandb:   _timestamp 1612904331
wandb:         loss 0.0264
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–…â–â–…â–„â–â–ˆâ–„â–ƒâ–†â–…â–…â–…â–†â–ƒâ–ˆâ–†â–„â–„â–ƒâ–†â–…â–ƒâ–…â–‡â–‡â–‚â–‚â–†â–…â–‡â–ƒâ–„â–…â–†â–…â–‚â–‡â–ƒâ–†â–…
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced fast-sweep-70: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/cqanrqie
wandb: Agent Starting Run: 5pkasd9f with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.005
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 15:58:57.564780: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 15:58:57.570576: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run exalted-sweep-71
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/5pkasd9f
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_155855-5pkasd9f
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.026462191715836525
Epoch:  2 	 Loss:  0.026811176910996437
Epoch:  3 	 Loss:  0.026805078610777855
Epoch:  4 	 Loss:  0.026632679626345634
Epoch:  5 	 Loss:  0.02683120034635067
Epoch:  6 	 Loss:  0.026695242151618004
Epoch:  7 	 Loss:  0.026975538581609726
Epoch:  8 	 Loss:  0.026818787679076195
Epoch:  9 	 Loss:  0.02678241953253746
Epoch:  10 	 Loss:  0.02741476148366928
Epoch:  11 	 Loss:  0.027115758508443832
Epoch:  12 	 Loss:  0.027071719989180565
Epoch:  13 	 Loss:  0.02711513265967369
Epoch:  14 	 Loss:  0.026840493083000183
Epoch:  15 	 Loss:  0.02707214467227459
Epoch:  16 	 Loss:  0.026816418394446373
Epoch:  17 	 Loss:  0.027066545560956
Epoch:  18 	 Loss:  0.026635155081748962
Epoch:  19 	 Loss:  0.027028121054172516
Epoch:  20 	 Loss:  0.02684430405497551
Epoch:  21 	 Loss:  0.026721423491835594
Epoch:  22 	 Loss:  0.026810582727193832
Epoch:  23 	 Loss:  0.026847893372178078
Epoch:  24 	 Loss:  0.02695569023489952
Epoch:  25 	 Loss:  0.02679058536887169
Epoch:  26 	 Loss:  0.026705196127295494
Epoch:  27 	 Loss:  0.026732321828603745
Epoch:  28 	 Loss:  0.02681611105799675
Epoch:  29 	 Loss:  0.026595192030072212
Epoch:  30 	 Loss:  0.02636120840907097
Epoch:  31 	 Loss:  0.026664551347494125
Epoch:  32 	 Loss:  0.0267177727073431
Epoch:  33 	 Loss:  0.02714051678776741
Epoch:  34 	 Loss:  0.0266397874802351
Epoch:  35 	 Loss:  0.026859845966100693
Epoch:  36 	 Loss:  0.02737313136458397
Epoch:  37 	 Loss:  0.02682330831885338
Epoch:  38 	 Loss:  0.027004612609744072
Epoch:  39 	 Loss:  0.026891645044088364
Epoch:  40 	 Loss:  0.027284465730190277
Epoch:  41 	 Loss:  0.02676534652709961
Epoch:  42 	 Loss:  0.026757510378956795
Epoch:  43 	 Loss:  0.026605922728776932
Epoch:  44 	 Loss:  0.02660706825554371
Epoch:  45 	 Loss:  0.026631593704223633
Epoch:  46 	 Loss:  0.026832900941371918
Epoch:  47 	 Loss:  0.027040407061576843
Epoch:  48 	 Loss:  0.02656358852982521
Epoch:  49 	 Loss:  0.0271451473236084
Epoch:  50 	 Loss:  0.026856211945414543
Epoch:  51 	 Loss:  0.02698608860373497
Epoch:  52 	 Loss:  0.02733021043241024
Epoch:  53 	 Loss:  0.026922639459371567
Epoch:  54 	 Loss:  0.026946093887090683
Epoch:  55 	 Loss:  0.0268497746437788
Epoch:  56 	 Loss:  0.02682846412062645
Epoch:  57 	 Loss:  0.026857485994696617
Epoch:  58 	 Loss:  0.026643352583050728
Epoch:  59 	 Loss:  0.027205009013414383
Epoch:  60 	 Loss:  0.027253465726971626
Epoch:  61 	 Loss:  0.02671622298657894
Epoch:  62 	 Loss:  0.026861868798732758
Epoch:  63 	 Loss:  0.026989487931132317
Epoch:  64 	 Loss:  0.026880042627453804
Epoch:  65 	 Loss:  0.0275450199842453
Epoch:  66 	 Loss:  0.02675689570605755
Epoch:  67 	 Loss:  0.026987656950950623
Epoch:  68 	 Loss:  0.026914728805422783
Epoch:  69 	 Loss:  0.02663714624941349
Epoch:  70 	 Loss:  0.026787588372826576
Epoch:  71 	 Loss:  0.02706412971019745
Epoch:  72 	 Loss:  0.02660459652543068
Epoch:  73 	 Loss:  0.027129556983709335
Epoch:  74 	 Loss:  0.027071485295891762
Epoch:  75 	 Loss:  0.026877811178565025
Epoch:  76 	 Loss:  0.02668960578739643
Epoch:  77 	 Loss:  0.02698986791074276
Epoch:  78 	 Loss:  0.026575760915875435
Epoch:  79 	 Loss:  0.02717399038374424
Epoch:  80 	 Loss:  0.027128690853714943
Epoch:  81 	 Loss:  0.02727569453418255
Epoch:  82 	 Loss:  0.026810921728610992
Epoch:  83 	 Loss:  0.027014896273612976
Epoch:  84 	 Loss:  0.026942318305373192
Epoch:  85 	 Loss:  0.026988647878170013
Epoch:  86 	 Loss:  0.026513520628213882
Epoch:  87 	 Loss:  0.026948051527142525
Epoch:  88 	 Loss:  0.026599232107400894
Epoch:  89 	 Loss:  0.02704767882823944
Epoch:  90 	 Loss:  0.026861341670155525
Epoch:  91 	 Loss:  0.027454398572444916
Epoch:  92 	 Loss:  0.02705475501716137
Epoch:  93 	 Loss:  0.026991015300154686
Epoch:  94 	 Loss:  0.026578715071082115
Epoch:  95 	 Loss:  0.026846233755350113
Epoch:  96 	 Loss:  0.026838527992367744
Epoch:  97 	 Loss:  0.02687101997435093
Epoch:  98 	 Loss:  0.027243787422776222
Epoch:  99 	 Loss:  0.026723403483629227
Epoch:  100 	 Loss:  0.02713451348245144
wandb: Waiting for W&B process to finish, PID 1877655
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155855-5pkasd9f/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_155855-5pkasd9f/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02713
wandb:        _step 800000
wandb:     _runtime 82
wandb:   _timestamp 1612904417
wandb:         loss 0.02713
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–â–„â–ƒâ–„â–†â–†â–„â–‚â–ƒâ–„â–ƒâ–„â–ƒâ–‚â–ˆâ–„â–ƒâ–‚â–„â–†â–…â–…â–„â–‡â–ƒâ–„â–…â–‚â–‚â–†â–…â–†â–„â–…â–…â–…â–†â–‚â–„â–†
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced exalted-sweep-71: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/5pkasd9f
wandb: Agent Starting Run: baepe2qq with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 128
wandb: 	n_nodes_fc2: 128
wandb: 	optimizer: rmsprop
wandb: 	weight_decay: 0.05
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 16:00:23.825292: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 16:00:23.830874: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run glad-sweep-72
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/baepe2qq
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_160022-baepe2qq
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.026657722890377045
Epoch:  2 	 Loss:  0.026580654084682465
Epoch:  3 	 Loss:  0.02627573534846306
Epoch:  4 	 Loss:  0.02644968591630459
Epoch:  5 	 Loss:  0.026697998866438866
Epoch:  6 	 Loss:  0.02631021849811077
Epoch:  7 	 Loss:  0.02633948065340519
Epoch:  8 	 Loss:  0.026717161759734154
Epoch:  9 	 Loss:  0.026223981752991676
Epoch:  10 	 Loss:  0.026526616886258125
Epoch:  11 	 Loss:  0.02671266533434391
Epoch:  12 	 Loss:  0.026608159765601158
Epoch:  13 	 Loss:  0.026518065482378006
Epoch:  14 	 Loss:  0.026472732424736023
Epoch:  15 	 Loss:  0.02658333256840706
Epoch:  16 	 Loss:  0.0267147459089756
Epoch:  17 	 Loss:  0.02640611305832863
Epoch:  18 	 Loss:  0.026552533730864525
Epoch:  19 	 Loss:  0.026689115911722183
Epoch:  20 	 Loss:  0.026516033336520195
Epoch:  21 	 Loss:  0.02661677449941635
Epoch:  22 	 Loss:  0.026495810598134995
Epoch:  23 	 Loss:  0.02660888433456421
Epoch:  24 	 Loss:  0.026556843891739845
Epoch:  25 	 Loss:  0.0260439682751894
Epoch:  26 	 Loss:  0.026846541091799736
Epoch:  27 	 Loss:  0.02613101527094841
Epoch:  28 	 Loss:  0.02639720030128956
Epoch:  29 	 Loss:  0.026765739545226097
Epoch:  30 	 Loss:  0.026679785922169685
Epoch:  31 	 Loss:  0.026211583986878395
Epoch:  32 	 Loss:  0.026571616530418396
Epoch:  33 	 Loss:  0.02597399801015854
Epoch:  34 	 Loss:  0.026570841670036316
Epoch:  35 	 Loss:  0.026567798107862473
Epoch:  36 	 Loss:  0.026173211634159088
Epoch:  37 	 Loss:  0.02649468556046486
Epoch:  38 	 Loss:  0.02653956413269043
Epoch:  39 	 Loss:  0.02629510685801506
Epoch:  40 	 Loss:  0.026441363617777824
Epoch:  41 	 Loss:  0.0265874732285738
Epoch:  42 	 Loss:  0.02641914412379265
Epoch:  43 	 Loss:  0.026824869215488434
Epoch:  44 	 Loss:  0.026876317337155342
Epoch:  45 	 Loss:  0.0265799630433321
Epoch:  46 	 Loss:  0.026453165337443352
Epoch:  47 	 Loss:  0.026747575029730797
Epoch:  48 	 Loss:  0.026379616931080818
Epoch:  49 	 Loss:  0.026755085214972496
Epoch:  50 	 Loss:  0.026430146768689156
Epoch:  51 	 Loss:  0.02653755620121956
Epoch:  52 	 Loss:  0.02649257332086563
Epoch:  53 	 Loss:  0.026485007256269455
Epoch:  54 	 Loss:  0.026569988578557968
Epoch:  55 	 Loss:  0.026018362492322922
Epoch:  56 	 Loss:  0.0263555608689785
Epoch:  57 	 Loss:  0.026387590914964676
Epoch:  58 	 Loss:  0.026591269299387932
Epoch:  59 	 Loss:  0.026400653645396233
Epoch:  60 	 Loss:  0.026637939736247063
Epoch:  61 	 Loss:  0.026575280353426933
Epoch:  62 	 Loss:  0.026638347655534744
Epoch:  63 	 Loss:  0.026384444907307625
Epoch:  64 	 Loss:  0.02637660503387451
Epoch:  65 	 Loss:  0.02614775486290455
Epoch:  66 	 Loss:  0.02653156779706478
Epoch:  67 	 Loss:  0.02640261873602867
Epoch:  68 	 Loss:  0.026502516120672226
Epoch:  69 	 Loss:  0.026456832885742188
Epoch:  70 	 Loss:  0.02664012648165226
Epoch:  71 	 Loss:  0.02658776193857193
Epoch:  72 	 Loss:  0.0264180526137352
Epoch:  73 	 Loss:  0.026947975158691406
Epoch:  74 	 Loss:  0.02660495974123478
Epoch:  75 	 Loss:  0.026287853717803955
Epoch:  76 	 Loss:  0.026485459879040718
Epoch:  77 	 Loss:  0.026528701186180115
Epoch:  78 	 Loss:  0.026305295526981354
Epoch:  79 	 Loss:  0.026554198935627937
Epoch:  80 	 Loss:  0.02664664015173912
Epoch:  81 	 Loss:  0.026203613728284836
Epoch:  82 	 Loss:  0.026685554534196854
Epoch:  83 	 Loss:  0.02650044485926628
Epoch:  84 	 Loss:  0.026242461055517197
Epoch:  85 	 Loss:  0.026655830442905426
Epoch:  86 	 Loss:  0.026599865406751633
Epoch:  87 	 Loss:  0.02656031958758831
Epoch:  88 	 Loss:  0.026181111112236977
Epoch:  89 	 Loss:  0.02631402015686035
Epoch:  90 	 Loss:  0.026532502844929695
Epoch:  91 	 Loss:  0.026512516662478447
Epoch:  92 	 Loss:  0.026340026408433914
Epoch:  93 	 Loss:  0.026241976767778397
Epoch:  94 	 Loss:  0.026577549055218697
Epoch:  95 	 Loss:  0.02657756768167019
Epoch:  96 	 Loss:  0.02626916952431202
Epoch:  97 	 Loss:  0.026543516665697098
Epoch:  98 	 Loss:  0.026723163202404976
Epoch:  99 	 Loss:  0.02637081779539585
Epoch:  100 	 Loss:  0.026762444525957108
wandb: Waiting for W&B process to finish, PID 1881351
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_160022-baepe2qq/logs/debug.log
wandb: Find internal logs for this run at: /home/naddeok5/its_always_sunny/wandb/run-20210209_160022-baepe2qq/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 99
wandb:   batch_loss 0.02676
wandb:        _step 800000
wandb:     _runtime 75
wandb:   _timestamp 1612904497
wandb:         loss 0.02676
wandb: Run history:
wandb:        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   batch_loss â–†â–‚â–‚â–†â–†â–„â–†â–…â–…â–…â–ˆâ–ƒâ–â–…â–â–‚â–…â–ˆâ–„â–‡â–…â–…â–ƒâ–ƒâ–…â–ƒâ–ƒâ–„â–ƒâ–…â–…â–…â–†â–‚â–…â–‚â–ƒâ–…â–…â–‡
wandb:        _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         loss â–
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced glad-sweep-72: https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/baepe2qq
wandb: Agent Starting Run: axf4zsbk with config:
wandb: 	batch_size: 256
wandb: 	embedding_size: 8
wandb: 	epochs: 100
wandb: 	learning_rate: 1e-10
wandb: 	momentum: 0
wandb: 	n_nodes_fc1: 256
wandb: 	n_nodes_fc2: 32
wandb: 	optimizer: adam
wandb: 	weight_decay: 0
wandb: WARNING Ignored wandb.init() arg project when running a sweep
wandb: WARNING Ignored wandb.init() arg entity when running a sweep

wandb: wandb version 0.10.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
2021-02-09 16:01:43.399246: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2021-02-09 16:01:43.404702: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
wandb: Tracking run with wandb version 0.10.15
wandb: Syncing run good-sweep-73
wandb: â­ï¸ View project at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000
wandb: ğŸ§¹ View sweep at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/sweeps/2w567fe2
wandb: ğŸš€ View run at https://wandb.ai/naddeok/Year_AutoEnc_Trunc10000/runs/axf4zsbk
wandb: Run data is saved locally in /home/naddeok5/its_always_sunny/wandb/run-20210209_160141-axf4zsbk
wandb: Run `wandb offline` to turn off syncing.

Epoch:  1 	 Loss:  0.027546491473913193
Epoch:  2 	 Loss:  0.027640104293823242
Epoch:  3 	 Loss:  0.027159150689840317
Epoch:  4 	 Loss:  0.027445651590824127
Epoch:  5 	 Loss:  0.02698243223130703
Epoch:  6 	 Loss:  0.027337083593010902
Epoch:  7 	 Loss:  0.027781665325164795
Epoch:  8 	 Loss:  0.02704872377216816
Epoch:  9 	 Loss:  0.02773425541818142
Epoch:  10 	 Loss:  0.02754398062825203
Epoch:  11 	 Loss:  0.027226712554693222
Epoch:  12 	 Loss:  0.02709837444126606
Epoch:  13 	 Loss:  0.027348211035132408
Epoch:  14 	 Loss:  0.027723513543605804
Epoch:  15 	 Loss:  0.027384625747799873
Epoch:  16 	 Loss:  0.02769026719033718
Epoch:  17 	 Loss:  0.0274520106613636
Epoch:  18 	 Loss:  0.027433734387159348
Epoch:  19 	 Loss:  0.027564970776438713
Epoch:  20 	 Loss:  0.027552654966711998
Epoch:  21 	 Loss:  0.027729978784918785
Epoch:  22 	 Loss:  0.027208365499973297
Epoch:  23 	 Loss:  0.02671215496957302
Epoch:  24 	 Loss:  0.027663156390190125
Epoch:  25 	 Loss:  0.02732049487531185
Epoch:  26 	 Loss:  0.026902712881565094
Epoch:  27 	 Loss:  0.02719191275537014
Epoch:  28 	 Loss:  0.02714039757847786
Epoch:  29 	 Loss:  0.027597134932875633
Epoch:  30 	 Loss:  0.027035826817154884
Epoch:  31 	 Loss:  0.02729254961013794
Epoch:  32 	 Loss:  0.027751220390200615
Epoch:  33 	 Loss:  0.02702455408871174
Epoch:  34 	 Loss:  0.02721407823264599
Epoch:  35 	 Loss:  0.027497442439198494
Epoch:  36 	 Loss:  0.027646398171782494
Epoch:  37 	 Loss:  0.027786418795585632
Epoch:  38 	 Loss:  0.02734936587512493
Epoch:  39 	 Loss:  0.02756870724260807
Epoch:  40 	 Loss:  0.027737805619835854
Epoch:  41 	 Loss:  0.027304094284772873
Epoch:  42 	 Loss:  0.027338018640875816
Epoch:  43 	 Loss:  0.027647262439131737
Epoch:  44 	 Loss:  0.026903152465820312
Epoch:  45 	 Loss:  0.027261128649115562
Epoch:  46 	 Loss:  0.027445103973150253
Epoch:  47 	 Loss:  0.02733246050775051
Epoch:  48 	 Loss:  0.0278098676353693
Epoch:  49 	 Loss:  0.027604982256889343
Epoch:  50 	 Loss:  0.02751961164176464
Epoch:  51 	 Loss:  0.026949342340230942
Epoch:  52 	 Loss:  0.02717592567205429
Epoch:  53 	 Loss:  0.026957295835018158
Epoch:  54 	 Loss:  0.027561897411942482
Epoch:  55 	 Loss:  0.02702561765909195
Epoch:  56 	 Loss:  0.027408191934227943
Epoch:  57 	 Loss:  0.027365541085600853
Epoch:  58 	 Loss:  0.027501143515110016
Epoch:  59 	 Loss:  0.027569040656089783
Epoch:  60 	 Loss:  0.02782071940600872
Epoch:  61 	 Loss:  0.027237510308623314
Epoch:  62 	 Loss:  0.027423426508903503
Epoch:  63 	 Loss:  0.027329470962285995
Epoch:  64 	 Loss:  0.02727276086807251
Epoch:  65 	 Loss:  0.027723466977477074
Epoch:  66 	 Loss:  0.027060722932219505
Epoch:  67 	 Loss:  0.026979245245456696
Epoch:  68 	 Loss:  0.02717326208949089
Epoch:  69 	 Loss:  0.02763611078262329
Epoch:  70 	 Loss:  0.027003737166523933
Epoch:  71 	 Loss:  0.02704242430627346
Epoch:  72 	 Loss:  0.02746196836233139
Epoch:  73 	 Loss:  0.02706332877278328
Epoch:  74 	 Loss:  0.027301104739308357
Epoch:  75 	 Loss:  0.02745373733341694
Epoch:  76 	 Loss:  0.027640914544463158
Epoch:  77 	 Loss:  0.027099546045064926
Epoch:  78 	 Loss:  0.02734997496008873
Epoch:  79 	 Loss:  0.027210235595703125
Epoch:  80 	 Loss:  0.027073176577687263
Epoch:  81 	 Loss:  0.02734023705124855
Epoch:  82 	 Loss:  0.027165761217474937
Epoch:  83 	 Loss:  0.02701377496123314
Epoch:  84 	 Loss:  0.02741776965558529
Epoch:  85 	 Loss:  0.0278102345764637
Epoch:  86 	 Loss:  0.027264174073934555
Epoch:  87 	 Loss:  0.027427617460489273
Epoch:  88 	 Loss:  0.02744390442967415
Epoch:  89 	 Loss:  0.027156375348567963
Epoch:  90 	 Loss:  0.02658742107450962
Epoch:  91 	 Loss:  0.027305006980895996
Epoch:  92 	 Loss:  0.02718311920762062
Epoch:  93 	 Loss:  0.02704673446714878
Epoch:  94 	 Loss:  0.027473747730255127
Epoch:  95 	 Loss:  0.027662493288517
Epoch:  96 	 Loss:  0.02745421975851059
Epoch:  97 	 Loss:  0.027103161439299583
Epoch:  98 	 Loss:  0.02708224207162857
Epoch:  99 	 Loss:  0.02718968130648136
Epoch:  100 	 Loss:  0.027525024488568306
Exception in thread Exception in threading.excepthook:
Traceback (most recent call last):
  File "/usr/lib/python3.8/threading.py", line 1202, in invoke_excepthook
    hook(args)
ValueError: I/O operation on closed file.
Problem finishing run
Traceback (most recent call last):
  File "/home/naddeok5/.local/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 1374, in _atexit_cleanup
    self._on_finish()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 1513, in _on_finish
    self._console_stop()  # TODO: there's a race here with jupyter console logging
  File "/home/naddeok5/.local/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 1410, in _console_stop
    self._restore()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 1342, in _restore
    self._out_redir.uninstall()
  File "/home/naddeok5/.local/lib/python3.8/site-packages/wandb/sdk/lib/redirect.py", line 233, in uninstall
    self._redirect(to_fd=self._old_fp.fileno(), close=True)
  File "/home/naddeok5/.local/lib/python3.8/site-packages/wandb/sdk/lib/redirect.py", line 195, in _redirect
    setattr(sys, self._stream, os.fdopen(self._old_fd, "w"))
  File "/usr/lib/python3.8/os.py", line 1023, in fdopen
    return io.open(fd, *args, **kwargs)
OSError: [Errno 9] Bad file descriptor
wandb: ERROR Problem finishing run
/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 438 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
